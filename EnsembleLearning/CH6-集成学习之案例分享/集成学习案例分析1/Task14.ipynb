{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习案例一 （幸福感预测）\n",
    "\n",
    "### 背景介绍\n",
    "\n",
    "幸福感是一个古老而深刻的话题，是人类世代追求的方向。与幸福感相关的因素成千上万、因人而异，大如国计民生，小如路边烤红薯，都会对幸福感产生影响。这些错综复杂的因素中，我们能找到其中的共性，一窥幸福感的要义吗？\n",
    "\n",
    "另外，在社会科学领域，幸福感的研究占有重要的位置。这个涉及了哲学、心理学、社会学、经济学等多方学科的话题复杂而有趣；同时与大家生活息息相关，每个人对幸福感都有自己的衡量标准。如果能发现影响幸福感的共性，生活中是不是将多一些乐趣；如果能找到影响幸福感的政策因素，便能优化资源配置来提升国民的幸福感。目前社会科学研究注重变量的可解释性和未来政策的落地，主要采用了线性回归和逻辑回归的方法，在收入、健康、职业、社交关系、休闲方式等经济人口因素；以及政府公共服务、宏观经济环境、税负等宏观因素上有了一系列的推测和发现。\n",
    "\n",
    "该案例为幸福感预测这一经典课题，希望在现有社会科学研究外有其他维度的算法尝试，结合多学科各自优势，挖掘潜在的影响因素，发现更多可解释、可理解的相关关系。\n",
    "\n",
    "具体来说，该案例就是一个数据挖掘类型的比赛——幸福感预测的baseline。具体来说，我们需要使用包括个体变量（性别、年龄、地域、职业、健康、婚姻与政治面貌等等）、家庭变量（父母、配偶、子女、家庭资本等等）、社会态度（公平、信用、公共服务等等）等139维度的信息来预测其对幸福感的影响。\n",
    "\n",
    "我们的数据来源于国家官方的《中国综合社会调查（CGSS）》文件中的调查结果中的数据，数据来源可靠可依赖:)\n",
    "\n",
    "### 数据信息\n",
    "赛题要求使用以上 **139** 维的特征，使用 **8000** 余组数据进行对于个人幸福感的预测（预测值为1，2，3，4，5，其中1代表幸福感最低，5代表幸福感最高）。\n",
    "因为考虑到变量个数较多，部分变量间关系复杂，数据分为完整版和精简版两类。可从精简版入手熟悉赛题后，使用完整版挖掘更多信息。在这里我直接使用了完整版的数据。赛题也给出了index文件中包含每个变量对应的问卷题目，以及变量取值的含义；survey文件中为原版问卷，作为补充以方便理解问题背景。\n",
    "\n",
    "### 评价指标\n",
    "最终的评价指标为均方误差MSE，即：\n",
    "$$Score = \\frac{1}{n} \\sum_1 ^n (y_i - y ^*)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error,mean_absolute_error, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import ExtraTreesRegressor as etr\n",
    "from sklearn.linear_model import BayesianRidge as br\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.kernel_ridge import KernelRidge as kr\n",
    "from sklearn.model_selection import  KFold, StratifiedKFold,GroupKFold, RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') #消除warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据集并查看数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据_处理前： (7988, 140)\n",
      "训练数据_处理后： (7988, 139)\n",
      "测试数据： (2968, 139)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\", parse_dates=['survey_time'],encoding='latin-1') \n",
    "test = pd.read_csv(\"test.csv\", parse_dates=['survey_time'],encoding='latin-1') #latin-1向下兼容ASCII\n",
    "#删去\"happiness\" 为-8的行\n",
    "train = train[train[\"happiness\"]!=-8].reset_index(drop=True) # reset_index:讲happiness为异常值的数据删除后，重置index\n",
    "train_data_copy = train.copy() \n",
    "target_col = \"happiness\" #目标列\n",
    "target = train_data_copy[target_col]\n",
    "del train_data_copy[target_col] #去除目标列\n",
    "print(\"训练数据_处理前：\",train.shape)\n",
    "print(\"训练数据_处理后：\",train_data_copy.shape)\n",
    "print(\"测试数据：\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据+测试数据： (10956, 139)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10956 entries, 0 to 10955\n",
      "Columns: 139 entries, id to public_service_9\n",
      "dtypes: datetime64[ns](1), float64(26), int64(109), object(3)\n",
      "memory usage: 11.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 拼接traindata和testdata\n",
    "data = pd.concat([train_data_copy,test],axis=0,ignore_index=True)\n",
    "data.head()\n",
    "print(\"训练数据+测试数据：\",data.shape)\n",
    "# print(\",\".join(train.columns))\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7988.000000\n",
       "mean        3.867927\n",
       "std         0.818717\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         4.000000\n",
       "75%         4.000000\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.happiness.describe() #训练集中幸福感的概括，四分位数为4，如此看来，大部分人都还挺幸福的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "#### 缺失值概况\n",
    "首先需要对于数据中的连续出现的负数值(即缺失值/异常值)进行处理。由于数据中的负数值只有-1，-2，-3，-8这几种数值，所以它们进行分别的操作，实现代码如下：\n",
    "\n",
    "<font color=red>以下代码仅统计各个样本中缺失值的特征数量，未统计各个特征中的缺失值比例.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数量+5=139+5=144\n",
    "#csv中有负数值：-1、-2、-3、-8，将他们视为有问题的特征，但是不删去\n",
    "def getres1(row):\n",
    "    return len([x for x in row.values if type(x)==int and x<0])\n",
    "\n",
    "def getres2(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-8])\n",
    "\n",
    "def getres3(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-1])\n",
    "\n",
    "def getres4(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-2])\n",
    "\n",
    "def getres5(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-3])\n",
    "\n",
    "#检查数据\n",
    "# axis=1 水平方向检查该样本的特征，统计其缺失特征的个数，超过20个的归为20.\n",
    "data['neg1'] = data[data.columns].apply(lambda row:getres1(row),axis=1)\n",
    "data.loc[data['neg1']>20,'neg1'] = 20  #平滑处理,最多出现20次\n",
    "\n",
    "data['neg2'] = data[data.columns].apply(lambda row:getres2(row),axis=1)\n",
    "data['neg3'] = data[data.columns].apply(lambda row:getres3(row),axis=1)\n",
    "data['neg4'] = data[data.columns].apply(lambda row:getres4(row),axis=1)\n",
    "data['neg5'] = data[data.columns].apply(lambda row:getres5(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各个样本的特征缺失情况：\n",
      "    neg1  neg2  neg3  neg4  neg5\n",
      "0     5     3     0     2     0\n",
      "1     0     0     0     0     0\n",
      "2     3     1     0     2     0\n",
      "3     2     0     0     2     0\n",
      "4     2     1     1     0     0\n",
      "5     1     0     0     1     0\n",
      "6     1     0     0     0     1\n",
      "7     5     2     1     2     0\n",
      "8     0     0     0     0     0\n",
      "9     1     1     0     0     0\n"
     ]
    }
   ],
   "source": [
    "print(\"各个样本的特征缺失情况：\\n\",data[['neg1','neg2','neg3','neg4','neg5']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缺失值填充\n",
    "填充缺失值，这里采取的方式是将缺失值补全，使用fillna(value)，其中value的数值根据具体的情况来确定。例如将大部分缺失信息认为是零，将家庭成员数认为是1，将家庭收入这个特征认为是66365，即所有家庭的收入平均值。部分实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填充缺失值 共25列 去掉4列 填充21列\n",
    "#以下的列都是缺省的，视情况填补\n",
    "data['work_status'] = data['work_status'].fillna(0)\n",
    "data['work_yr'] = data['work_yr'].fillna(0)\n",
    "data['work_manage'] = data['work_manage'].fillna(0)\n",
    "data['work_type'] = data['work_type'].fillna(0)\n",
    "\n",
    "data['edu_yr'] = data['edu_yr'].fillna(0)\n",
    "data['edu_status'] = data['edu_status'].fillna(0)\n",
    "\n",
    "data['s_work_type'] = data['s_work_type'].fillna(0)\n",
    "data['s_work_status'] = data['s_work_status'].fillna(0)\n",
    "data['s_political'] = data['s_political'].fillna(0)\n",
    "data['s_hukou'] = data['s_hukou'].fillna(0)\n",
    "data['s_income'] = data['s_income'].fillna(0)\n",
    "data['s_birth'] = data['s_birth'].fillna(0)\n",
    "data['s_edu'] = data['s_edu'].fillna(0)\n",
    "data['s_work_exper'] = data['s_work_exper'].fillna(0)\n",
    "\n",
    "data['minor_child'] = data['minor_child'].fillna(0)\n",
    "data['marital_now'] = data['marital_now'].fillna(0)\n",
    "data['marital_1st'] = data['marital_1st'].fillna(0)\n",
    "data['social_neighbor']=data['social_neighbor'].fillna(0)\n",
    "data['social_friend']=data['social_friend'].fillna(0)\n",
    "data['hukou_loc']=data['hukou_loc'].fillna(1) #最少为1，表示户口\n",
    "#data['family_income']=data['family_income'].fillna(66365) #删除问题值后的平均值？？\n",
    "data['family_income']=data['family_income'].fillna(data[data['family_income']>0]['family_income'].median()) # 家庭收入的中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剔除异常值样本_家庭年收入均值： 73785.026182261\n",
      "剔除异常值样本_家庭年收入中位数： 40000.0\n",
      "全部样本_家庭年收入均值： 66363.23110624315\n",
      "全部样本_家庭年收入中位数： 38580.0\n"
     ]
    }
   ],
   "source": [
    "#data.loc[data['family_income']==9999992,'family_income'] = -8\n",
    "#data['family_income'].max()\n",
    "print(\"剔除异常值样本_家庭年收入均值：\",data[data['family_income']>0]['family_income'].mean())\n",
    "print(\"剔除异常值样本_家庭年收入中位数：\",data[data['family_income']>0]['family_income'].median())\n",
    "print(\"全部样本_家庭年收入均值：\",data['family_income'].mean())\n",
    "print(\"全部样本_家庭年收入中位数：\",data['family_income'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，还有特殊格式的信息需要另外处理，比如与时间有关的信息，这里主要分为两部分进行处理：首先是将“连续”的年龄，进行分层处理，即划分年龄段，具体地在这里我们将年龄分为了6个区间。其次是计算具体的年龄，在Excel表格中，只有出生年月以及调查时间等信息，我们根据此计算出每一位调查者的真实年龄。具体实现代码如下：\n",
    "#### 特殊类型数据处理（时间类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  survey_time  birth\n",
      "0   56         2015   1959\n",
      "1   23         2015   1992\n",
      "2   48         2015   1967\n",
      "3   72         2015   1943\n",
      "4   21         2015   1994\n"
     ]
    }
   ],
   "source": [
    "#特征数量+1 = 144+1 =145\n",
    "#继续进行特殊的列进行数据处理\n",
    "#读happiness_index.xlsx\n",
    "data['survey_time'] = pd.to_datetime(data['survey_time'], format='%Y-%m-%d',errors='coerce')#防止时间格式不同的报错errors='coerce‘\n",
    "data['survey_time'] = data['survey_time'].dt.year #仅仅是year，方便计算年龄\n",
    "data['age'] = data['survey_time']-data['birth']\n",
    "print(data[['age','survey_time','birth']].head(5))\n",
    "#年龄分层 \n",
    "#特征数量+1 = 145+1=146\n",
    "bins = [0,17,26,34,50,63,100]\n",
    "data['age_bin'] = pd.cut(data['age'], bins, labels=[0,1,2,3,4,5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。第三种方法是使用我们日常生活中的真实情况，例如“宗教信息”特征为负数的认为是“不信仰宗教”，并认为“参加宗教活动的频率”为1，即没有参加过宗教活动，主观的进行补全，这也是我在这一步骤中使用最多的一种方式。就像我自己填表一样，这里我全部都使用了我自己的想法进行缺省值的补全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对‘宗教’处理\n",
    "data.loc[data['religion']<0,'religion'] = 1 #1为不信仰宗教\n",
    "data.loc[data['religion_freq']<0,'religion_freq'] = 1 #1为从来没有参加过\n",
    "#对‘教育程度’处理\n",
    "data.loc[data['edu']<0,'edu'] = 4 #初中\n",
    "data.loc[data['edu_status']<0,'edu_status'] = 0\n",
    "data.loc[data['edu_yr']<0,'edu_yr'] = 0\n",
    "#对‘个人收入’处理\n",
    "data.loc[data['income']<0,'income'] = 0 #认为无收入\n",
    "#对‘政治面貌’处理\n",
    "data.loc[data['political']<0,'political'] = 1 #认为是群众\n",
    "#对体重处理\n",
    "data.loc[(data['weight_jin']<=80)&(data['height_cm']>=160),'weight_jin']= data['weight_jin']*2\n",
    "data.loc[data['weight_jin']<=60,'weight_jin']= data['weight_jin']*2  #个人的想法，哈哈哈，没有60斤的成年人吧\n",
    "#对身高处理\n",
    "data.loc[data['height_cm']<150,'height_cm'] = 150 #成年人的实际情况\n",
    "#对‘健康’处理\n",
    "data.loc[data['health']<0,'health'] = 4 #认为是比较健康\n",
    "data.loc[data['health_problem']<0,'health_problem'] = 4\n",
    "#对‘沮丧’处理\n",
    "data.loc[data['depression']<0,'depression'] = 4 #一般人都是很少吧\n",
    "#对‘媒体’处理\n",
    "data.loc[data['media_1']<0,'media_1'] = 1 #都是从不\n",
    "data.loc[data['media_2']<0,'media_2'] = 1\n",
    "data.loc[data['media_3']<0,'media_3'] = 1\n",
    "data.loc[data['media_4']<0,'media_4'] = 1\n",
    "data.loc[data['media_5']<0,'media_5'] = 1\n",
    "data.loc[data['media_6']<0,'media_6'] = 1\n",
    "#对‘空闲活动’处理\n",
    "data.loc[data['leisure_1']<0,'leisure_1'] = 1 #都是根据自己的想法\n",
    "data.loc[data['leisure_2']<0,'leisure_2'] = 5\n",
    "data.loc[data['leisure_3']<0,'leisure_3'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用众数（代码中使用mode()来实现异常值的修正），由于这里的特征是空闲活动，所以采用众数对于缺失值进行处理比较合理。具体的代码参考如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['leisure_4']<0,'leisure_4'] = data['leisure_4'].mode() #取众数\n",
    "data.loc[data['leisure_5']<0,'leisure_5'] = data['leisure_5'].mode()\n",
    "data.loc[data['leisure_6']<0,'leisure_6'] = data['leisure_6'].mode()\n",
    "data.loc[data['leisure_7']<0,'leisure_7'] = data['leisure_7'].mode()\n",
    "data.loc[data['leisure_8']<0,'leisure_8'] = data['leisure_8'].mode()\n",
    "data.loc[data['leisure_9']<0,'leisure_9'] = data['leisure_9'].mode()\n",
    "data.loc[data['leisure_10']<0,'leisure_10'] = data['leisure_10'].mode()\n",
    "data.loc[data['leisure_11']<0,'leisure_11'] = data['leisure_11'].mode()\n",
    "data.loc[data['leisure_12']<0,'leisure_12'] = data['leisure_12'].mode()\n",
    "data.loc[data['socialize']<0,'socialize'] = 2 #很少\n",
    "data.loc[data['relax']<0,'relax'] = 4 #经常\n",
    "data.loc[data['learn']<0,'learn'] = 1 #从不，哈哈哈哈\n",
    "#对‘社交’处理\n",
    "data.loc[data['social_neighbor']<0,'social_neighbor'] = 0\n",
    "data.loc[data['social_friend']<0,'social_friend'] = 0\n",
    "data.loc[data['socia_outing']<0,'socia_outing'] = 1\n",
    "data.loc[data['neighbor_familiarity']<0,'social_neighbor']= 4\n",
    "#对‘社会公平性’处理\n",
    "data.loc[data['equity']<0,'equity'] = 4\n",
    "#对‘社会等级’处理\n",
    "data.loc[data['class_10_before']<0,'class_10_before'] = 3\n",
    "data.loc[data['class']<0,'class'] = 5\n",
    "data.loc[data['class_10_after']<0,'class_10_after'] = 5\n",
    "data.loc[data['class_14']<0,'class_14'] = 2\n",
    "#对‘工作情况’处理\n",
    "data.loc[data['work_status']<0,'work_status'] = 0\n",
    "data.loc[data['work_yr']<0,'work_yr'] = 0\n",
    "data.loc[data['work_manage']<0,'work_manage'] = 0\n",
    "data.loc[data['work_type']<0,'work_type'] = 0\n",
    "#对‘社会保障’处理\n",
    "data.loc[data['insur_1']<0,'insur_1'] = 1\n",
    "data.loc[data['insur_2']<0,'insur_2'] = 1\n",
    "data.loc[data['insur_3']<0,'insur_3'] = 1\n",
    "data.loc[data['insur_4']<0,'insur_4'] = 1\n",
    "data.loc[data['insur_1']==0,'insur_1'] = 0\n",
    "data.loc[data['insur_2']==0,'insur_2'] = 0\n",
    "data.loc[data['insur_3']==0,'insur_3'] = 0\n",
    "data.loc[data['insur_4']==0,'insur_4'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取均值进行缺失值的补全（代码实现为mean()），在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。具体的代码参考如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对家庭情况处理\n",
    "family_income_mean = data['family_income'].mean()\n",
    "data.loc[data['family_income']<0,'family_income'] = family_income_mean\n",
    "data.loc[data['family_m']<0,'family_m'] = 2\n",
    "data.loc[data['family_status']<0,'family_status'] = 3\n",
    "data.loc[data['house']<0,'house'] = 1\n",
    "data.loc[data['car']<0,'car'] = 0\n",
    "data.loc[data['car']==2,'car'] = 0\n",
    "data.loc[data['son']<0,'son'] = 1\n",
    "data.loc[data['daughter']<0,'daughter'] = 0\n",
    "data.loc[data['minor_child']<0,'minor_child'] = 0\n",
    "#对‘婚姻’处理\n",
    "data.loc[data['marital_1st']<0,'marital_1st'] = 0\n",
    "data.loc[data['marital_now']<0,'marital_now'] = 0\n",
    "#对‘配偶’处理\n",
    "data.loc[data['s_birth']<0,'s_birth'] = 0\n",
    "data.loc[data['s_edu']<0,'s_edu'] = 0\n",
    "data.loc[data['s_political']<0,'s_political'] = 0\n",
    "data.loc[data['s_hukou']<0,'s_hukou'] = 0\n",
    "data.loc[data['s_income']<0,'s_income'] = 0\n",
    "data.loc[data['s_work_type']<0,'s_work_type'] = 0\n",
    "data.loc[data['s_work_status']<0,'s_work_status'] = 0\n",
    "data.loc[data['s_work_exper']<0,'s_work_exper'] = 0\n",
    "#对‘父母情况’处理\n",
    "data.loc[data['f_birth']<0,'f_birth'] = 1945\n",
    "data.loc[data['f_edu']<0,'f_edu'] = 1\n",
    "data.loc[data['f_political']<0,'f_political'] = 1\n",
    "data.loc[data['f_work_14']<0,'f_work_14'] = 2\n",
    "data.loc[data['m_birth']<0,'m_birth'] = 1940\n",
    "data.loc[data['m_edu']<0,'m_edu'] = 1\n",
    "data.loc[data['m_political']<0,'m_political'] = 1\n",
    "data.loc[data['m_work_14']<0,'m_work_14'] = 2\n",
    "#和同龄人相比社会经济地位\n",
    "data.loc[data['status_peer']<0,'status_peer'] = 2\n",
    "#和3年前比社会经济地位\n",
    "data.loc[data['status_3_before']<0,'status_3_before'] = 2\n",
    "#对‘观点’处理\n",
    "data.loc[data['view']<0,'view'] = 4\n",
    "#对期望年收入处理\n",
    "data.loc[data['inc_ability']<=0,'inc_ability']= 2\n",
    "inc_exp_mean = data['inc_exp'].mean()\n",
    "data.loc[data['inc_exp']<=0,'inc_exp']= inc_exp_mean #取均值\n",
    "\n",
    "#部分特征处理，取众数\n",
    "for i in range(1,9+1):\n",
    "    data.loc[data['public_service_'+str(i)]<0,'public_service_'+str(i)] = data['public_service_'+str(i)].dropna().mode()\n",
    "for i in range(1,13+1):\n",
    "    data.loc[data['trust_'+str(i)]<0,'trust_'+str(i)] = data['trust_'+str(i)].dropna().mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10956, 146)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征扩增\n",
    "\n",
    "这一步，我们需要进一步分析每一个特征之间的关系，从而进行数据增广。经过思考，这里我添加了如下的特征：第一次结婚年龄、最近结婚年龄、是否再婚、配偶年龄、配偶年龄差、各种收入比（与配偶之间的收入比、十年后预期收入与现在收入之比等等）、收入与住房面积比（其中也包括10年后期望收入等等各种情况）、社会阶级（10年后的社会阶级、14年后的社会阶级等等）、悠闲指数、满意指数、信任指数等等。除此之外，我还考虑了对于同一省、市、县进行了归一化。例如同一省市内的收入的平均值等以及一个个体相对于同省、市、县其他人的各个指标的情况。同时也考虑了对于同龄人之间的相互比较，即在同龄人中的收入情况、健康情况等等。具体的实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一次结婚年龄 特征数量+1 = 147\n",
    "data['marital_1stbir'] = data['marital_1st'] - data['birth'] \n",
    "#最近结婚年龄 特征数量+1 = 148\n",
    "data['marital_nowtbir'] = data['marital_now'] - data['birth'] \n",
    "#是否再婚 特征数量+1 = 149\n",
    "data['mar'] = data['marital_nowtbir'] - data['marital_1stbir']\n",
    "#配偶年龄 特征数量+1 = 150\n",
    "data['marital_sbir'] = data['marital_now']-data['s_birth']\n",
    "#配偶年龄差 特征数量+1 = 151\n",
    "data['age_'] = data['marital_nowtbir'] - data['marital_sbir'] \n",
    "\n",
    "#收入比 特征数量 = 151+7 =158\n",
    "data['income/s_income'] = data['income']/(data['s_income']+1)\n",
    "data['income+s_income'] = data['income']+(data['s_income']+1)\n",
    "data['income/family_income'] = data['income']/(data['family_income']+1)\n",
    "data['all_income/family_income'] = (data['income']+data['s_income'])/(data['family_income']+1)\n",
    "data['income/inc_exp'] = data['income']/(data['inc_exp']+1)\n",
    "data['family_income/m'] = data['family_income']/(data['family_m']+0.01)\n",
    "data['income/m'] = data['income']/(data['family_m']+0.01)\n",
    "\n",
    "#收入/面积比 特征数量 = 158+4=162\n",
    "data['income/floor_area'] = data['income']/(data['floor_area']+0.01)\n",
    "data['all_income/floor_area'] = (data['income']+data['s_income'])/(data['floor_area']+0.01)\n",
    "data['family_income/floor_area'] = data['family_income']/(data['floor_area']+0.01)\n",
    "data['floor_area/m'] = data['floor_area']/(data['family_m']+0.01)\n",
    "\n",
    "#社会等级 特征数量 = 162+3=165\n",
    "data['class_10_diff'] = (data['class_10_after'] - data['class'])\n",
    "data['class_diff'] = data['class'] - data['class_10_before']\n",
    "data['class_14_diff'] = data['class'] - data['class_14']\n",
    "#悠闲指数 特征数量+1 = 166\n",
    "leisure_fea_lis = ['leisure_'+str(i) for i in range(1,13)]\n",
    "data['leisure_sum'] = data[leisure_fea_lis].sum(axis=1) #skew\n",
    "#满意指数 特征数量+1 = 1167\n",
    "public_service_fea_lis = ['public_service_'+str(i) for i in range(1,10)]\n",
    "data['public_service_sum'] = data[public_service_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#信任指数 特征数量+1 = 1168\n",
    "trust_fea_lis = ['trust_'+str(i) for i in range(1,14)]\n",
    "data['trust_sum'] = data[trust_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#province mean 特征数量 = 168+13=181\n",
    "data['province_income_mean'] = data.groupby(['province'])['income'].transform('mean').values\n",
    "data['province_family_income_mean'] = data.groupby(['province'])['family_income'].transform('mean').values\n",
    "data['province_equity_mean'] = data.groupby(['province'])['equity'].transform('mean').values\n",
    "data['province_depression_mean'] = data.groupby(['province'])['depression'].transform('mean').values\n",
    "data['province_floor_area_mean'] = data.groupby(['province'])['floor_area'].transform('mean').values\n",
    "data['province_health_mean'] = data.groupby(['province'])['health'].transform('mean').values\n",
    "data['province_class_10_diff_mean'] = data.groupby(['province'])['class_10_diff'].transform('mean').values\n",
    "data['province_class_mean'] = data.groupby(['province'])['class'].transform('mean').values\n",
    "data['province_health_problem_mean'] = data.groupby(['province'])['health_problem'].transform('mean').values\n",
    "data['province_family_status_mean'] = data.groupby(['province'])['family_status'].transform('mean').values\n",
    "data['province_leisure_sum_mean'] = data.groupby(['province'])['leisure_sum'].transform('mean').values\n",
    "data['province_public_service_sum_mean'] = data.groupby(['province'])['public_service_sum'].transform('mean').values\n",
    "data['province_trust_sum_mean'] = data.groupby(['province'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#city   mean 特征数量 = 特征数量 = 181+13=194\n",
    "data['city_income_mean'] = data.groupby(['city'])['income'].transform('mean').values\n",
    "data['city_family_income_mean'] = data.groupby(['city'])['family_income'].transform('mean').values\n",
    "data['city_equity_mean'] = data.groupby(['city'])['equity'].transform('mean').values\n",
    "data['city_depression_mean'] = data.groupby(['city'])['depression'].transform('mean').values\n",
    "data['city_floor_area_mean'] = data.groupby(['city'])['floor_area'].transform('mean').values\n",
    "data['city_health_mean'] = data.groupby(['city'])['health'].transform('mean').values\n",
    "data['city_class_10_diff_mean'] = data.groupby(['city'])['class_10_diff'].transform('mean').values\n",
    "data['city_class_mean'] = data.groupby(['city'])['class'].transform('mean').values\n",
    "data['city_health_problem_mean'] = data.groupby(['city'])['health_problem'].transform('mean').values\n",
    "data['city_family_status_mean'] = data.groupby(['city'])['family_status'].transform('mean').values\n",
    "data['city_leisure_sum_mean'] = data.groupby(['city'])['leisure_sum'].transform('mean').values\n",
    "data['city_public_service_sum_mean'] = data.groupby(['city'])['public_service_sum'].transform('mean').values\n",
    "data['city_trust_sum_mean'] = data.groupby(['city'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#county  mean 特征数量 = 194 + 13 = 207\n",
    "data['county_income_mean'] = data.groupby(['county'])['income'].transform('mean').values\n",
    "data['county_family_income_mean'] = data.groupby(['county'])['family_income'].transform('mean').values\n",
    "data['county_equity_mean'] = data.groupby(['county'])['equity'].transform('mean').values\n",
    "data['county_depression_mean'] = data.groupby(['county'])['depression'].transform('mean').values\n",
    "data['county_floor_area_mean'] = data.groupby(['county'])['floor_area'].transform('mean').values\n",
    "data['county_health_mean'] = data.groupby(['county'])['health'].transform('mean').values\n",
    "data['county_class_10_diff_mean'] = data.groupby(['county'])['class_10_diff'].transform('mean').values\n",
    "data['county_class_mean'] = data.groupby(['county'])['class'].transform('mean').values\n",
    "data['county_health_problem_mean'] = data.groupby(['county'])['health_problem'].transform('mean').values\n",
    "data['county_family_status_mean'] = data.groupby(['county'])['family_status'].transform('mean').values\n",
    "data['county_leisure_sum_mean'] = data.groupby(['county'])['leisure_sum'].transform('mean').values\n",
    "data['county_public_service_sum_mean'] = data.groupby(['county'])['public_service_sum'].transform('mean').values\n",
    "data['county_trust_sum_mean'] = data.groupby(['county'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#ratio 相比同省 特征数量 = 207 + 13 =220\n",
    "data['income/province'] = data['income']/(data['province_income_mean'])                                      \n",
    "data['family_income/province'] = data['family_income']/(data['province_family_income_mean'])   \n",
    "data['equity/province'] = data['equity']/(data['province_equity_mean'])       \n",
    "data['depression/province'] = data['depression']/(data['province_depression_mean'])                                                \n",
    "data['floor_area/province'] = data['floor_area']/(data['province_floor_area_mean'])\n",
    "data['health/province'] = data['health']/(data['province_health_mean'])\n",
    "data['class_10_diff/province'] = data['class_10_diff']/(data['province_class_10_diff_mean'])\n",
    "data['class/province'] = data['class']/(data['province_class_mean'])\n",
    "data['health_problem/province'] = data['health_problem']/(data['province_health_problem_mean'])\n",
    "data['family_status/province'] = data['family_status']/(data['province_family_status_mean'])\n",
    "data['leisure_sum/province'] = data['leisure_sum']/(data['province_leisure_sum_mean'])\n",
    "data['public_service_sum/province'] = data['public_service_sum']/(data['province_public_service_sum_mean'])\n",
    "data['trust_sum/province'] = data['trust_sum']/(data['province_trust_sum_mean']+1)\n",
    "\n",
    "#ratio 相比同市 特征数量 = 220 + 13 =233\n",
    "data['income/city'] = data['income']/(data['city_income_mean'])                                      \n",
    "data['family_income/city'] = data['family_income']/(data['city_family_income_mean'])   \n",
    "data['equity/city'] = data['equity']/(data['city_equity_mean'])       \n",
    "data['depression/city'] = data['depression']/(data['city_depression_mean'])                                                \n",
    "data['floor_area/city'] = data['floor_area']/(data['city_floor_area_mean'])\n",
    "data['health/city'] = data['health']/(data['city_health_mean'])\n",
    "data['class_10_diff/city'] = data['class_10_diff']/(data['city_class_10_diff_mean'])\n",
    "data['class/city'] = data['class']/(data['city_class_mean'])\n",
    "data['health_problem/city'] = data['health_problem']/(data['city_health_problem_mean'])\n",
    "data['family_status/city'] = data['family_status']/(data['city_family_status_mean'])\n",
    "data['leisure_sum/city'] = data['leisure_sum']/(data['city_leisure_sum_mean'])\n",
    "data['public_service_sum/city'] = data['public_service_sum']/(data['city_public_service_sum_mean'])\n",
    "data['trust_sum/city'] = data['trust_sum']/(data['city_trust_sum_mean'])\n",
    "\n",
    "#ratio 相比同个地区 特征数量 = 233 + 13 =246\n",
    "data['income/county'] = data['income']/(data['county_income_mean'])                                      \n",
    "data['family_income/county'] = data['family_income']/(data['county_family_income_mean'])   \n",
    "data['equity/county'] = data['equity']/(data['county_equity_mean'])       \n",
    "data['depression/county'] = data['depression']/(data['county_depression_mean'])                                                \n",
    "data['floor_area/county'] = data['floor_area']/(data['county_floor_area_mean'])\n",
    "data['health/county'] = data['health']/(data['county_health_mean'])\n",
    "data['class_10_diff/county'] = data['class_10_diff']/(data['county_class_10_diff_mean'])\n",
    "data['class/county'] = data['class']/(data['county_class_mean'])\n",
    "data['health_problem/county'] = data['health_problem']/(data['county_health_problem_mean'])\n",
    "data['family_status/county'] = data['family_status']/(data['county_family_status_mean'])\n",
    "data['leisure_sum/county'] = data['leisure_sum']/(data['county_leisure_sum_mean'])\n",
    "data['public_service_sum/county'] = data['public_service_sum']/(data['county_public_service_sum_mean'])\n",
    "data['trust_sum/county'] = data['trust_sum']/(data['county_trust_sum_mean'])\n",
    "\n",
    "#age   mean 特征数量 = 246+ 13 =259\n",
    "data['age_income_mean'] = data.groupby(['age'])['income'].transform('mean').values\n",
    "data['age_family_income_mean'] = data.groupby(['age'])['family_income'].transform('mean').values\n",
    "data['age_equity_mean'] = data.groupby(['age'])['equity'].transform('mean').values\n",
    "data['age_depression_mean'] = data.groupby(['age'])['depression'].transform('mean').values\n",
    "data['age_floor_area_mean'] = data.groupby(['age'])['floor_area'].transform('mean').values\n",
    "data['age_health_mean'] = data.groupby(['age'])['health'].transform('mean').values\n",
    "data['age_class_10_diff_mean'] = data.groupby(['age'])['class_10_diff'].transform('mean').values\n",
    "data['age_class_mean'] = data.groupby(['age'])['class'].transform('mean').values\n",
    "data['age_health_problem_mean'] = data.groupby(['age'])['health_problem'].transform('mean').values\n",
    "data['age_family_status_mean'] = data.groupby(['age'])['family_status'].transform('mean').values\n",
    "data['age_leisure_sum_mean'] = data.groupby(['age'])['leisure_sum'].transform('mean').values\n",
    "data['age_public_service_sum_mean'] = data.groupby(['age'])['public_service_sum'].transform('mean').values\n",
    "data['age_trust_sum_mean'] = data.groupby(['age'])['trust_sum'].transform('mean').values\n",
    "\n",
    "# 和同龄人相比  特征数量 = 259 + 13 =272\n",
    "data['income/age'] = data['income']/(data['age_income_mean'])                                      \n",
    "data['family_income/age'] = data['family_income']/(data['age_family_income_mean'])   \n",
    "data['equity/age'] = data['equity']/(data['age_equity_mean'])       \n",
    "data['depression/age'] = data['depression']/(data['age_depression_mean'])                                                \n",
    "data['floor_area/age'] = data['floor_area']/(data['age_floor_area_mean'])\n",
    "data['health/age'] = data['health']/(data['age_health_mean'])\n",
    "data['class_10_diff/age'] = data['class_10_diff']/(data['age_class_10_diff_mean'])\n",
    "data['class/age'] = data['class']/(data['age_class_mean'])\n",
    "data['health_problem/age'] = data['health_problem']/(data['age_health_problem_mean'])\n",
    "data['family_status/age'] = data['family_status']/(data['age_family_status_mean'])\n",
    "data['leisure_sum/age'] = data['leisure_sum']/(data['age_leisure_sum_mean'])\n",
    "data['public_service_sum/age'] = data['public_service_sum']/(data['age_public_service_sum_mean'])\n",
    "data['trust_sum/age'] = data['trust_sum']/(data['age_trust_sum_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过如上的操作后，最终我们的特征从一开始的131维，扩充为了272维的特征。接下来考虑特征工程、训练模型以及模型融合的工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据的shape: (10956, 272)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey_type</th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>survey_time</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>religion</th>\n",
       "      <th>religion_freq</th>\n",
       "      <th>edu</th>\n",
       "      <th>edu_other</th>\n",
       "      <th>edu_status</th>\n",
       "      <th>edu_yr</th>\n",
       "      <th>income</th>\n",
       "      <th>political</th>\n",
       "      <th>join_party</th>\n",
       "      <th>floor_area</th>\n",
       "      <th>property_0</th>\n",
       "      <th>property_1</th>\n",
       "      <th>property_2</th>\n",
       "      <th>property_3</th>\n",
       "      <th>property_4</th>\n",
       "      <th>property_5</th>\n",
       "      <th>property_6</th>\n",
       "      <th>property_7</th>\n",
       "      <th>property_8</th>\n",
       "      <th>property_other</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_jin</th>\n",
       "      <th>health</th>\n",
       "      <th>health_problem</th>\n",
       "      <th>depression</th>\n",
       "      <th>hukou</th>\n",
       "      <th>hukou_loc</th>\n",
       "      <th>media_1</th>\n",
       "      <th>media_2</th>\n",
       "      <th>media_3</th>\n",
       "      <th>media_4</th>\n",
       "      <th>media_5</th>\n",
       "      <th>media_6</th>\n",
       "      <th>leisure_1</th>\n",
       "      <th>leisure_2</th>\n",
       "      <th>leisure_3</th>\n",
       "      <th>leisure_4</th>\n",
       "      <th>leisure_5</th>\n",
       "      <th>leisure_6</th>\n",
       "      <th>leisure_7</th>\n",
       "      <th>leisure_8</th>\n",
       "      <th>leisure_9</th>\n",
       "      <th>leisure_10</th>\n",
       "      <th>leisure_11</th>\n",
       "      <th>leisure_12</th>\n",
       "      <th>socialize</th>\n",
       "      <th>relax</th>\n",
       "      <th>learn</th>\n",
       "      <th>social_neighbor</th>\n",
       "      <th>social_friend</th>\n",
       "      <th>socia_outing</th>\n",
       "      <th>equity</th>\n",
       "      <th>class</th>\n",
       "      <th>class_10_before</th>\n",
       "      <th>class_10_after</th>\n",
       "      <th>class_14</th>\n",
       "      <th>work_exper</th>\n",
       "      <th>work_status</th>\n",
       "      <th>work_yr</th>\n",
       "      <th>work_type</th>\n",
       "      <th>work_manage</th>\n",
       "      <th>insur_1</th>\n",
       "      <th>insur_2</th>\n",
       "      <th>insur_3</th>\n",
       "      <th>insur_4</th>\n",
       "      <th>family_income</th>\n",
       "      <th>family_m</th>\n",
       "      <th>family_status</th>\n",
       "      <th>house</th>\n",
       "      <th>car</th>\n",
       "      <th>invest_0</th>\n",
       "      <th>invest_1</th>\n",
       "      <th>invest_2</th>\n",
       "      <th>invest_3</th>\n",
       "      <th>invest_4</th>\n",
       "      <th>invest_5</th>\n",
       "      <th>invest_6</th>\n",
       "      <th>invest_7</th>\n",
       "      <th>invest_8</th>\n",
       "      <th>invest_other</th>\n",
       "      <th>son</th>\n",
       "      <th>daughter</th>\n",
       "      <th>minor_child</th>\n",
       "      <th>marital</th>\n",
       "      <th>marital_1st</th>\n",
       "      <th>s_birth</th>\n",
       "      <th>marital_now</th>\n",
       "      <th>s_edu</th>\n",
       "      <th>s_political</th>\n",
       "      <th>s_hukou</th>\n",
       "      <th>s_income</th>\n",
       "      <th>s_work_exper</th>\n",
       "      <th>s_work_status</th>\n",
       "      <th>s_work_type</th>\n",
       "      <th>f_birth</th>\n",
       "      <th>f_edu</th>\n",
       "      <th>f_political</th>\n",
       "      <th>f_work_14</th>\n",
       "      <th>m_birth</th>\n",
       "      <th>m_edu</th>\n",
       "      <th>m_political</th>\n",
       "      <th>m_work_14</th>\n",
       "      <th>status_peer</th>\n",
       "      <th>status_3_before</th>\n",
       "      <th>view</th>\n",
       "      <th>inc_ability</th>\n",
       "      <th>inc_exp</th>\n",
       "      <th>trust_1</th>\n",
       "      <th>trust_2</th>\n",
       "      <th>trust_3</th>\n",
       "      <th>trust_4</th>\n",
       "      <th>trust_5</th>\n",
       "      <th>trust_6</th>\n",
       "      <th>trust_7</th>\n",
       "      <th>trust_8</th>\n",
       "      <th>trust_9</th>\n",
       "      <th>trust_10</th>\n",
       "      <th>trust_11</th>\n",
       "      <th>trust_12</th>\n",
       "      <th>trust_13</th>\n",
       "      <th>neighbor_familiarity</th>\n",
       "      <th>public_service_1</th>\n",
       "      <th>public_service_2</th>\n",
       "      <th>public_service_3</th>\n",
       "      <th>public_service_4</th>\n",
       "      <th>public_service_5</th>\n",
       "      <th>public_service_6</th>\n",
       "      <th>public_service_7</th>\n",
       "      <th>public_service_8</th>\n",
       "      <th>public_service_9</th>\n",
       "      <th>neg1</th>\n",
       "      <th>neg2</th>\n",
       "      <th>neg3</th>\n",
       "      <th>neg4</th>\n",
       "      <th>neg5</th>\n",
       "      <th>age</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>marital_1stbir</th>\n",
       "      <th>marital_nowtbir</th>\n",
       "      <th>mar</th>\n",
       "      <th>marital_sbir</th>\n",
       "      <th>age_</th>\n",
       "      <th>income/s_income</th>\n",
       "      <th>income+s_income</th>\n",
       "      <th>income/family_income</th>\n",
       "      <th>all_income/family_income</th>\n",
       "      <th>income/inc_exp</th>\n",
       "      <th>family_income/m</th>\n",
       "      <th>income/m</th>\n",
       "      <th>income/floor_area</th>\n",
       "      <th>all_income/floor_area</th>\n",
       "      <th>family_income/floor_area</th>\n",
       "      <th>floor_area/m</th>\n",
       "      <th>class_10_diff</th>\n",
       "      <th>class_diff</th>\n",
       "      <th>class_14_diff</th>\n",
       "      <th>leisure_sum</th>\n",
       "      <th>public_service_sum</th>\n",
       "      <th>trust_sum</th>\n",
       "      <th>province_income_mean</th>\n",
       "      <th>province_family_income_mean</th>\n",
       "      <th>province_equity_mean</th>\n",
       "      <th>province_depression_mean</th>\n",
       "      <th>province_floor_area_mean</th>\n",
       "      <th>province_health_mean</th>\n",
       "      <th>province_class_10_diff_mean</th>\n",
       "      <th>province_class_mean</th>\n",
       "      <th>province_health_problem_mean</th>\n",
       "      <th>province_family_status_mean</th>\n",
       "      <th>province_leisure_sum_mean</th>\n",
       "      <th>province_public_service_sum_mean</th>\n",
       "      <th>province_trust_sum_mean</th>\n",
       "      <th>city_income_mean</th>\n",
       "      <th>city_family_income_mean</th>\n",
       "      <th>city_equity_mean</th>\n",
       "      <th>city_depression_mean</th>\n",
       "      <th>city_floor_area_mean</th>\n",
       "      <th>city_health_mean</th>\n",
       "      <th>city_class_10_diff_mean</th>\n",
       "      <th>city_class_mean</th>\n",
       "      <th>city_health_problem_mean</th>\n",
       "      <th>city_family_status_mean</th>\n",
       "      <th>city_leisure_sum_mean</th>\n",
       "      <th>city_public_service_sum_mean</th>\n",
       "      <th>city_trust_sum_mean</th>\n",
       "      <th>county_income_mean</th>\n",
       "      <th>county_family_income_mean</th>\n",
       "      <th>county_equity_mean</th>\n",
       "      <th>county_depression_mean</th>\n",
       "      <th>county_floor_area_mean</th>\n",
       "      <th>county_health_mean</th>\n",
       "      <th>county_class_10_diff_mean</th>\n",
       "      <th>county_class_mean</th>\n",
       "      <th>county_health_problem_mean</th>\n",
       "      <th>county_family_status_mean</th>\n",
       "      <th>county_leisure_sum_mean</th>\n",
       "      <th>county_public_service_sum_mean</th>\n",
       "      <th>county_trust_sum_mean</th>\n",
       "      <th>income/province</th>\n",
       "      <th>family_income/province</th>\n",
       "      <th>equity/province</th>\n",
       "      <th>depression/province</th>\n",
       "      <th>floor_area/province</th>\n",
       "      <th>health/province</th>\n",
       "      <th>class_10_diff/province</th>\n",
       "      <th>class/province</th>\n",
       "      <th>health_problem/province</th>\n",
       "      <th>family_status/province</th>\n",
       "      <th>leisure_sum/province</th>\n",
       "      <th>public_service_sum/province</th>\n",
       "      <th>trust_sum/province</th>\n",
       "      <th>income/city</th>\n",
       "      <th>family_income/city</th>\n",
       "      <th>equity/city</th>\n",
       "      <th>depression/city</th>\n",
       "      <th>floor_area/city</th>\n",
       "      <th>health/city</th>\n",
       "      <th>class_10_diff/city</th>\n",
       "      <th>class/city</th>\n",
       "      <th>health_problem/city</th>\n",
       "      <th>family_status/city</th>\n",
       "      <th>leisure_sum/city</th>\n",
       "      <th>public_service_sum/city</th>\n",
       "      <th>trust_sum/city</th>\n",
       "      <th>income/county</th>\n",
       "      <th>family_income/county</th>\n",
       "      <th>equity/county</th>\n",
       "      <th>depression/county</th>\n",
       "      <th>floor_area/county</th>\n",
       "      <th>health/county</th>\n",
       "      <th>class_10_diff/county</th>\n",
       "      <th>class/county</th>\n",
       "      <th>health_problem/county</th>\n",
       "      <th>family_status/county</th>\n",
       "      <th>leisure_sum/county</th>\n",
       "      <th>public_service_sum/county</th>\n",
       "      <th>trust_sum/county</th>\n",
       "      <th>age_income_mean</th>\n",
       "      <th>age_family_income_mean</th>\n",
       "      <th>age_equity_mean</th>\n",
       "      <th>age_depression_mean</th>\n",
       "      <th>age_floor_area_mean</th>\n",
       "      <th>age_health_mean</th>\n",
       "      <th>age_class_10_diff_mean</th>\n",
       "      <th>age_class_mean</th>\n",
       "      <th>age_health_problem_mean</th>\n",
       "      <th>age_family_status_mean</th>\n",
       "      <th>age_leisure_sum_mean</th>\n",
       "      <th>age_public_service_sum_mean</th>\n",
       "      <th>age_trust_sum_mean</th>\n",
       "      <th>income/age</th>\n",
       "      <th>family_income/age</th>\n",
       "      <th>equity/age</th>\n",
       "      <th>depression/age</th>\n",
       "      <th>floor_area/age</th>\n",
       "      <th>health/age</th>\n",
       "      <th>class_10_diff/age</th>\n",
       "      <th>class/age</th>\n",
       "      <th>health_problem/age</th>\n",
       "      <th>family_status/age</th>\n",
       "      <th>leisure_sum/age</th>\n",
       "      <th>public_service_sum/age</th>\n",
       "      <th>trust_sum/age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>29850.746269</td>\n",
       "      <td>9950.248756</td>\n",
       "      <td>444.345701</td>\n",
       "      <td>1333.037103</td>\n",
       "      <td>1333.037103</td>\n",
       "      <td>22.388060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>61859.505703</td>\n",
       "      <td>131638.458098</td>\n",
       "      <td>2.773764</td>\n",
       "      <td>3.954373</td>\n",
       "      <td>88.692205</td>\n",
       "      <td>3.701521</td>\n",
       "      <td>1.127376</td>\n",
       "      <td>4.572243</td>\n",
       "      <td>4.051331</td>\n",
       "      <td>2.716730</td>\n",
       "      <td>41.701521</td>\n",
       "      <td>506.456274</td>\n",
       "      <td>35.511407</td>\n",
       "      <td>43096.656716</td>\n",
       "      <td>93764.077651</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>4.011940</td>\n",
       "      <td>82.744776</td>\n",
       "      <td>3.650746</td>\n",
       "      <td>1.164179</td>\n",
       "      <td>4.465672</td>\n",
       "      <td>4.113433</td>\n",
       "      <td>2.665672</td>\n",
       "      <td>40.059701</td>\n",
       "      <td>542.671642</td>\n",
       "      <td>34.955224</td>\n",
       "      <td>28979.591837</td>\n",
       "      <td>60630.401904</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>4.081633</td>\n",
       "      <td>38.530612</td>\n",
       "      <td>3.489796</td>\n",
       "      <td>1.183673</td>\n",
       "      <td>4.938776</td>\n",
       "      <td>3.877551</td>\n",
       "      <td>2.489796</td>\n",
       "      <td>36.693878</td>\n",
       "      <td>559.163265</td>\n",
       "      <td>31.346939</td>\n",
       "      <td>0.323313</td>\n",
       "      <td>0.455794</td>\n",
       "      <td>1.081563</td>\n",
       "      <td>1.264423</td>\n",
       "      <td>0.507373</td>\n",
       "      <td>0.810478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656133</td>\n",
       "      <td>0.493665</td>\n",
       "      <td>0.736179</td>\n",
       "      <td>0.791338</td>\n",
       "      <td>0.829292</td>\n",
       "      <td>0.849050</td>\n",
       "      <td>0.464073</td>\n",
       "      <td>0.639904</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.246280</td>\n",
       "      <td>0.543841</td>\n",
       "      <td>0.821750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671791</td>\n",
       "      <td>0.486212</td>\n",
       "      <td>0.750280</td>\n",
       "      <td>0.823770</td>\n",
       "      <td>0.773949</td>\n",
       "      <td>0.886849</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.989603</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>1.167903</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.899333</td>\n",
       "      <td>0.751122</td>\n",
       "      <td>0.988932</td>\n",
       "      <td>24371.808219</td>\n",
       "      <td>68176.681796</td>\n",
       "      <td>3.061644</td>\n",
       "      <td>3.890411</td>\n",
       "      <td>109.662329</td>\n",
       "      <td>3.534247</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>4.390411</td>\n",
       "      <td>3.835616</td>\n",
       "      <td>2.726027</td>\n",
       "      <td>45.541096</td>\n",
       "      <td>608.657534</td>\n",
       "      <td>36.054795</td>\n",
       "      <td>0.820620</td>\n",
       "      <td>0.880066</td>\n",
       "      <td>0.979866</td>\n",
       "      <td>1.285211</td>\n",
       "      <td>0.410351</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683307</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>0.733668</td>\n",
       "      <td>0.724620</td>\n",
       "      <td>0.690043</td>\n",
       "      <td>0.859802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1972</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1973</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20001.0</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>13289.036545</td>\n",
       "      <td>6644.518272</td>\n",
       "      <td>181.801654</td>\n",
       "      <td>181.801654</td>\n",
       "      <td>363.603309</td>\n",
       "      <td>36.544850</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>29806.230241</td>\n",
       "      <td>37758.898778</td>\n",
       "      <td>3.249141</td>\n",
       "      <td>3.991409</td>\n",
       "      <td>148.463918</td>\n",
       "      <td>3.955326</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>4.414089</td>\n",
       "      <td>3.960481</td>\n",
       "      <td>2.737113</td>\n",
       "      <td>47.194158</td>\n",
       "      <td>625.233677</td>\n",
       "      <td>41.434708</td>\n",
       "      <td>64016.125654</td>\n",
       "      <td>34201.145410</td>\n",
       "      <td>3.371728</td>\n",
       "      <td>3.989529</td>\n",
       "      <td>157.528796</td>\n",
       "      <td>3.968586</td>\n",
       "      <td>0.942408</td>\n",
       "      <td>4.335079</td>\n",
       "      <td>3.963351</td>\n",
       "      <td>2.811518</td>\n",
       "      <td>48.303665</td>\n",
       "      <td>634.460733</td>\n",
       "      <td>41.732984</td>\n",
       "      <td>11628.659794</td>\n",
       "      <td>34758.996059</td>\n",
       "      <td>3.350515</td>\n",
       "      <td>3.958763</td>\n",
       "      <td>154.608247</td>\n",
       "      <td>3.927835</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>4.123711</td>\n",
       "      <td>3.948454</td>\n",
       "      <td>2.752577</td>\n",
       "      <td>48.628866</td>\n",
       "      <td>629.123711</td>\n",
       "      <td>42.731959</td>\n",
       "      <td>0.671001</td>\n",
       "      <td>1.059353</td>\n",
       "      <td>0.923321</td>\n",
       "      <td>0.751614</td>\n",
       "      <td>0.740921</td>\n",
       "      <td>1.264118</td>\n",
       "      <td>2.063830</td>\n",
       "      <td>1.359284</td>\n",
       "      <td>1.009978</td>\n",
       "      <td>1.461394</td>\n",
       "      <td>0.826373</td>\n",
       "      <td>1.079596</td>\n",
       "      <td>1.013321</td>\n",
       "      <td>0.312421</td>\n",
       "      <td>1.169551</td>\n",
       "      <td>0.889752</td>\n",
       "      <td>0.751969</td>\n",
       "      <td>0.698285</td>\n",
       "      <td>1.259894</td>\n",
       "      <td>2.122222</td>\n",
       "      <td>1.384058</td>\n",
       "      <td>1.009247</td>\n",
       "      <td>1.422719</td>\n",
       "      <td>0.807392</td>\n",
       "      <td>1.063896</td>\n",
       "      <td>1.030360</td>\n",
       "      <td>1.719889</td>\n",
       "      <td>1.150781</td>\n",
       "      <td>0.895385</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.711476</td>\n",
       "      <td>1.272966</td>\n",
       "      <td>2.179775</td>\n",
       "      <td>1.455000</td>\n",
       "      <td>1.013055</td>\n",
       "      <td>1.453184</td>\n",
       "      <td>0.801993</td>\n",
       "      <td>1.072921</td>\n",
       "      <td>1.006273</td>\n",
       "      <td>20771.900826</td>\n",
       "      <td>71277.916142</td>\n",
       "      <td>3.157025</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>115.446281</td>\n",
       "      <td>4.239669</td>\n",
       "      <td>1.975207</td>\n",
       "      <td>4.462810</td>\n",
       "      <td>4.487603</td>\n",
       "      <td>2.942149</td>\n",
       "      <td>38.545455</td>\n",
       "      <td>579.082645</td>\n",
       "      <td>39.107438</td>\n",
       "      <td>0.962839</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.950262</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.952824</td>\n",
       "      <td>1.179337</td>\n",
       "      <td>1.012552</td>\n",
       "      <td>1.344444</td>\n",
       "      <td>0.891344</td>\n",
       "      <td>1.359551</td>\n",
       "      <td>1.011792</td>\n",
       "      <td>1.165637</td>\n",
       "      <td>1.099535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>126</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333278</td>\n",
       "      <td>8001.0</td>\n",
       "      <td>0.249969</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>2657.807309</td>\n",
       "      <td>664.451827</td>\n",
       "      <td>16.665278</td>\n",
       "      <td>66.661112</td>\n",
       "      <td>66.661112</td>\n",
       "      <td>39.867110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16246.747967</td>\n",
       "      <td>95335.856164</td>\n",
       "      <td>3.298103</td>\n",
       "      <td>3.647696</td>\n",
       "      <td>107.902439</td>\n",
       "      <td>3.582656</td>\n",
       "      <td>1.029810</td>\n",
       "      <td>4.243902</td>\n",
       "      <td>3.596206</td>\n",
       "      <td>2.607046</td>\n",
       "      <td>46.653117</td>\n",
       "      <td>623.013550</td>\n",
       "      <td>36.200542</td>\n",
       "      <td>6522.105263</td>\n",
       "      <td>32477.297170</td>\n",
       "      <td>3.294737</td>\n",
       "      <td>3.957895</td>\n",
       "      <td>120.684211</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.505263</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>2.768421</td>\n",
       "      <td>48.263158</td>\n",
       "      <td>654.547368</td>\n",
       "      <td>38.778947</td>\n",
       "      <td>6522.105263</td>\n",
       "      <td>32477.297170</td>\n",
       "      <td>3.294737</td>\n",
       "      <td>3.957895</td>\n",
       "      <td>120.684211</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.505263</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>2.768421</td>\n",
       "      <td>48.263158</td>\n",
       "      <td>654.547368</td>\n",
       "      <td>38.778947</td>\n",
       "      <td>0.123102</td>\n",
       "      <td>0.083914</td>\n",
       "      <td>1.212818</td>\n",
       "      <td>1.370728</td>\n",
       "      <td>1.112116</td>\n",
       "      <td>1.116490</td>\n",
       "      <td>0.971053</td>\n",
       "      <td>1.178161</td>\n",
       "      <td>1.112283</td>\n",
       "      <td>1.150728</td>\n",
       "      <td>0.964566</td>\n",
       "      <td>1.202221</td>\n",
       "      <td>0.940847</td>\n",
       "      <td>0.306649</td>\n",
       "      <td>0.246326</td>\n",
       "      <td>1.214058</td>\n",
       "      <td>1.263298</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.109813</td>\n",
       "      <td>1.041096</td>\n",
       "      <td>1.083650</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>1.144302</td>\n",
       "      <td>0.902552</td>\n",
       "      <td>0.306649</td>\n",
       "      <td>0.246326</td>\n",
       "      <td>1.214058</td>\n",
       "      <td>1.263298</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.109813</td>\n",
       "      <td>1.041096</td>\n",
       "      <td>1.083650</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>1.144302</td>\n",
       "      <td>0.902552</td>\n",
       "      <td>23468.987342</td>\n",
       "      <td>51169.247670</td>\n",
       "      <td>3.025316</td>\n",
       "      <td>3.721519</td>\n",
       "      <td>123.415190</td>\n",
       "      <td>3.476793</td>\n",
       "      <td>0.839662</td>\n",
       "      <td>4.181435</td>\n",
       "      <td>3.789030</td>\n",
       "      <td>2.518987</td>\n",
       "      <td>46.561181</td>\n",
       "      <td>607.130802</td>\n",
       "      <td>37.772152</td>\n",
       "      <td>0.085219</td>\n",
       "      <td>0.156344</td>\n",
       "      <td>1.322176</td>\n",
       "      <td>1.343537</td>\n",
       "      <td>0.972328</td>\n",
       "      <td>1.150485</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>1.195762</td>\n",
       "      <td>1.055679</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>0.966470</td>\n",
       "      <td>1.233672</td>\n",
       "      <td>0.926609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>6420</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12000.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1943.0</td>\n",
       "      <td>-1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1943.0</td>\n",
       "      <td>6420.000000</td>\n",
       "      <td>6421.0</td>\n",
       "      <td>0.534955</td>\n",
       "      <td>0.534955</td>\n",
       "      <td>0.641936</td>\n",
       "      <td>3986.710963</td>\n",
       "      <td>2132.890365</td>\n",
       "      <td>82.297141</td>\n",
       "      <td>82.297141</td>\n",
       "      <td>153.826433</td>\n",
       "      <td>25.913621</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>26771.017391</td>\n",
       "      <td>61036.929244</td>\n",
       "      <td>3.401739</td>\n",
       "      <td>3.954783</td>\n",
       "      <td>113.826087</td>\n",
       "      <td>3.827826</td>\n",
       "      <td>0.918261</td>\n",
       "      <td>4.577391</td>\n",
       "      <td>3.923478</td>\n",
       "      <td>2.772174</td>\n",
       "      <td>45.375652</td>\n",
       "      <td>655.568696</td>\n",
       "      <td>42.198261</td>\n",
       "      <td>45077.070707</td>\n",
       "      <td>102113.496521</td>\n",
       "      <td>3.252525</td>\n",
       "      <td>4.030303</td>\n",
       "      <td>98.858586</td>\n",
       "      <td>4.070707</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>4.464646</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>2.696970</td>\n",
       "      <td>42.727273</td>\n",
       "      <td>631.636364</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>45077.070707</td>\n",
       "      <td>102113.496521</td>\n",
       "      <td>3.252525</td>\n",
       "      <td>4.030303</td>\n",
       "      <td>98.858586</td>\n",
       "      <td>4.070707</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>4.464646</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>2.696970</td>\n",
       "      <td>42.727273</td>\n",
       "      <td>631.636364</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>0.239812</td>\n",
       "      <td>0.196602</td>\n",
       "      <td>1.175869</td>\n",
       "      <td>1.011434</td>\n",
       "      <td>0.685256</td>\n",
       "      <td>1.044980</td>\n",
       "      <td>2.178030</td>\n",
       "      <td>1.092325</td>\n",
       "      <td>1.019504</td>\n",
       "      <td>1.082183</td>\n",
       "      <td>0.947645</td>\n",
       "      <td>1.159299</td>\n",
       "      <td>1.018560</td>\n",
       "      <td>0.142423</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>1.229814</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.789006</td>\n",
       "      <td>0.982630</td>\n",
       "      <td>2.869565</td>\n",
       "      <td>1.119910</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.112360</td>\n",
       "      <td>1.006383</td>\n",
       "      <td>1.203224</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>0.142423</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>1.229814</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.789006</td>\n",
       "      <td>0.982630</td>\n",
       "      <td>2.869565</td>\n",
       "      <td>1.119910</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.112360</td>\n",
       "      <td>1.006383</td>\n",
       "      <td>1.203224</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>15072.375000</td>\n",
       "      <td>123040.259642</td>\n",
       "      <td>3.348214</td>\n",
       "      <td>3.598214</td>\n",
       "      <td>121.433036</td>\n",
       "      <td>3.133929</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>4.169643</td>\n",
       "      <td>3.366071</td>\n",
       "      <td>2.580357</td>\n",
       "      <td>47.812500</td>\n",
       "      <td>607.258929</td>\n",
       "      <td>36.857143</td>\n",
       "      <td>0.425945</td>\n",
       "      <td>0.097529</td>\n",
       "      <td>1.194667</td>\n",
       "      <td>1.111663</td>\n",
       "      <td>0.642329</td>\n",
       "      <td>1.276353</td>\n",
       "      <td>4.977778</td>\n",
       "      <td>1.199143</td>\n",
       "      <td>1.188329</td>\n",
       "      <td>1.162630</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>1.251525</td>\n",
       "      <td>1.193798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>66363.231106</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1970</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1972</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16549.434191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>947.910743</td>\n",
       "      <td>17.456359</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>33930.569444</td>\n",
       "      <td>73738.595633</td>\n",
       "      <td>3.309028</td>\n",
       "      <td>4.093750</td>\n",
       "      <td>57.526389</td>\n",
       "      <td>3.774306</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.729167</td>\n",
       "      <td>44.447917</td>\n",
       "      <td>626.340278</td>\n",
       "      <td>32.631944</td>\n",
       "      <td>33930.569444</td>\n",
       "      <td>73738.595633</td>\n",
       "      <td>3.309028</td>\n",
       "      <td>4.093750</td>\n",
       "      <td>57.526389</td>\n",
       "      <td>3.774306</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.729167</td>\n",
       "      <td>44.447917</td>\n",
       "      <td>626.340278</td>\n",
       "      <td>32.631944</td>\n",
       "      <td>28430.769231</td>\n",
       "      <td>85328.619373</td>\n",
       "      <td>3.589744</td>\n",
       "      <td>3.974359</td>\n",
       "      <td>57.553846</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.641026</td>\n",
       "      <td>4.179487</td>\n",
       "      <td>3.025641</td>\n",
       "      <td>44.846154</td>\n",
       "      <td>513.076923</td>\n",
       "      <td>39.256410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899980</td>\n",
       "      <td>0.604407</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>1.216833</td>\n",
       "      <td>1.324747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.212121</td>\n",
       "      <td>1.099237</td>\n",
       "      <td>0.899930</td>\n",
       "      <td>0.718459</td>\n",
       "      <td>1.308280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899980</td>\n",
       "      <td>0.604407</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>1.216833</td>\n",
       "      <td>1.324747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.212121</td>\n",
       "      <td>1.099237</td>\n",
       "      <td>0.899930</td>\n",
       "      <td>0.718459</td>\n",
       "      <td>1.348372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777737</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.754839</td>\n",
       "      <td>1.216252</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274648</td>\n",
       "      <td>1.196319</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.891938</td>\n",
       "      <td>0.877061</td>\n",
       "      <td>1.120836</td>\n",
       "      <td>14158.165138</td>\n",
       "      <td>69149.961916</td>\n",
       "      <td>3.082569</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>119.192661</td>\n",
       "      <td>4.247706</td>\n",
       "      <td>1.779817</td>\n",
       "      <td>4.220183</td>\n",
       "      <td>4.477064</td>\n",
       "      <td>2.743119</td>\n",
       "      <td>38.266055</td>\n",
       "      <td>595.243119</td>\n",
       "      <td>41.743119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959700</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.587284</td>\n",
       "      <td>1.177106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236957</td>\n",
       "      <td>1.116803</td>\n",
       "      <td>1.093645</td>\n",
       "      <td>1.045313</td>\n",
       "      <td>0.755994</td>\n",
       "      <td>1.054066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  survey_type  province  city  county  survey_time  gender  birth  \\\n",
       "0   1            1        12    32      59         2015       1   1959   \n",
       "1   2            2        18    52      85         2015       1   1992   \n",
       "2   3            2        29    83     126         2015       2   1967   \n",
       "3   4            2        10    28      51         2015       2   1943   \n",
       "4   5            1         7    18      36         2015       2   1994   \n",
       "\n",
       "   nationality  religion  religion_freq  edu edu_other  edu_status  edu_yr  \\\n",
       "0            1         1              1   11         0         4.0     0.0   \n",
       "1            1         1              1   12         0         4.0  2013.0   \n",
       "2            1         0              3    4         0         4.0     0.0   \n",
       "3            1         1              1    3         0         4.0  1959.0   \n",
       "4            1         1              1   12         0         1.0  2014.0   \n",
       "\n",
       "   income  political  join_party  floor_area  property_0  property_1  \\\n",
       "0   20000          1         0.0        45.0           0           1   \n",
       "1   20000          1         0.0       110.0           0           0   \n",
       "2    2000          1         0.0       120.0           0           1   \n",
       "3    6420          1         0.0        78.0           0           0   \n",
       "4       0          2         0.0        70.0           0           0   \n",
       "\n",
       "   property_2  property_3  property_4  property_5  property_6  property_7  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           1           0           0           0   \n",
       "2           1           0           0           0           0           0   \n",
       "3           0           1           0           0           0           0   \n",
       "4           0           0           1           0           0           0   \n",
       "\n",
       "   property_8 property_other  height_cm  weight_jin  health  health_problem  \\\n",
       "0           0              0        176         155       3               2   \n",
       "1           0              0        170         110       5               4   \n",
       "2           0              0        160         122       4               4   \n",
       "3           0              0        163         170       4               4   \n",
       "4           0              0        165         110       5               5   \n",
       "\n",
       "   depression  hukou  hukou_loc  media_1  media_2  media_3  media_4  media_5  \\\n",
       "0           5      5        2.0        4        2        5        5        4   \n",
       "1           3      1        1.0        2        2        1        3        5   \n",
       "2           5      1        1.0        2        2        2        5        1   \n",
       "3           4      1        2.0        2        1        1        5        1   \n",
       "4           3      2        3.0        1        3        4        2        5   \n",
       "\n",
       "   media_6  leisure_1  leisure_2  leisure_3  leisure_4  leisure_5  leisure_6  \\\n",
       "0        3          1          4          3        1.0        2.0        3.0   \n",
       "1        1          2          3          4        3.0        5.0        4.0   \n",
       "2        3          1          4          4        3.0        5.0        4.0   \n",
       "3        1          1          5          2        4.0        5.0        4.0   \n",
       "4        5          3          3          3        2.0        4.0        4.0   \n",
       "\n",
       "   leisure_7  leisure_8  leisure_9  leisure_10  leisure_11  leisure_12  \\\n",
       "0        4.0        1.0        4.0         5.0         4.0         1.0   \n",
       "1        3.0        2.0        3.0         4.0         5.0         1.0   \n",
       "2        4.0        2.0        3.0         5.0         5.0         5.0   \n",
       "3        5.0        1.0        1.0         5.0         5.0         5.0   \n",
       "4        3.0        5.0        2.0         5.0         5.0         1.0   \n",
       "\n",
       "   socialize  relax  learn  social_neighbor  social_friend  socia_outing  \\\n",
       "0          2      4      3              3.0            3.0             2   \n",
       "1          2      4      3              6.0            2.0             1   \n",
       "2          3      4      2              2.0            5.0             2   \n",
       "3          2      4      4              1.0            6.0             1   \n",
       "4          4      3      4              7.0            5.0             3   \n",
       "\n",
       "   equity  class  class_10_before  class_10_after  class_14  work_exper  \\\n",
       "0       3      3                3               3         1           1   \n",
       "1       3      6                4               8         5           1   \n",
       "2       4      5                4               6         3           2   \n",
       "3       4      5                5               7         2           4   \n",
       "4       2      1                1               1         4           6   \n",
       "\n",
       "   work_status  work_yr  work_type  work_manage  insur_1  insur_2  insur_3  \\\n",
       "0          3.0     30.0        1.0          2.0        1        1        1   \n",
       "1          3.0      2.0        1.0          3.0        1        1        1   \n",
       "2          0.0      0.0        0.0          0.0        1        1        2   \n",
       "3          0.0      0.0        0.0          0.0        2        2        2   \n",
       "4          0.0      0.0        0.0          0.0        1        2        2   \n",
       "\n",
       "   insur_4  family_income  family_m  family_status  house  car  invest_0  \\\n",
       "0        2   60000.000000         2              2      1    0         0   \n",
       "1        1   40000.000000         3              4      1    0         0   \n",
       "2        2    8000.000000         3              3      1    0         0   \n",
       "3        2   12000.000000         3              3      1    1         0   \n",
       "4        2   66363.231106         4              3      1    1         0   \n",
       "\n",
       "   invest_1  invest_2  invest_3  invest_4  invest_5  invest_6  invest_7  \\\n",
       "0         1         0         0         0         0         0         0   \n",
       "1         1         0         0         0         0         0         0   \n",
       "2         1         0         0         0         0         0         0   \n",
       "3         1         0         0         0         0         0         0   \n",
       "4         1         0         0         0         0         0         0   \n",
       "\n",
       "   invest_8 invest_other  son  daughter  minor_child  marital  marital_1st  \\\n",
       "0         0            0    1         0          0.0        3       1984.0   \n",
       "1         0            0    0         0          0.0        1          0.0   \n",
       "2         0            0    0         2          1.0        3       1990.0   \n",
       "3         0            0    1         4          0.0        7       1960.0   \n",
       "4         0            0    0         0          0.0        1          0.0   \n",
       "\n",
       "   s_birth  marital_now  s_edu  s_political  s_hukou  s_income  s_work_exper  \\\n",
       "0   1958.0       1984.0    6.0          1.0      5.0   40000.0           5.0   \n",
       "1      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "2   1968.0       1990.0    3.0          1.0      1.0    6000.0           3.0   \n",
       "3      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "4      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "\n",
       "   s_work_status  s_work_type  f_birth  f_edu  f_political  f_work_14  \\\n",
       "0            0.0          0.0     1945      4            4          1   \n",
       "1            0.0          0.0     1972      3            1          2   \n",
       "2            0.0          0.0     1945      1            1          2   \n",
       "3            0.0          0.0     1945     14            1          2   \n",
       "4            0.0          0.0     1970      6            1         10   \n",
       "\n",
       "   m_birth  m_edu  m_political  m_work_14  status_peer  status_3_before  view  \\\n",
       "0     1940      4            1          1            3                2     4   \n",
       "1     1973      3            1          2            1                1     4   \n",
       "2     1940      1            1          2            2                1     4   \n",
       "3     1940      1            1          2            2                1     3   \n",
       "4     1972      4            1         15            3                2     3   \n",
       "\n",
       "   inc_ability   inc_exp  trust_1  trust_2  trust_3  trust_4  trust_5  \\\n",
       "0            3   50000.0      4.0      2.0      4.0      4.0      5.0   \n",
       "1            2   50000.0      5.0      4.0      4.0      3.0      5.0   \n",
       "2            2   80000.0      3.0      3.0      3.0      3.0      4.0   \n",
       "3            2   10000.0      3.0      3.0      4.0      3.0      5.0   \n",
       "4            2  200000.0      4.0      3.0      3.0      3.0      5.0   \n",
       "\n",
       "   trust_6  trust_7  trust_8  trust_9  trust_10  trust_11  trust_12  trust_13  \\\n",
       "0      3.0      2.0      3.0      4.0       3.0      -8.0       4.0       1.0   \n",
       "1      3.0      3.0      3.0      2.0       3.0       3.0       3.0       2.0   \n",
       "2      3.0      3.0      3.0      3.0       3.0       0.0       3.0       1.0   \n",
       "3      3.0      3.0      5.0      4.0       3.0       3.0       3.0       2.0   \n",
       "4      5.0      3.0      4.0      3.0       3.0       3.0       3.0       2.0   \n",
       "\n",
       "   neighbor_familiarity  public_service_1  public_service_2  public_service_3  \\\n",
       "0                     4              50.0              60.0              50.0   \n",
       "1                     3              90.0              70.0              70.0   \n",
       "2                     4              90.0              80.0              75.0   \n",
       "3                     3             100.0              90.0              70.0   \n",
       "4                     2              50.0              50.0              50.0   \n",
       "\n",
       "   public_service_4  public_service_5  public_service_6  public_service_7  \\\n",
       "0              50.0              30.0              30.0              50.0   \n",
       "1              80.0              85.0              70.0              90.0   \n",
       "2              79.0              80.0              90.0              90.0   \n",
       "3              80.0              80.0              90.0              90.0   \n",
       "4              50.0              50.0              50.0              50.0   \n",
       "\n",
       "   public_service_8  public_service_9  neg1  neg2  neg3  neg4  neg5  age  \\\n",
       "0              50.0              50.0     5     3     0     2     0   56   \n",
       "1              60.0              60.0     0     0     0     0     0   23   \n",
       "2              90.0              75.0     3     1     0     2     0   48   \n",
       "3              80.0              80.0     2     0     0     2     0   72   \n",
       "4              50.0              50.0     2     1     1     0     0   21   \n",
       "\n",
       "  age_bin  marital_1stbir  marital_nowtbir     mar  marital_sbir    age_  \\\n",
       "0       4            25.0             25.0     0.0          26.0    -1.0   \n",
       "1       1         -1992.0          -1992.0     0.0           0.0 -1992.0   \n",
       "2       3            23.0             23.0     0.0          22.0     1.0   \n",
       "3       5            17.0          -1943.0 -1960.0           0.0 -1943.0   \n",
       "4       1         -1994.0          -1994.0     0.0           0.0 -1994.0   \n",
       "\n",
       "   income/s_income  income+s_income  income/family_income  \\\n",
       "0         0.499988          60001.0              0.333328   \n",
       "1     20000.000000          20001.0              0.499988   \n",
       "2         0.333278           8001.0              0.249969   \n",
       "3      6420.000000           6421.0              0.534955   \n",
       "4         0.000000              1.0              0.000000   \n",
       "\n",
       "   all_income/family_income  income/inc_exp  family_income/m     income/m  \\\n",
       "0                  0.999983        0.399992     29850.746269  9950.248756   \n",
       "1                  0.499988        0.399992     13289.036545  6644.518272   \n",
       "2                  0.999875        0.025000      2657.807309   664.451827   \n",
       "3                  0.534955        0.641936      3986.710963  2132.890365   \n",
       "4                  0.000000        0.000000     16549.434191     0.000000   \n",
       "\n",
       "   income/floor_area  all_income/floor_area  family_income/floor_area  \\\n",
       "0         444.345701            1333.037103               1333.037103   \n",
       "1         181.801654             181.801654                363.603309   \n",
       "2          16.665278              66.661112                 66.661112   \n",
       "3          82.297141              82.297141                153.826433   \n",
       "4           0.000000               0.000000                947.910743   \n",
       "\n",
       "   floor_area/m  class_10_diff  class_diff  class_14_diff  leisure_sum  \\\n",
       "0     22.388060              0           0              2         33.0   \n",
       "1     36.544850              2           2              1         39.0   \n",
       "2     39.867110              1           1              2         45.0   \n",
       "3     25.913621              2           0              3         43.0   \n",
       "4     17.456359              0           0             -3         40.0   \n",
       "\n",
       "   public_service_sum  trust_sum  province_income_mean  \\\n",
       "0               420.0       31.0          61859.505703   \n",
       "1               675.0       43.0          29806.230241   \n",
       "2               749.0       35.0          16246.747967   \n",
       "3               760.0       44.0          26771.017391   \n",
       "4               450.0       44.0          33930.569444   \n",
       "\n",
       "   province_family_income_mean  province_equity_mean  \\\n",
       "0                131638.458098              2.773764   \n",
       "1                 37758.898778              3.249141   \n",
       "2                 95335.856164              3.298103   \n",
       "3                 61036.929244              3.401739   \n",
       "4                 73738.595633              3.309028   \n",
       "\n",
       "   province_depression_mean  province_floor_area_mean  province_health_mean  \\\n",
       "0                  3.954373                 88.692205              3.701521   \n",
       "1                  3.991409                148.463918              3.955326   \n",
       "2                  3.647696                107.902439              3.582656   \n",
       "3                  3.954783                113.826087              3.827826   \n",
       "4                  4.093750                 57.526389              3.774306   \n",
       "\n",
       "   province_class_10_diff_mean  province_class_mean  \\\n",
       "0                     1.127376             4.572243   \n",
       "1                     0.969072             4.414089   \n",
       "2                     1.029810             4.243902   \n",
       "3                     0.918261             4.577391   \n",
       "4                     0.527778             4.444444   \n",
       "\n",
       "   province_health_problem_mean  province_family_status_mean  \\\n",
       "0                      4.051331                     2.716730   \n",
       "1                      3.960481                     2.737113   \n",
       "2                      3.596206                     2.607046   \n",
       "3                      3.923478                     2.772174   \n",
       "4                      4.125000                     2.729167   \n",
       "\n",
       "   province_leisure_sum_mean  province_public_service_sum_mean  \\\n",
       "0                  41.701521                        506.456274   \n",
       "1                  47.194158                        625.233677   \n",
       "2                  46.653117                        623.013550   \n",
       "3                  45.375652                        655.568696   \n",
       "4                  44.447917                        626.340278   \n",
       "\n",
       "   province_trust_sum_mean  city_income_mean  city_family_income_mean  \\\n",
       "0                35.511407      43096.656716             93764.077651   \n",
       "1                41.434708      64016.125654             34201.145410   \n",
       "2                36.200542       6522.105263             32477.297170   \n",
       "3                42.198261      45077.070707            102113.496521   \n",
       "4                32.631944      33930.569444             73738.595633   \n",
       "\n",
       "   city_equity_mean  city_depression_mean  city_floor_area_mean  \\\n",
       "0          2.800000              4.011940             82.744776   \n",
       "1          3.371728              3.989529            157.528796   \n",
       "2          3.294737              3.957895            120.684211   \n",
       "3          3.252525              4.030303             98.858586   \n",
       "4          3.309028              4.093750             57.526389   \n",
       "\n",
       "   city_health_mean  city_class_10_diff_mean  city_class_mean  \\\n",
       "0          3.650746                 1.164179         4.465672   \n",
       "1          3.968586                 0.942408         4.335079   \n",
       "2          3.684211                 0.789474         4.505263   \n",
       "3          4.070707                 0.696970         4.464646   \n",
       "4          3.774306                 0.527778         4.444444   \n",
       "\n",
       "   city_health_problem_mean  city_family_status_mean  city_leisure_sum_mean  \\\n",
       "0                  4.113433                 2.665672              40.059701   \n",
       "1                  3.963351                 2.811518              48.303665   \n",
       "2                  3.842105                 2.768421              48.263158   \n",
       "3                  4.090909                 2.696970              42.727273   \n",
       "4                  4.125000                 2.729167              44.447917   \n",
       "\n",
       "   city_public_service_sum_mean  city_trust_sum_mean  county_income_mean  \\\n",
       "0                    542.671642            34.955224        28979.591837   \n",
       "1                    634.460733            41.732984        11628.659794   \n",
       "2                    654.547368            38.778947         6522.105263   \n",
       "3                    631.636364            38.333333        45077.070707   \n",
       "4                    626.340278            32.631944        28430.769231   \n",
       "\n",
       "   county_family_income_mean  county_equity_mean  county_depression_mean  \\\n",
       "0               60630.401904            2.571429                4.081633   \n",
       "1               34758.996059            3.350515                3.958763   \n",
       "2               32477.297170            3.294737                3.957895   \n",
       "3              102113.496521            3.252525                4.030303   \n",
       "4               85328.619373            3.589744                3.974359   \n",
       "\n",
       "   county_floor_area_mean  county_health_mean  county_class_10_diff_mean  \\\n",
       "0               38.530612            3.489796                   1.183673   \n",
       "1              154.608247            3.927835                   0.917526   \n",
       "2              120.684211            3.684211                   0.789474   \n",
       "3               98.858586            4.070707                   0.696970   \n",
       "4               57.553846            4.000000                   0.666667   \n",
       "\n",
       "   county_class_mean  county_health_problem_mean  county_family_status_mean  \\\n",
       "0           4.938776                    3.877551                   2.489796   \n",
       "1           4.123711                    3.948454                   2.752577   \n",
       "2           4.505263                    3.842105                   2.768421   \n",
       "3           4.464646                    4.090909                   2.696970   \n",
       "4           3.641026                    4.179487                   3.025641   \n",
       "\n",
       "   county_leisure_sum_mean  county_public_service_sum_mean  \\\n",
       "0                36.693878                      559.163265   \n",
       "1                48.628866                      629.123711   \n",
       "2                48.263158                      654.547368   \n",
       "3                42.727273                      631.636364   \n",
       "4                44.846154                      513.076923   \n",
       "\n",
       "   county_trust_sum_mean  income/province  family_income/province  \\\n",
       "0              31.346939         0.323313                0.455794   \n",
       "1              42.731959         0.671001                1.059353   \n",
       "2              38.778947         0.123102                0.083914   \n",
       "3              38.333333         0.239812                0.196602   \n",
       "4              39.256410         0.000000                0.899980   \n",
       "\n",
       "   equity/province  depression/province  floor_area/province  health/province  \\\n",
       "0         1.081563             1.264423             0.507373         0.810478   \n",
       "1         0.923321             0.751614             0.740921         1.264118   \n",
       "2         1.212818             1.370728             1.112116         1.116490   \n",
       "3         1.175869             1.011434             0.685256         1.044980   \n",
       "4         0.604407             0.732824             1.216833         1.324747   \n",
       "\n",
       "   class_10_diff/province  class/province  health_problem/province  \\\n",
       "0                0.000000        0.656133                 0.493665   \n",
       "1                2.063830        1.359284                 1.009978   \n",
       "2                0.971053        1.178161                 1.112283   \n",
       "3                2.178030        1.092325                 1.019504   \n",
       "4                0.000000        0.225000                 1.212121   \n",
       "\n",
       "   family_status/province  leisure_sum/province  public_service_sum/province  \\\n",
       "0                0.736179              0.791338                     0.829292   \n",
       "1                1.461394              0.826373                     1.079596   \n",
       "2                1.150728              0.964566                     1.202221   \n",
       "3                1.082183              0.947645                     1.159299   \n",
       "4                1.099237              0.899930                     0.718459   \n",
       "\n",
       "   trust_sum/province  income/city  family_income/city  equity/city  \\\n",
       "0            0.849050     0.464073            0.639904     1.071429   \n",
       "1            1.013321     0.312421            1.169551     0.889752   \n",
       "2            0.940847     0.306649            0.246326     1.214058   \n",
       "3            1.018560     0.142423            0.117516     1.229814   \n",
       "4            1.308280     0.000000            0.899980     0.604407   \n",
       "\n",
       "   depression/city  floor_area/city  health/city  class_10_diff/city  \\\n",
       "0         1.246280         0.543841     0.821750            0.000000   \n",
       "1         0.751969         0.698285     1.259894            2.122222   \n",
       "2         1.263298         0.994331     1.085714            1.266667   \n",
       "3         0.992481         0.789006     0.982630            2.869565   \n",
       "4         0.732824         1.216833     1.324747            0.000000   \n",
       "\n",
       "   class/city  health_problem/city  family_status/city  leisure_sum/city  \\\n",
       "0    0.671791             0.486212            0.750280          0.823770   \n",
       "1    1.384058             1.009247            1.422719          0.807392   \n",
       "2    1.109813             1.041096            1.083650          0.932388   \n",
       "3    1.119910             0.977778            1.112360          1.006383   \n",
       "4    0.225000             1.212121            1.099237          0.899930   \n",
       "\n",
       "   public_service_sum/city  trust_sum/city  income/county  \\\n",
       "0                 0.773949        0.886849       0.690141   \n",
       "1                 1.063896        1.030360       1.719889   \n",
       "2                 1.144302        0.902552       0.306649   \n",
       "3                 1.203224        1.147826       0.142423   \n",
       "4                 0.718459        1.348372       0.000000   \n",
       "\n",
       "   family_income/county  equity/county  depression/county  floor_area/county  \\\n",
       "0              0.989603       1.166667           1.225000           1.167903   \n",
       "1              1.150781       0.895385           0.757812           0.711476   \n",
       "2              0.246326       1.214058           1.263298           0.994331   \n",
       "3              0.117516       1.229814           0.992481           0.789006   \n",
       "4              0.777737       0.557143           0.754839           1.216252   \n",
       "\n",
       "   health/county  class_10_diff/county  class/county  health_problem/county  \\\n",
       "0       0.859649              0.000000      0.607438               0.515789   \n",
       "1       1.272966              2.179775      1.455000               1.013055   \n",
       "2       1.085714              1.266667      1.109813               1.041096   \n",
       "3       0.982630              2.869565      1.119910               0.977778   \n",
       "4       1.250000              0.000000      0.274648               1.196319   \n",
       "\n",
       "   family_status/county  leisure_sum/county  public_service_sum/county  \\\n",
       "0              0.803279            0.899333                   0.751122   \n",
       "1              1.453184            0.801993                   1.072921   \n",
       "2              1.083650            0.932388                   1.144302   \n",
       "3              1.112360            1.006383                   1.203224   \n",
       "4              0.991525            0.891938                   0.877061   \n",
       "\n",
       "   trust_sum/county  age_income_mean  age_family_income_mean  age_equity_mean  \\\n",
       "0          0.988932     24371.808219            68176.681796         3.061644   \n",
       "1          1.006273     20771.900826            71277.916142         3.157025   \n",
       "2          0.902552     23468.987342            51169.247670         3.025316   \n",
       "3          1.147826     15072.375000           123040.259642         3.348214   \n",
       "4          1.120836     14158.165138            69149.961916         3.082569   \n",
       "\n",
       "   age_depression_mean  age_floor_area_mean  age_health_mean  \\\n",
       "0             3.890411           109.662329         3.534247   \n",
       "1             4.090909           115.446281         4.239669   \n",
       "2             3.721519           123.415190         3.476793   \n",
       "3             3.598214           121.433036         3.133929   \n",
       "4             4.000000           119.192661         4.247706   \n",
       "\n",
       "   age_class_10_diff_mean  age_class_mean  age_health_problem_mean  \\\n",
       "0                0.479452        4.390411                 3.835616   \n",
       "1                1.975207        4.462810                 4.487603   \n",
       "2                0.839662        4.181435                 3.789030   \n",
       "3                0.401786        4.169643                 3.366071   \n",
       "4                1.779817        4.220183                 4.477064   \n",
       "\n",
       "   age_family_status_mean  age_leisure_sum_mean  age_public_service_sum_mean  \\\n",
       "0                2.726027             45.541096                   608.657534   \n",
       "1                2.942149             38.545455                   579.082645   \n",
       "2                2.518987             46.561181                   607.130802   \n",
       "3                2.580357             47.812500                   607.258929   \n",
       "4                2.743119             38.266055                   595.243119   \n",
       "\n",
       "   age_trust_sum_mean  income/age  family_income/age  equity/age  \\\n",
       "0           36.054795    0.820620           0.880066    0.979866   \n",
       "1           39.107438    0.962839           0.561184    0.950262   \n",
       "2           37.772152    0.085219           0.156344    1.322176   \n",
       "3           36.857143    0.425945           0.097529    1.194667   \n",
       "4           41.743119    0.000000           0.959700    0.648810   \n",
       "\n",
       "   depression/age  floor_area/age  health/age  class_10_diff/age  class/age  \\\n",
       "0        1.285211        0.410351    0.848837           0.000000   0.683307   \n",
       "1        0.733333        0.952824    1.179337           1.012552   1.344444   \n",
       "2        1.343537        0.972328    1.150485           1.190955   1.195762   \n",
       "3        1.111663        0.642329    1.276353           4.977778   1.199143   \n",
       "4        0.750000        0.587284    1.177106           0.000000   0.236957   \n",
       "\n",
       "   health_problem/age  family_status/age  leisure_sum/age  \\\n",
       "0            0.521429           0.733668         0.724620   \n",
       "1            0.891344           1.359551         1.011792   \n",
       "2            1.055679           1.190955         0.966470   \n",
       "3            1.188329           1.162630         0.899346   \n",
       "4            1.116803           1.093645         1.045313   \n",
       "\n",
       "   public_service_sum/age  trust_sum/age  \n",
       "0                0.690043       0.859802  \n",
       "1                1.165637       1.099535  \n",
       "2                1.233672       0.926609  \n",
       "3                1.251525       1.193798  \n",
       "4                0.755994       1.054066  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('当前数据的shape:',data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还应该删去有效样本数很少的特征，例如负值太多的特征或者是缺失值太多的特征，这里我一共删除了包括“目前的最高教育程度”在内的9类特征，得到了最终的263维的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 263)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#272-9=263\n",
    "#删除数值特别少的和之前用过的特征\n",
    "del_list=['id','survey_time','edu_other','invest_other','property_other','join_party','province','city','county']\n",
    "use_feature = [clo for clo in data.columns if clo not in del_list]\n",
    "data.fillna(0,inplace=True) #还是补0\n",
    "train_shape = train.shape[0] #训练集的样本数\n",
    "features = data[use_feature].columns #删除后所有的特征\n",
    "X_train_263 = data[:train_shape][use_feature].values\n",
    "y_train = target\n",
    "X_test_263 = data[train_shape:][use_feature].values\n",
    "X_train_263.shape #最终一种263个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里选择了最重要的49个特征，作为除了以上263维特征外的另外一组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 49)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_fea_49 = ['equity','depression','health','class','family_status','health_problem','class_10_after',\n",
    "           'equity/province','equity/city','equity/county',\n",
    "           'depression/province','depression/city','depression/county',\n",
    "           'health/province','health/city','health/county',\n",
    "           'class/province','class/city','class/county',\n",
    "           'family_status/province','family_status/city','family_status/county',\n",
    "           'family_income/province','family_income/city','family_income/county',\n",
    "           'floor_area/province','floor_area/city','floor_area/county',\n",
    "           'leisure_sum/province','leisure_sum/city','leisure_sum/county',\n",
    "           'public_service_sum/province','public_service_sum/city','public_service_sum/county',\n",
    "           'trust_sum/province','trust_sum/city','trust_sum/county',\n",
    "           'income/m','public_service_sum','class_diff','status_3_before','age_income_mean','age_floor_area_mean',\n",
    "           'weight_jin','height_cm',\n",
    "           'health/age','depression/age','equity/age','leisure_sum/age'\n",
    "          ]\n",
    "train_shape = train.shape[0]\n",
    "X_train_49 = data[:train_shape][imp_fea_49].values\n",
    "X_test_49 = data[train_shape:][imp_fea_49].values\n",
    "X_train_49.shape #最重要的49个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择需要进行onehot编码的离散变量进行one-hot编码，再合成为第三类特征，共383维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10956, 21)\n",
      "(10956, 21)\n",
      "[[ 1.  1.  1.  4.  1.  5.  2.  1.  3.  1.  2.  3.  1.  5.  5.  0.  0.  4.\n",
      "   1.  1.  1.]\n",
      " [ 2.  1.  1.  4.  1.  1.  1.  1.  3.  1.  3.  1.  0.  0.  0.  0.  0.  1.\n",
      "   2.  1.  2.]\n",
      " [ 2.  2.  1.  4.  1.  1.  1.  2.  0.  0.  0.  3.  1.  1.  3.  0.  0.  1.\n",
      "   2.  1.  2.]\n",
      " [ 2.  2.  1.  4.  1.  1.  2.  4.  0.  0.  0.  7.  0.  0.  0.  0.  0.  1.\n",
      "   2.  1.  2.]\n",
      " [ 1.  2.  1.  1.  2.  2.  3.  6.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "  10.  1. 15.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7988, 383)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fea = ['survey_type','gender','nationality','edu_status','political','hukou','hukou_loc','work_exper','work_status','work_type',\n",
    "           'work_manage','marital','s_political','s_hukou','s_work_exper','s_work_status','s_work_type','f_political','f_work_14',\n",
    "           'm_political','m_work_14']\n",
    "noc_fea = [clo for clo in use_feature if clo not in cat_fea]\n",
    "\n",
    "onehot_data = data[cat_fea].values\n",
    "print(data[cat_fea].shape)\n",
    "print(onehot_data.shape)\n",
    "print(onehot_data[:5])\n",
    "enc = preprocessing.OneHotEncoder(categories = 'auto')\n",
    "oh_data=enc.fit_transform(onehot_data).toarray()\n",
    "oh_data.shape #变为onehot编码格式\n",
    "\n",
    "X_train_oh = oh_data[:train_shape,:]\n",
    "X_test_oh = oh_data[train_shape:,:]\n",
    "X_train_oh.shape #其中的训练集\n",
    "\n",
    "X_train_383 = np.column_stack([data[:train_shape][noc_fea].values,X_train_oh])#先是noc，再是cat_fea\n",
    "X_test_383 = np.column_stack([data[train_shape:][noc_fea].values,X_test_oh])\n",
    "X_train_383.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于此，我们构建完成了三种特征工程（训练数据集），其一是上面提取的最重要的49中特征，其中包括健康程度、社会阶级、在同龄人中的收入情况等等特征。其二是扩充后的263维特征（这里可以认为是初始特征）。其三是使用One-hot编码后的特征，这里要使用One-hot进行编码的原因在于，有部分特征为分离值，例如性别中男女，男为1，女为2，我们想使用One-hot将其变为男为0，女为1，来增强机器学习算法的鲁棒性能；再如民族这个特征，原本是1-56这56个数值，如果直接分类会让分类器的鲁棒性变差，所以使用One-hot编码将其变为6个特征进行非零即一的处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征建模\n",
    "\n",
    "首先我们对于原始的263维的特征，使用lightGBM进行处理，这里我们使用5折交叉验证的方法：\n",
    "\n",
    "1.lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.5001\tvalid_1's l2: 0.533751\n",
      "[1000]\ttraining's l2: 0.451908\tvalid_1's l2: 0.501198\n",
      "[1500]\ttraining's l2: 0.425949\tvalid_1's l2: 0.488293\n",
      "[2000]\ttraining's l2: 0.407968\tvalid_1's l2: 0.482061\n",
      "[2500]\ttraining's l2: 0.393574\tvalid_1's l2: 0.478426\n",
      "[3000]\ttraining's l2: 0.381332\tvalid_1's l2: 0.476212\n",
      "[3500]\ttraining's l2: 0.370554\tvalid_1's l2: 0.474885\n",
      "[4000]\ttraining's l2: 0.360716\tvalid_1's l2: 0.473582\n",
      "[4500]\ttraining's l2: 0.351707\tvalid_1's l2: 0.472661\n",
      "[5000]\ttraining's l2: 0.343341\tvalid_1's l2: 0.472194\n",
      "[5500]\ttraining's l2: 0.335558\tvalid_1's l2: 0.471812\n",
      "[6000]\ttraining's l2: 0.32791\tvalid_1's l2: 0.471695\n",
      "[6500]\ttraining's l2: 0.320627\tvalid_1's l2: 0.471377\n",
      "[7000]\ttraining's l2: 0.313794\tvalid_1's l2: 0.471256\n",
      "[7500]\ttraining's l2: 0.307106\tvalid_1's l2: 0.470938\n",
      "[8000]\ttraining's l2: 0.300754\tvalid_1's l2: 0.470951\n",
      "[8500]\ttraining's l2: 0.294578\tvalid_1's l2: 0.471331\n",
      "Early stopping, best iteration is:\n",
      "[7884]\ttraining's l2: 0.30219\tvalid_1's l2: 0.470843\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.504838\tvalid_1's l2: 0.514283\n",
      "[1000]\ttraining's l2: 0.455763\tvalid_1's l2: 0.479868\n",
      "[1500]\ttraining's l2: 0.429705\tvalid_1's l2: 0.466721\n",
      "[2000]\ttraining's l2: 0.411908\tvalid_1's l2: 0.459789\n",
      "[2500]\ttraining's l2: 0.397781\tvalid_1's l2: 0.45585\n",
      "[3000]\ttraining's l2: 0.385646\tvalid_1's l2: 0.453043\n",
      "[3500]\ttraining's l2: 0.374972\tvalid_1's l2: 0.451489\n",
      "[4000]\ttraining's l2: 0.365196\tvalid_1's l2: 0.450187\n",
      "[4500]\ttraining's l2: 0.356074\tvalid_1's l2: 0.44927\n",
      "[5000]\ttraining's l2: 0.347628\tvalid_1's l2: 0.448364\n",
      "[5500]\ttraining's l2: 0.339665\tvalid_1's l2: 0.447851\n",
      "[6000]\ttraining's l2: 0.331968\tvalid_1's l2: 0.447708\n",
      "[6500]\ttraining's l2: 0.324755\tvalid_1's l2: 0.447816\n",
      "Early stopping, best iteration is:\n",
      "[6067]\ttraining's l2: 0.330981\tvalid_1's l2: 0.447567\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.503936\tvalid_1's l2: 0.518085\n",
      "[1000]\ttraining's l2: 0.455869\tvalid_1's l2: 0.481586\n",
      "[1500]\ttraining's l2: 0.430477\tvalid_1's l2: 0.465326\n",
      "[2000]\ttraining's l2: 0.412971\tvalid_1's l2: 0.456094\n",
      "[2500]\ttraining's l2: 0.398755\tvalid_1's l2: 0.450597\n",
      "[3000]\ttraining's l2: 0.38671\tvalid_1's l2: 0.447239\n",
      "[3500]\ttraining's l2: 0.375902\tvalid_1's l2: 0.445184\n",
      "[4000]\ttraining's l2: 0.366077\tvalid_1's l2: 0.443307\n",
      "[4500]\ttraining's l2: 0.357097\tvalid_1's l2: 0.442447\n",
      "[5000]\ttraining's l2: 0.348557\tvalid_1's l2: 0.441847\n",
      "[5500]\ttraining's l2: 0.340403\tvalid_1's l2: 0.441358\n",
      "[6000]\ttraining's l2: 0.332749\tvalid_1's l2: 0.440776\n",
      "[6500]\ttraining's l2: 0.325366\tvalid_1's l2: 0.440421\n",
      "[7000]\ttraining's l2: 0.318358\tvalid_1's l2: 0.440151\n",
      "[7500]\ttraining's l2: 0.311704\tvalid_1's l2: 0.440185\n",
      "[8000]\ttraining's l2: 0.305337\tvalid_1's l2: 0.439871\n",
      "[8500]\ttraining's l2: 0.299152\tvalid_1's l2: 0.439878\n",
      "[9000]\ttraining's l2: 0.293267\tvalid_1's l2: 0.439833\n",
      "[9500]\ttraining's l2: 0.287479\tvalid_1's l2: 0.439858\n",
      "Early stopping, best iteration is:\n",
      "[8964]\ttraining's l2: 0.293701\tvalid_1's l2: 0.439774\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.505016\tvalid_1's l2: 0.512576\n",
      "[1000]\ttraining's l2: 0.456388\tvalid_1's l2: 0.477883\n",
      "[1500]\ttraining's l2: 0.429915\tvalid_1's l2: 0.46594\n",
      "[2000]\ttraining's l2: 0.411795\tvalid_1's l2: 0.460032\n",
      "[2500]\ttraining's l2: 0.397564\tvalid_1's l2: 0.456841\n",
      "[3000]\ttraining's l2: 0.385508\tvalid_1's l2: 0.454998\n",
      "[3500]\ttraining's l2: 0.374765\tvalid_1's l2: 0.453924\n",
      "[4000]\ttraining's l2: 0.364971\tvalid_1's l2: 0.453105\n",
      "[4500]\ttraining's l2: 0.355916\tvalid_1's l2: 0.452174\n",
      "[5000]\ttraining's l2: 0.347367\tvalid_1's l2: 0.451817\n",
      "[5500]\ttraining's l2: 0.339388\tvalid_1's l2: 0.451525\n",
      "[6000]\ttraining's l2: 0.331805\tvalid_1's l2: 0.451483\n",
      "[6500]\ttraining's l2: 0.32442\tvalid_1's l2: 0.45134\n",
      "[7000]\ttraining's l2: 0.317559\tvalid_1's l2: 0.451085\n",
      "[7500]\ttraining's l2: 0.310909\tvalid_1's l2: 0.451035\n",
      "Early stopping, best iteration is:\n",
      "[7169]\ttraining's l2: 0.315328\tvalid_1's l2: 0.450971\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.503608\tvalid_1's l2: 0.520348\n",
      "[1000]\ttraining's l2: 0.455513\tvalid_1's l2: 0.485561\n",
      "[1500]\ttraining's l2: 0.429732\tvalid_1's l2: 0.472124\n",
      "[2000]\ttraining's l2: 0.411651\tvalid_1's l2: 0.465725\n",
      "[2500]\ttraining's l2: 0.397112\tvalid_1's l2: 0.461931\n",
      "[3000]\ttraining's l2: 0.384817\tvalid_1's l2: 0.45978\n",
      "[3500]\ttraining's l2: 0.373864\tvalid_1's l2: 0.458444\n",
      "[4000]\ttraining's l2: 0.363883\tvalid_1's l2: 0.457687\n",
      "[4500]\ttraining's l2: 0.354631\tvalid_1's l2: 0.457417\n",
      "[5000]\ttraining's l2: 0.345962\tvalid_1's l2: 0.457266\n",
      "[5500]\ttraining's l2: 0.337767\tvalid_1's l2: 0.456928\n",
      "[6000]\ttraining's l2: 0.330051\tvalid_1's l2: 0.456908\n",
      "[6500]\ttraining's l2: 0.322638\tvalid_1's l2: 0.457042\n",
      "Early stopping, best iteration is:\n",
      "[5771]\ttraining's l2: 0.333515\tvalid_1's l2: 0.456774\n",
      "CV score: 0.45318556\n"
     ]
    }
   ],
   "source": [
    "##### lgb_263 #\n",
    "#lightGBM决策树\n",
    "lgb_263_param = {\n",
    "'num_leaves': 7, \n",
    "'min_data_in_leaf': 20, #叶子可能具有的最小记录数\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.003,\n",
    "\"boosting\": \"gbdt\", #用gbdt算法\n",
    "\"feature_fraction\": 0.18, #例如 0.18时，意味着在每次迭代中随机选择18％的参数来建树\n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.55, #每次迭代时用的数据比例\n",
    "\"bagging_seed\": 14,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l1\": 0.1005,\n",
    "\"lambda_l2\": 0.1996, \n",
    "\"verbosity\": -1}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)   #交叉切分：5\n",
    "oof_lgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_lgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_263[val_idx], y_train[val_idx])#train:val=4:1\n",
    "\n",
    "    num_round = 10000\n",
    "    lgb_263 = lgb.train(lgb_263_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 800)\n",
    "    oof_lgb_263[val_idx] = lgb_263.predict(X_train_263[val_idx], num_iteration=lgb_263.best_iteration)\n",
    "    predictions_lgb_263 += lgb_263.predict(X_test_263, num_iteration=lgb_263.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我使用已经训练完的lightGBM的模型进行特征重要性的判断以及可视化，从结果我们可以看出，排在重要性第一位的是health/age，就是同龄人中的健康程度，与我们主观的看法基本一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x2016 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------特征重要性\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "df = pd.DataFrame(data[use_feature].columns.tolist(), columns=['feature'])\n",
    "df['importance']=list(lgb_263.feature_importance())\n",
    "df = df.sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df.head(50))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面，我们使用常见的机器学习方法，对于263维特征进行建模：\n",
    "\n",
    "2.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[18:02:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:02:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40426\tvalid_data-rmse:3.38320\n",
      "[500]\ttrain-rmse:0.40602\tvalid_data-rmse:0.70888\n",
      "[977]\ttrain-rmse:0.27314\tvalid_data-rmse:0.71213\n",
      "fold n°2\n",
      "[18:02:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:02:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39812\tvalid_data-rmse:3.40793\n",
      "[500]\ttrain-rmse:0.40425\tvalid_data-rmse:0.69590\n",
      "[1000]\ttrain-rmse:0.27142\tvalid_data-rmse:0.69568\n",
      "[1202]\ttrain-rmse:0.23042\tvalid_data-rmse:0.69698\n",
      "fold n°3\n",
      "[18:02:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:02:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40183\tvalid_data-rmse:3.39295\n",
      "[500]\ttrain-rmse:0.41103\tvalid_data-rmse:0.66062\n",
      "[1000]\ttrain-rmse:0.27205\tvalid_data-rmse:0.66275\n",
      "[1052]\ttrain-rmse:0.26202\tvalid_data-rmse:0.66329\n",
      "fold n°4\n",
      "[18:02:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:02:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40240\tvalid_data-rmse:3.39014\n",
      "[500]\ttrain-rmse:0.40794\tvalid_data-rmse:0.66856\n",
      "[1000]\ttrain-rmse:0.26989\tvalid_data-rmse:0.67006\n",
      "[1035]\ttrain-rmse:0.26201\tvalid_data-rmse:0.67079\n",
      "fold n°5\n",
      "[18:02:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:02:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39342\tvalid_data-rmse:3.42633\n",
      "[500]\ttrain-rmse:0.41331\tvalid_data-rmse:0.65291\n",
      "[1000]\ttrain-rmse:0.27402\tvalid_data-rmse:0.64990\n",
      "[1500]\ttrain-rmse:0.18408\tvalid_data-rmse:0.65140\n",
      "[1622]\ttrain-rmse:0.16754\tvalid_data-rmse:0.65180\n",
      "CV score: 0.45721409\n"
     ]
    }
   ],
   "source": [
    "##### xgb_263\n",
    "#xgboost\n",
    "xgb_263_params = {'eta': 0.02,  #lr\n",
    "              'max_depth': 6,  \n",
    "              'min_child_weight':3,#最小叶子节点样本权重和\n",
    "              'gamma':0, #指定节点分裂所需的最小损失函数下降值。\n",
    "              'subsample': 0.7,  #控制对于每棵树，随机采样的比例\n",
    "              'colsample_bytree': 0.3,  #用来控制每棵随机采样的列数的占比 (每一列是一个特征)。\n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_xgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_263[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_263 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_263_params)\n",
    "    oof_xgb_263[val_idx] = xgb_263.predict(xgb.DMatrix(X_train_263[val_idx]), ntree_limit=xgb_263.best_ntree_limit)\n",
    "    predictions_xgb_263 += xgb_263.predict(xgb.DMatrix(X_test_263), ntree_limit=xgb_263.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RandomForestRegressor随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   17.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   17.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   16.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   16.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   16.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.47911072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#RandomForestRegressor随机森林\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_rfr_263 = np.zeros(len(X_train_263))\n",
    "predictions_rfr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    rfr_263 = rfr(n_estimators=1600,max_depth=9, min_samples_leaf=9, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.25,verbose=1,n_jobs=-1)\n",
    "    #verbose = 0 为不在标准输出流输出日志信息\n",
    "#verbose = 1 为输出进度条记录\n",
    "#verbose = 2 为每个epoch输出一行记录\n",
    "    rfr_263.fit(tr_x,tr_y)\n",
    "    oof_rfr_263[val_idx] = rfr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_rfr_263 += rfr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_rfr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6527           0.0034           17.32s\n",
      "         2           0.6570           0.0031           16.97s\n",
      "         3           0.6564           0.0027           16.71s\n",
      "         4           0.6540           0.0032           16.69s\n",
      "         5           0.6478           0.0032           16.83s\n",
      "         6           0.6542           0.0028           16.88s\n",
      "         7           0.6319           0.0032           16.84s\n",
      "         8           0.6449           0.0026           16.83s\n",
      "         9           0.6422           0.0030           16.81s\n",
      "        10           0.6255           0.0027           16.72s\n",
      "        20           0.5899           0.0023           16.13s\n",
      "        30           0.5660           0.0017           15.58s\n",
      "        40           0.5363           0.0015           15.13s\n",
      "        50           0.5211           0.0012           14.64s\n",
      "        60           0.4761           0.0012           14.18s\n",
      "        70           0.4801           0.0010           13.73s\n",
      "        80           0.4585           0.0007           13.30s\n",
      "        90           0.4501           0.0007           12.87s\n",
      "       100           0.4317           0.0005           12.44s\n",
      "       200           0.3457           0.0001            8.24s\n",
      "       300           0.2921          -0.0000            4.15s\n",
      "       400           0.2689          -0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6627           0.0034           17.18s\n",
      "         2           0.6597           0.0036           17.04s\n",
      "         3           0.6377           0.0032           16.87s\n",
      "         4           0.6310           0.0031           16.96s\n",
      "         5           0.6547           0.0030           16.84s\n",
      "         6           0.6523           0.0028           16.78s\n",
      "         7           0.6341           0.0026           16.74s\n",
      "         8           0.6326           0.0029           16.69s\n",
      "         9           0.6240           0.0030           16.61s\n",
      "        10           0.6410           0.0024           16.60s\n",
      "        20           0.5965           0.0024           16.03s\n",
      "        30           0.5776           0.0017           15.58s\n",
      "        40           0.5195           0.0016           15.08s\n",
      "        50           0.5324           0.0010           14.65s\n",
      "        60           0.5050           0.0012           14.20s\n",
      "        70           0.4765           0.0009           13.77s\n",
      "        80           0.4593           0.0009           13.36s\n",
      "        90           0.4396           0.0005           12.91s\n",
      "       100           0.4262           0.0006           12.48s\n",
      "       200           0.3389           0.0001            8.28s\n",
      "       300           0.3032           0.0000            4.13s\n",
      "       400           0.2766          -0.0000            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6565           0.0032           17.09s\n",
      "         2           0.6485           0.0032           16.65s\n",
      "         3           0.6564           0.0033           16.47s\n",
      "         4           0.6406           0.0033           16.46s\n",
      "         5           0.6607           0.0029           16.36s\n",
      "         6           0.6495           0.0030           16.37s\n",
      "         7           0.6421           0.0028           16.30s\n",
      "         8           0.6493           0.0027           16.22s\n",
      "         9           0.6443           0.0030           16.24s\n",
      "        10           0.6449           0.0030           16.18s\n",
      "        20           0.6006           0.0022           15.60s\n",
      "        30           0.5583           0.0018           15.19s\n",
      "        40           0.5297           0.0018           14.88s\n",
      "        50           0.5217           0.0013           14.48s\n",
      "        60           0.4885           0.0010           14.07s\n",
      "        70           0.4868           0.0007           13.70s\n",
      "        80           0.4438           0.0008           13.38s\n",
      "        90           0.4401           0.0006           12.98s\n",
      "       100           0.4271           0.0004           12.56s\n",
      "       200           0.3401           0.0002            8.31s\n",
      "       300           0.2918          -0.0000            4.14s\n",
      "       400           0.2671          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6665           0.0031           16.54s\n",
      "         2           0.6639           0.0033           16.62s\n",
      "         3           0.6513           0.0032           16.41s\n",
      "         4           0.6581           0.0036           16.43s\n",
      "         5           0.6374           0.0030           16.43s\n",
      "         6           0.6407           0.0030           16.36s\n",
      "         7           0.6338           0.0032           16.29s\n",
      "         8           0.6338           0.0032           16.28s\n",
      "         9           0.6289           0.0026           16.27s\n",
      "        10           0.6356           0.0027           16.24s\n",
      "        20           0.5863           0.0023           15.81s\n",
      "        30           0.5479           0.0022           15.40s\n",
      "        40           0.5406           0.0014           14.93s\n",
      "        50           0.5035           0.0014           14.52s\n",
      "        60           0.5029           0.0010           14.07s\n",
      "        70           0.4707           0.0008           13.64s\n",
      "        80           0.4371           0.0008           13.22s\n",
      "        90           0.4400           0.0007           12.79s\n",
      "       100           0.4221           0.0006           12.38s\n",
      "       200           0.3456           0.0002            8.24s\n",
      "       300           0.3064          -0.0000            4.10s\n",
      "       400           0.2660          -0.0000            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6747           0.0031           16.83s\n",
      "         2           0.6701           0.0032           16.66s\n",
      "         3           0.6527           0.0034           16.45s\n",
      "         4           0.6556           0.0031           16.52s\n",
      "         5           0.6446           0.0032           16.48s\n",
      "         6           0.6426           0.0026           16.41s\n",
      "         7           0.6472           0.0029           16.45s\n",
      "         8           0.6407           0.0029           16.35s\n",
      "         9           0.6272           0.0029           16.31s\n",
      "        10           0.6072           0.0031           16.30s\n",
      "        20           0.6060           0.0021           16.02s\n",
      "        30           0.5608           0.0019           15.47s\n",
      "        40           0.5268           0.0017           15.10s\n",
      "        50           0.4976           0.0014           14.60s\n",
      "        60           0.4839           0.0010           14.15s\n",
      "        70           0.4606           0.0008           13.76s\n",
      "        80           0.4703           0.0008           13.39s\n",
      "        90           0.4375           0.0006           12.96s\n",
      "       100           0.4230           0.0007           12.54s\n",
      "       200           0.3441           0.0001            8.29s\n",
      "       300           0.3002           0.0001            4.14s\n",
      "       400           0.2774          -0.0000            0.00s\n",
      "CV score: 0.45737842\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingRegressor梯度提升决策树\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_263 = np.zeros(train_shape)\n",
    "predictions_gbr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_263 = gbr(n_estimators=400, learning_rate=0.01,subsample=0.65,max_depth=7, min_samples_leaf=20,\n",
    "            max_features=0.22,verbose=1)\n",
    "    gbr_263.fit(tr_x,tr_y)\n",
    "    oof_gbr_263[val_idx] = gbr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_gbr_263 += gbr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ExtraTreesRegressor 极端随机森林回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.48563915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesRegressor 极端随机森林回归\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_etr_263 = np.zeros(train_shape)\n",
    "predictions_etr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    etr_263 = etr(n_estimators=1000,max_depth=8, min_samples_leaf=12, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.4,verbose=1,n_jobs=-1)\n",
    "    etr_263.fit(tr_x,tr_y)\n",
    "    oof_etr_263[val_idx] = etr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_etr_263 += etr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_etr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上5种模型的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44996614343677105"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack2 = np.vstack([oof_lgb_263,oof_xgb_263,oof_gbr_263,oof_rfr_263,oof_etr_263]).transpose()\n",
    "# transpose()函数的作用就是调换x,y,z的位置,也就是数组的索引值\n",
    "test_stack2 = np.vstack([predictions_lgb_263, predictions_xgb_263,predictions_gbr_263,predictions_rfr_263,predictions_etr_263]).transpose()\n",
    "\n",
    "#交叉验证:5折，重复2次\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack2 = np.zeros(train_stack2.shape[0])\n",
    "predictions_lr2 = np.zeros(test_stack2.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack2,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack2[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack2[val_idx], target.iloc[val_idx].values\n",
    "    #Kernel Ridge Regression\n",
    "    lr2 = kr()\n",
    "    lr2.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack2[val_idx] = lr2.predict(val_data)\n",
    "    predictions_lr2 += lr2.predict(test_stack2) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们对于49维的数据进行与上述263维数据相同的操作\n",
    "\n",
    "1.lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.472117\tvalid_1's l2: 0.494335\n",
      "[2000]\ttraining's l2: 0.432469\tvalid_1's l2: 0.474129\n",
      "[3000]\ttraining's l2: 0.410256\tvalid_1's l2: 0.468791\n",
      "[4000]\ttraining's l2: 0.39256\tvalid_1's l2: 0.466552\n",
      "[5000]\ttraining's l2: 0.377013\tvalid_1's l2: 0.466016\n",
      "[6000]\ttraining's l2: 0.363014\tvalid_1's l2: 0.466067\n",
      "Early stopping, best iteration is:\n",
      "[5373]\ttraining's l2: 0.371605\tvalid_1's l2: 0.465741\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.471663\tvalid_1's l2: 0.494652\n",
      "[2000]\ttraining's l2: 0.431201\tvalid_1's l2: 0.475832\n",
      "[3000]\ttraining's l2: 0.40857\tvalid_1's l2: 0.471509\n",
      "[4000]\ttraining's l2: 0.390546\tvalid_1's l2: 0.470323\n",
      "[5000]\ttraining's l2: 0.375102\tvalid_1's l2: 0.470296\n",
      "Early stopping, best iteration is:\n",
      "[4431]\ttraining's l2: 0.38363\tvalid_1's l2: 0.470181\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.474605\tvalid_1's l2: 0.488842\n",
      "[2000]\ttraining's l2: 0.434093\tvalid_1's l2: 0.465848\n",
      "[3000]\ttraining's l2: 0.411314\tvalid_1's l2: 0.461478\n",
      "[4000]\ttraining's l2: 0.393582\tvalid_1's l2: 0.460987\n",
      "Early stopping, best iteration is:\n",
      "[3568]\ttraining's l2: 0.400807\tvalid_1's l2: 0.460616\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.467855\tvalid_1's l2: 0.509235\n",
      "[2000]\ttraining's l2: 0.42834\tvalid_1's l2: 0.492237\n",
      "[3000]\ttraining's l2: 0.406451\tvalid_1's l2: 0.487713\n",
      "[4000]\ttraining's l2: 0.389168\tvalid_1's l2: 0.484667\n",
      "[5000]\ttraining's l2: 0.374352\tvalid_1's l2: 0.482771\n",
      "[6000]\ttraining's l2: 0.361117\tvalid_1's l2: 0.481064\n",
      "[7000]\ttraining's l2: 0.348909\tvalid_1's l2: 0.48018\n",
      "[8000]\ttraining's l2: 0.337402\tvalid_1's l2: 0.479332\n",
      "[9000]\ttraining's l2: 0.326674\tvalid_1's l2: 0.47855\n",
      "[10000]\ttraining's l2: 0.316538\tvalid_1's l2: 0.478033\n",
      "[11000]\ttraining's l2: 0.30707\tvalid_1's l2: 0.477687\n",
      "[12000]\ttraining's l2: 0.297954\tvalid_1's l2: 0.47717\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\ttraining's l2: 0.297954\tvalid_1's l2: 0.47717\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.469248\tvalid_1's l2: 0.504587\n",
      "[2000]\ttraining's l2: 0.429303\tvalid_1's l2: 0.487942\n",
      "[3000]\ttraining's l2: 0.40704\tvalid_1's l2: 0.484008\n",
      "[4000]\ttraining's l2: 0.389376\tvalid_1's l2: 0.482436\n",
      "[5000]\ttraining's l2: 0.374067\tvalid_1's l2: 0.481523\n",
      "[6000]\ttraining's l2: 0.360334\tvalid_1's l2: 0.48131\n",
      "Early stopping, best iteration is:\n",
      "[5907]\ttraining's l2: 0.361534\tvalid_1's l2: 0.481207\n",
      "CV score: 0.47098109\n"
     ]
    }
   ],
   "source": [
    "##### lgb_49\n",
    "lgb_49_param = {\n",
    "'num_leaves': 9,\n",
    "'min_data_in_leaf': 23,\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.002,\n",
    "\"boosting\": \"gbdt\",\n",
    "\"feature_fraction\": 0.45,\n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.65,\n",
    "\"bagging_seed\": 15,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l2\": 0.2, \n",
    "\"verbosity\": -1}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)   \n",
    "oof_lgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_lgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 12000\n",
    "    lgb_49 = lgb.train(lgb_49_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
    "    oof_lgb_49[val_idx] = lgb_49.predict(X_train_49[val_idx], num_iteration=lgb_49.best_iteration)\n",
    "    predictions_lgb_49 += lgb_49.predict(X_test_49, num_iteration=lgb_49.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[18:07:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:07:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40429\tvalid_data-rmse:3.38309\n",
      "[500]\ttrain-rmse:0.52836\tvalid_data-rmse:0.72140\n",
      "[960]\ttrain-rmse:0.44314\tvalid_data-rmse:0.72419\n",
      "fold n°2\n",
      "[18:07:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:07:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39821\tvalid_data-rmse:3.40792\n",
      "[500]\ttrain-rmse:0.52810\tvalid_data-rmse:0.70530\n",
      "[1000]\ttrain-rmse:0.43820\tvalid_data-rmse:0.70914\n",
      "[1088]\ttrain-rmse:0.42459\tvalid_data-rmse:0.70970\n",
      "fold n°3\n",
      "[18:07:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:07:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40184\tvalid_data-rmse:3.39298\n",
      "[500]\ttrain-rmse:0.53301\tvalid_data-rmse:0.66837\n",
      "[1000]\ttrain-rmse:0.44182\tvalid_data-rmse:0.67092\n",
      "[1024]\ttrain-rmse:0.43818\tvalid_data-rmse:0.67125\n",
      "fold n°4\n",
      "[18:07:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:07:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40244\tvalid_data-rmse:3.39015\n",
      "[500]\ttrain-rmse:0.53045\tvalid_data-rmse:0.67763\n",
      "[1000]\ttrain-rmse:0.43863\tvalid_data-rmse:0.68145\n",
      "[1104]\ttrain-rmse:0.42242\tvalid_data-rmse:0.68202\n",
      "fold n°5\n",
      "[18:07:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:07:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39348\tvalid_data-rmse:3.42626\n",
      "[500]\ttrain-rmse:0.53623\tvalid_data-rmse:0.66441\n",
      "[1000]\ttrain-rmse:0.44450\tvalid_data-rmse:0.66730\n",
      "[1095]\ttrain-rmse:0.42990\tvalid_data-rmse:0.66836\n",
      "CV score: 0.47234781\n"
     ]
    }
   ],
   "source": [
    "##### xgb_49\n",
    "xgb_49_params = {'eta': 0.02, \n",
    "              'max_depth': 5, \n",
    "              'min_child_weight':3,\n",
    "              'gamma':0,\n",
    "              'subsample': 0.7, \n",
    "              'colsample_bytree': 0.35, \n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_xgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_49 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_49_params)\n",
    "    oof_xgb_49[val_idx] = xgb_49.predict(xgb.DMatrix(X_train_49[val_idx]), ntree_limit=xgb_49.best_ntree_limit)\n",
    "    predictions_xgb_49 += xgb_49.predict(xgb.DMatrix(X_test_49), ntree_limit=xgb_49.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6691           0.0037           12.05s\n",
      "         2           0.6483           0.0033           11.46s\n",
      "         3           0.6487           0.0032           11.31s\n",
      "         4           0.6710           0.0028           11.16s\n",
      "         5           0.6508           0.0030           11.11s\n",
      "         6           0.6473           0.0028           11.04s\n",
      "         7           0.6493           0.0029           11.05s\n",
      "         8           0.6325           0.0030           10.96s\n",
      "         9           0.6124           0.0030           10.92s\n",
      "        10           0.6336           0.0028           10.87s\n",
      "        20           0.6062           0.0022           10.45s\n",
      "        30           0.5852           0.0018           10.22s\n",
      "        40           0.5615           0.0014           10.03s\n",
      "        50           0.5402           0.0011            9.83s\n",
      "        60           0.5285           0.0010            9.65s\n",
      "        70           0.5053           0.0009            9.45s\n",
      "        80           0.4899           0.0008            9.27s\n",
      "        90           0.4587           0.0006            9.08s\n",
      "       100           0.4665           0.0006            8.91s\n",
      "       200           0.4027           0.0001            7.11s\n",
      "       300           0.3623           0.0000            5.33s\n",
      "       400           0.3398          -0.0001            3.55s\n",
      "       500           0.3326          -0.0000            1.77s\n",
      "       600           0.3151          -0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6503           0.0034           10.93s\n",
      "         2           0.6514           0.0034           10.61s\n",
      "         3           0.6571           0.0029           10.64s\n",
      "         4           0.6487           0.0030           10.47s\n",
      "         5           0.6759           0.0028           10.48s\n",
      "         6           0.6557           0.0027           10.49s\n",
      "         7           0.6350           0.0030           10.48s\n",
      "         8           0.6394           0.0030           10.45s\n",
      "         9           0.6321           0.0030           10.47s\n",
      "        10           0.6449           0.0026           10.46s\n",
      "        20           0.6006           0.0023           10.35s\n",
      "        30           0.5618           0.0019           10.17s\n",
      "        40           0.5515           0.0015            9.96s\n",
      "        50           0.5398           0.0013            9.77s\n",
      "        60           0.5114           0.0011            9.56s\n",
      "        70           0.4883           0.0009            9.40s\n",
      "        80           0.4986           0.0007            9.22s\n",
      "        90           0.4650           0.0007            9.04s\n",
      "       100           0.4804           0.0004            8.87s\n",
      "       200           0.3982           0.0001            7.09s\n",
      "       300           0.3644          -0.0000            5.32s\n",
      "       400           0.3454          -0.0000            3.54s\n",
      "       500           0.3297          -0.0000            1.77s\n",
      "       600           0.3027          -0.0000            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6525           0.0037           10.79s\n",
      "         2           0.6689           0.0032           10.64s\n",
      "         3           0.6653           0.0034           10.58s\n",
      "         4           0.6638           0.0034           10.61s\n",
      "         5           0.6773           0.0030           10.72s\n",
      "         6           0.6547           0.0030           10.75s\n",
      "         7           0.6645           0.0028           10.68s\n",
      "         8           0.6393           0.0030           10.70s\n",
      "         9           0.6488           0.0030           10.66s\n",
      "        10           0.6239           0.0031           10.67s\n",
      "        20           0.5999           0.0025           10.39s\n",
      "        30           0.5784           0.0019           10.17s\n",
      "        40           0.5345           0.0015           10.06s\n",
      "        50           0.5313           0.0014            9.90s\n",
      "        60           0.5052           0.0011            9.74s\n",
      "        70           0.4929           0.0008            9.58s\n",
      "        80           0.4655           0.0007            9.40s\n",
      "        90           0.4759           0.0005            9.20s\n",
      "       100           0.4626           0.0005            9.02s\n",
      "       200           0.3891           0.0000            7.25s\n",
      "       300           0.3618           0.0000            5.41s\n",
      "       400           0.3455          -0.0000            3.60s\n",
      "       500           0.3268           0.0000            1.80s\n",
      "       600           0.3118          -0.0001            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6640           0.0034           11.41s\n",
      "         2           0.6501           0.0032           11.06s\n",
      "         3           0.6575           0.0032           11.18s\n",
      "         4           0.6568           0.0032           11.11s\n",
      "         5           0.6620           0.0032           11.10s\n",
      "         6           0.6274           0.0033           11.06s\n",
      "         7           0.6412           0.0034           11.00s\n",
      "         8           0.6407           0.0029           10.98s\n",
      "         9           0.6251           0.0027           10.95s\n",
      "        10           0.6343           0.0030           10.94s\n",
      "        20           0.6007           0.0023           10.62s\n",
      "        30           0.5704           0.0021           10.44s\n",
      "        40           0.5486           0.0017           10.21s\n",
      "        50           0.5289           0.0013            9.98s\n",
      "        60           0.5011           0.0012            9.82s\n",
      "        70           0.4928           0.0008            9.65s\n",
      "        80           0.4889           0.0009            9.47s\n",
      "        90           0.4799           0.0008            9.30s\n",
      "       100           0.4670           0.0006            9.12s\n",
      "       200           0.3878           0.0000            7.20s\n",
      "       300           0.3548           0.0000            5.39s\n",
      "       400           0.3296          -0.0000            3.58s\n",
      "       500           0.3276          -0.0000            1.79s\n",
      "       600           0.3083          -0.0000            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6853           0.0034           11.14s\n",
      "         2           0.6460           0.0030           10.76s\n",
      "         3           0.6616           0.0034           10.68s\n",
      "         4           0.6588           0.0030           10.58s\n",
      "         5           0.6687           0.0031           10.63s\n",
      "         6           0.6514           0.0029           10.64s\n",
      "         7           0.6518           0.0029           10.63s\n",
      "         8           0.6392           0.0027           10.61s\n",
      "         9           0.6539           0.0027           10.63s\n",
      "        10           0.6462           0.0026           10.62s\n",
      "        20           0.6041           0.0025           10.46s\n",
      "        30           0.5798           0.0019           10.25s\n",
      "        40           0.5499           0.0017           10.07s\n",
      "        50           0.5145           0.0013            9.86s\n",
      "        60           0.5127           0.0010            9.68s\n",
      "        70           0.5226           0.0008            9.48s\n",
      "        80           0.4839           0.0007            9.30s\n",
      "        90           0.4785           0.0006            9.11s\n",
      "       100           0.4590           0.0005            8.94s\n",
      "       200           0.4039           0.0001            7.13s\n",
      "       300           0.3588          -0.0000            5.35s\n",
      "       400           0.3441          -0.0001            3.57s\n",
      "       500           0.3272          -0.0001            1.78s\n",
      "       600           0.3078          -0.0001            0.00s\n",
      "CV score: 0.47202637\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_49 = np.zeros(train_shape)\n",
    "predictions_gbr_49 = np.zeros(len(X_test_49))\n",
    "#GradientBoostingRegressor梯度提升决策树\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_49 = gbr(n_estimators=600, learning_rate=0.01,subsample=0.65,max_depth=6, min_samples_leaf=20,\n",
    "            max_features=0.35,verbose=1)\n",
    "    gbr_49.fit(tr_x,tr_y)\n",
    "    oof_gbr_49[val_idx] = gbr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_gbr_49 += gbr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上3种模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4696909101153167"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack3 = np.vstack([oof_lgb_49,oof_xgb_49,oof_gbr_49]).transpose()\n",
    "test_stack3 = np.vstack([predictions_lgb_49, predictions_xgb_49,predictions_gbr_49]).transpose()\n",
    "#\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack3 = np.zeros(train_stack3.shape[0])\n",
    "predictions_lr3 = np.zeros(test_stack3.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack3,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack3[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack3[val_idx], target.iloc[val_idx].values\n",
    "        #Kernel Ridge Regression\n",
    "    lr3 = kr()\n",
    "    lr3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack3[val_idx] = lr3.predict(val_data)\n",
    "    predictions_lr3 += lr3.predict(test_stack3) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们对于383维的数据进行与上述263以及49维数据相同的操作\n",
    "\n",
    "1. Kernel Ridge Regression 基于核的岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.51732109\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_383 = np.zeros(train_shape)\n",
    "predictions_kr_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #Kernel Ridge Regression 岭回归\n",
    "    kr_383 = kr()\n",
    "    kr_383.fit(tr_x,tr_y)\n",
    "    oof_kr_383[val_idx] = kr_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_kr_383 += kr_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用普通岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48782897\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_383 = np.zeros(train_shape)\n",
    "predictions_ridge_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #使用岭回归\n",
    "    ridge_383 = Ridge(alpha=1200)\n",
    "    ridge_383.fit(tr_x,tr_y)\n",
    "    oof_ridge_383[val_idx] = ridge_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_ridge_383 += ridge_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53650414\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_383 = np.zeros(train_shape)\n",
    "predictions_en_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #ElasticNet 弹性网络\n",
    "    en_383 = en(alpha=1.0,l1_ratio=0.06)\n",
    "    en_383.fit(tr_x,tr_y)\n",
    "    oof_en_383[val_idx] = en_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_en_383 += en_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 使用BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48828501\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_383 = np.zeros(train_shape)\n",
    "predictions_br_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #BayesianRidge 贝叶斯回归\n",
    "    br_383 = br()\n",
    "    br_383.fit(tr_x,tr_y)\n",
    "    oof_br_383[val_idx] = br_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_br_383 += br_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上4种模型的基于383个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4890155890702697"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack1 = np.vstack([oof_br_383,oof_kr_383,oof_en_383,oof_ridge_383]).transpose()\n",
    "test_stack1 = np.vstack([predictions_br_383, predictions_kr_383,predictions_en_383,predictions_ridge_383]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack1 = np.zeros(train_stack1.shape[0])\n",
    "predictions_lr1 = np.zeros(test_stack1.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack1,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack1[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack1[val_idx], target.iloc[val_idx].values\n",
    "    # LinearRegression简单的线性回归\n",
    "    lr1 = lr()\n",
    "    lr1.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack1[val_idx] = lr1.predict(val_data)\n",
    "    predictions_lr1 += lr1.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于49维的特征是最重要的特征，所以这里考虑增加更多的模型进行49维特征的数据的构建工作。\n",
    "1. KernelRidge 核岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.50446138\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_49 = np.zeros(train_shape)\n",
    "predictions_kr_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    kr_49 = kr()\n",
    "    kr_49.fit(tr_x,tr_y)\n",
    "    oof_kr_49[val_idx] = kr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_kr_49 += kr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ridge 岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49587513\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_49 = np.zeros(train_shape)\n",
    "predictions_ridge_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    ridge_49 = Ridge(alpha=6)\n",
    "    ridge_49.fit(tr_x,tr_y)\n",
    "    oof_ridge_49[val_idx] = ridge_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_ridge_49 += ridge_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49684901\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_49 = np.zeros(train_shape)\n",
    "predictions_br_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    br_49 = br()\n",
    "    br_49.fit(tr_x,tr_y)\n",
    "    oof_br_49[val_idx] = br_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_br_49 += br_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53979354\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_49 = np.zeros(train_shape)\n",
    "predictions_en_49 = np.zeros(len(X_test_49))\n",
    "#\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    en_49 = en(alpha=1.0,l1_ratio=0.05)\n",
    "    en_49.fit(tr_x,tr_y)\n",
    "    oof_en_49[val_idx] = en_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_en_49 += en_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了以上4种新模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49622456741436527"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack4 = np.vstack([oof_br_49,oof_kr_49,oof_en_49,oof_ridge_49]).transpose()\n",
    "test_stack4 = np.vstack([predictions_br_49, predictions_kr_49,predictions_en_49,predictions_ridge_49]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack4 = np.zeros(train_stack4.shape[0])\n",
    "predictions_lr4 = np.zeros(test_stack4.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack4,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack4[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack4[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr4 = lr()\n",
    "    lr4.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack4[val_idx] = lr4.predict(val_data)\n",
    "    predictions_lr4 += lr4.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合\n",
    "\n",
    "这里对于上述四种集成学习的模型的预测结果进行加权的求和，得到最终的结果，当然这种方式是很不准确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4548998418351435"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#和下面作对比\n",
    "mean_squared_error(target.values, 0.7*(0.6*oof_stack2 + 0.4*oof_stack3)+0.3*(0.55*oof_stack1+0.45*oof_stack4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更好的方式是将以上的4中集成学习模型再次进行集成学习的训练，这里直接使用LinearRegression简单线性回归的进行集成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4501006966464842"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack5 = np.vstack([oof_stack1,oof_stack2,oof_stack3,oof_stack4]).transpose()\n",
    "test_stack5 = np.vstack([predictions_lr1, predictions_lr2,predictions_lr3,predictions_lr4]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack5 = np.zeros(train_stack5.shape[0])\n",
    "predictions_lr5= np.zeros(test_stack5.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack5,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack5[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack5[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr5 = lr()\n",
    "    lr5.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack5[val_idx] = lr5.predict(val_data)\n",
    "    predictions_lr5 += lr5.predict(test_stack5) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果保存\n",
    "\n",
    "进行index的读取工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.879933\n",
       "std         0.460724\n",
       "min         1.600075\n",
       "25%         3.666202\n",
       "50%         3.955148\n",
       "75%         4.188459\n",
       "max         5.017111\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example = pd.read_csv('submit_example.csv',sep=',',encoding='latin-1')\n",
    "\n",
    "submit_example['happiness'] = predictions_lr5\n",
    "\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行结果保存，这里我们预测出的值是1-5的连续值，但是我们的ground truth是整数值，所以为了进一步优化我们的结果，我们对于结果进行了整数解的近似，并保存到了csv文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.879933\n",
       "std         0.460653\n",
       "min         1.600075\n",
       "25%         3.666202\n",
       "50%         3.955148\n",
       "75%         4.188459\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example.loc[submit_example['happiness']>4.96,'happiness']= 5\n",
    "submit_example.loc[submit_example['happiness']<=1.04,'happiness']= 1\n",
    "submit_example.loc[(submit_example['happiness']>1.96)&(submit_example['happiness']<2.04),'happiness']= 2\n",
    "\n",
    "submit_example.to_csv(\"submision.csv\",index=False)\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家可以对于model的参数进行更进一步的调整，例如使用网格搜索的方法。这留给大家做进一步的思考喽～"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
