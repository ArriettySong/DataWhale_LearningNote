{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习案例一 （幸福感预测）\n",
    "\n",
    "### 背景介绍\n",
    "\n",
    "幸福感是一个古老而深刻的话题，是人类世代追求的方向。与幸福感相关的因素成千上万、因人而异，大如国计民生，小如路边烤红薯，都会对幸福感产生影响。这些错综复杂的因素中，我们能找到其中的共性，一窥幸福感的要义吗？\n",
    "\n",
    "另外，在社会科学领域，幸福感的研究占有重要的位置。这个涉及了哲学、心理学、社会学、经济学等多方学科的话题复杂而有趣；同时与大家生活息息相关，每个人对幸福感都有自己的衡量标准。如果能发现影响幸福感的共性，生活中是不是将多一些乐趣；如果能找到影响幸福感的政策因素，便能优化资源配置来提升国民的幸福感。目前社会科学研究注重变量的可解释性和未来政策的落地，主要采用了线性回归和逻辑回归的方法，在收入、健康、职业、社交关系、休闲方式等经济人口因素；以及政府公共服务、宏观经济环境、税负等宏观因素上有了一系列的推测和发现。\n",
    "\n",
    "该案例为幸福感预测这一经典课题，希望在现有社会科学研究外有其他维度的算法尝试，结合多学科各自优势，挖掘潜在的影响因素，发现更多可解释、可理解的相关关系。\n",
    "\n",
    "具体来说，该案例就是一个数据挖掘类型的比赛——幸福感预测的baseline。具体来说，我们需要使用包括个体变量（性别、年龄、地域、职业、健康、婚姻与政治面貌等等）、家庭变量（父母、配偶、子女、家庭资本等等）、社会态度（公平、信用、公共服务等等）等139维度的信息来预测其对幸福感的影响。\n",
    "\n",
    "我们的数据来源于国家官方的《中国综合社会调查（CGSS）》文件中的调查结果中的数据，数据来源可靠可依赖:)\n",
    "\n",
    "### 数据信息\n",
    "赛题要求使用以上 **139** 维的特征，使用 **8000** 余组数据进行对于个人幸福感的预测（预测值为1，2，3，4，5，其中1代表幸福感最低，5代表幸福感最高）。\n",
    "因为考虑到变量个数较多，部分变量间关系复杂，数据分为完整版和精简版两类。可从精简版入手熟悉赛题后，使用完整版挖掘更多信息。在这里我直接使用了完整版的数据。赛题也给出了index文件中包含每个变量对应的问卷题目，以及变量取值的含义；survey文件中为原版问卷，作为补充以方便理解问题背景。\n",
    "\n",
    "### 评价指标\n",
    "最终的评价指标为均方误差MSE，即：\n",
    "$$Score = \\frac{1}{n} \\sum_1 ^n (y_i - y ^*)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error,mean_absolute_error, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import ExtraTreesRegressor as etr\n",
    "from sklearn.linear_model import BayesianRidge as br\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.kernel_ridge import KernelRidge as kr\n",
    "from sklearn.model_selection import  KFold, StratifiedKFold,GroupKFold, RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') #消除warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据集并查看数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据_处理前： (7988, 140)\n",
      "训练数据_处理后： (7988, 139)\n",
      "测试数据： (2968, 139)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\", parse_dates=['survey_time'],encoding='latin-1') \n",
    "test = pd.read_csv(\"test.csv\", parse_dates=['survey_time'],encoding='latin-1') #latin-1向下兼容ASCII\n",
    "#删去\"happiness\" 为-8的行\n",
    "train = train[train[\"happiness\"]!=-8].reset_index(drop=True) # reset_index:讲happiness为异常值的数据删除后，重置index\n",
    "train_data_copy = train.copy() \n",
    "target_col = \"happiness\" #目标列\n",
    "target = train_data_copy[target_col]\n",
    "del train_data_copy[target_col] #去除目标列\n",
    "print(\"训练数据_处理前：\",train.shape)\n",
    "print(\"训练数据_处理后：\",train_data_copy.shape)\n",
    "print(\"测试数据：\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据+测试数据： (10956, 139)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10956 entries, 0 to 10955\n",
      "Columns: 139 entries, id to public_service_9\n",
      "dtypes: datetime64[ns](1), float64(26), int64(109), object(3)\n",
      "memory usage: 11.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 拼接traindata和testdata\n",
    "data = pd.concat([train_data_copy,test],axis=0,ignore_index=True)\n",
    "data.head()\n",
    "print(\"训练数据+测试数据：\",data.shape)\n",
    "# print(\",\".join(train.columns))\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7988.000000\n",
       "mean        3.867927\n",
       "std         0.818717\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         4.000000\n",
       "75%         4.000000\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.happiness.describe() #训练集中幸福感的概况，四分位数为4，如此看来，大部分人都还挺幸福的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "#### 缺失值概况\n",
    "首先需要对于数据中的连续出现的负数值(即缺失值/异常值)进行处理。由于数据中的负数值只有-1，-2，-3，-8这几种数值，所以它们进行分别的操作，实现代码如下：\n",
    "\n",
    "<font color=red>以下代码仅统计各个样本中缺失值的特征数量，未统计各个特征中的缺失值比例.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数量+5=139+5=144\n",
    "#csv中有负数值：-1、-2、-3、-8，将他们视为有问题的特征，但是不删去\n",
    "def getres1(row):\n",
    "    return len([x for x in row.values if type(x)==int and x<0])\n",
    "\n",
    "def getres2(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-8])\n",
    "\n",
    "def getres3(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-1])\n",
    "\n",
    "def getres4(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-2])\n",
    "\n",
    "def getres5(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-3])\n",
    "\n",
    "#检查数据\n",
    "# axis=1 水平方向检查该样本的特征，统计其缺失特征的个数，超过20个的归为20.\n",
    "data['neg1'] = data[data.columns].apply(lambda row:getres1(row),axis=1)\n",
    "data.loc[data['neg1']>20,'neg1'] = 20  #平滑处理,最多出现20次\n",
    "\n",
    "data['neg2'] = data[data.columns].apply(lambda row:getres2(row),axis=1)\n",
    "data['neg3'] = data[data.columns].apply(lambda row:getres3(row),axis=1)\n",
    "data['neg4'] = data[data.columns].apply(lambda row:getres4(row),axis=1)\n",
    "data['neg5'] = data[data.columns].apply(lambda row:getres5(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各个样本的特征缺失情况：\n",
      "    neg1  neg2  neg3  neg4  neg5\n",
      "0     5     3     0     2     0\n",
      "1     0     0     0     0     0\n",
      "2     3     1     0     2     0\n",
      "3     2     0     0     2     0\n",
      "4     2     1     1     0     0\n",
      "5     1     0     0     1     0\n",
      "6     1     0     0     0     1\n",
      "7     5     2     1     2     0\n",
      "8     0     0     0     0     0\n",
      "9     1     1     0     0     0\n"
     ]
    }
   ],
   "source": [
    "print(\"各个样本的特征缺失情况：\\n\",data[['neg1','neg2','neg3','neg4','neg5']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缺失值填充\n",
    "填充缺失值，这里采取的方式是将缺失值补全，使用fillna(value)，其中value的数值根据具体的情况来确定。例如将大部分缺失信息认为是零，将家庭成员数认为是1，将家庭收入这个特征认为是66365，即所有家庭的收入平均值。部分实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填充缺失值 共25列 去掉4列 填充21列\n",
    "#以下的列都是缺省的，视情况填补\n",
    "data['work_status'] = data['work_status'].fillna(0)\n",
    "data['work_yr'] = data['work_yr'].fillna(0)\n",
    "data['work_manage'] = data['work_manage'].fillna(0)\n",
    "data['work_type'] = data['work_type'].fillna(0)\n",
    "\n",
    "data['edu_yr'] = data['edu_yr'].fillna(0)\n",
    "data['edu_status'] = data['edu_status'].fillna(0)\n",
    "\n",
    "data['s_work_type'] = data['s_work_type'].fillna(0)\n",
    "data['s_work_status'] = data['s_work_status'].fillna(0)\n",
    "data['s_political'] = data['s_political'].fillna(0)\n",
    "data['s_hukou'] = data['s_hukou'].fillna(0)\n",
    "data['s_income'] = data['s_income'].fillna(0)\n",
    "data['s_birth'] = data['s_birth'].fillna(0)\n",
    "data['s_edu'] = data['s_edu'].fillna(0)\n",
    "data['s_work_exper'] = data['s_work_exper'].fillna(0)\n",
    "\n",
    "data['minor_child'] = data['minor_child'].fillna(0)\n",
    "data['marital_now'] = data['marital_now'].fillna(0)\n",
    "data['marital_1st'] = data['marital_1st'].fillna(0)\n",
    "data['social_neighbor']=data['social_neighbor'].fillna(0)\n",
    "data['social_friend']=data['social_friend'].fillna(0)\n",
    "data['hukou_loc']=data['hukou_loc'].fillna(1) #最少为1，表示户口\n",
    "#data['family_income']=data['family_income'].fillna(66365) #删除问题值后的平均值？？\n",
    "data['family_income']=data['family_income'].fillna(data[data['family_income']>0]['family_income'].median()) # 家庭收入的中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剔除异常值样本_家庭年收入均值： 73785.026182261\n",
      "剔除异常值样本_家庭年收入中位数： 40000.0\n",
      "全部样本_家庭年收入均值： 66363.23110624315\n",
      "全部样本_家庭年收入中位数： 38580.0\n"
     ]
    }
   ],
   "source": [
    "#data.loc[data['family_income']==9999992,'family_income'] = -8\n",
    "#data['family_income'].max()\n",
    "print(\"剔除异常值样本_家庭年收入均值：\",data[data['family_income']>0]['family_income'].mean())\n",
    "print(\"剔除异常值样本_家庭年收入中位数：\",data[data['family_income']>0]['family_income'].median())\n",
    "print(\"全部样本_家庭年收入均值：\",data['family_income'].mean())\n",
    "print(\"全部样本_家庭年收入中位数：\",data['family_income'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，还有特殊格式的信息需要另外处理，比如与时间有关的信息，这里主要分为两部分进行处理：首先是将“连续”的年龄，进行分层处理，即划分年龄段，具体地在这里我们将年龄分为了6个区间。其次是计算具体的年龄，在Excel表格中，只有出生年月以及调查时间等信息，我们根据此计算出每一位调查者的真实年龄。具体实现代码如下：\n",
    "#### 特殊类型数据处理（时间类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  survey_time  birth\n",
      "0   56         2015   1959\n",
      "1   23         2015   1992\n",
      "2   48         2015   1967\n",
      "3   72         2015   1943\n",
      "4   21         2015   1994\n"
     ]
    }
   ],
   "source": [
    "#特征数量+1 = 144+1 =145\n",
    "#继续进行特殊的列进行数据处理\n",
    "#读happiness_index.xlsx\n",
    "data['survey_time'] = pd.to_datetime(data['survey_time'], format='%Y-%m-%d',errors='coerce')#防止时间格式不同的报错errors='coerce‘\n",
    "data['survey_time'] = data['survey_time'].dt.year #仅仅是year，方便计算年龄\n",
    "data['age'] = data['survey_time']-data['birth']\n",
    "print(data[['age','survey_time','birth']].head(5))\n",
    "#年龄分层 \n",
    "#特征数量+1 = 145+1=146\n",
    "bins = [0,17,26,34,50,63,100]\n",
    "data['age_bin'] = pd.cut(data['age'], bins, labels=[0,1,2,3,4,5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。第三种方法是使用我们日常生活中的真实情况，例如“宗教信息”特征为负数的认为是“不信仰宗教”，并认为“参加宗教活动的频率”为1，即没有参加过宗教活动，主观的进行补全，这也是我在这一步骤中使用最多的一种方式。就像我自己填表一样，这里我全部都使用了我自己的想法进行缺省值的补全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对‘宗教’处理\n",
    "data.loc[data['religion']<0,'religion'] = 1 #1为不信仰宗教\n",
    "data.loc[data['religion_freq']<0,'religion_freq'] = 1 #1为从来没有参加过\n",
    "#对‘教育程度’处理\n",
    "data.loc[data['edu']<0,'edu'] = 4 #初中\n",
    "data.loc[data['edu_status']<0,'edu_status'] = 0\n",
    "data.loc[data['edu_yr']<0,'edu_yr'] = 0\n",
    "#对‘个人收入’处理\n",
    "data.loc[data['income']<0,'income'] = 0 #认为无收入\n",
    "#对‘政治面貌’处理\n",
    "data.loc[data['political']<0,'political'] = 1 #认为是群众\n",
    "#对体重处理\n",
    "data.loc[(data['weight_jin']<=80)&(data['height_cm']>=160),'weight_jin']= data['weight_jin']*2\n",
    "data.loc[data['weight_jin']<=60,'weight_jin']= data['weight_jin']*2  #个人的想法，哈哈哈，没有60斤的成年人吧\n",
    "#对身高处理\n",
    "data.loc[data['height_cm']<150,'height_cm'] = 150 #成年人的实际情况\n",
    "#对‘健康’处理\n",
    "data.loc[data['health']<0,'health'] = 4 #认为是比较健康\n",
    "data.loc[data['health_problem']<0,'health_problem'] = 4\n",
    "#对‘沮丧’处理\n",
    "data.loc[data['depression']<0,'depression'] = 4 #一般人都是很少吧\n",
    "#对‘媒体’处理\n",
    "data.loc[data['media_1']<0,'media_1'] = 1 #都是从不\n",
    "data.loc[data['media_2']<0,'media_2'] = 1\n",
    "data.loc[data['media_3']<0,'media_3'] = 1\n",
    "data.loc[data['media_4']<0,'media_4'] = 1\n",
    "data.loc[data['media_5']<0,'media_5'] = 1\n",
    "data.loc[data['media_6']<0,'media_6'] = 1\n",
    "#对‘空闲活动’处理\n",
    "data.loc[data['leisure_1']<0,'leisure_1'] = 1 #都是根据自己的想法\n",
    "data.loc[data['leisure_2']<0,'leisure_2'] = 5\n",
    "data.loc[data['leisure_3']<0,'leisure_3'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用众数（代码中使用mode()来实现异常值的修正），由于这里的特征是空闲活动，所以采用众数对于缺失值进行处理比较合理。具体的代码参考如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['leisure_4']<0,'leisure_4'] = data['leisure_4'].mode() #取众数\n",
    "data.loc[data['leisure_5']<0,'leisure_5'] = data['leisure_5'].mode()\n",
    "data.loc[data['leisure_6']<0,'leisure_6'] = data['leisure_6'].mode()\n",
    "data.loc[data['leisure_7']<0,'leisure_7'] = data['leisure_7'].mode()\n",
    "data.loc[data['leisure_8']<0,'leisure_8'] = data['leisure_8'].mode()\n",
    "data.loc[data['leisure_9']<0,'leisure_9'] = data['leisure_9'].mode()\n",
    "data.loc[data['leisure_10']<0,'leisure_10'] = data['leisure_10'].mode()\n",
    "data.loc[data['leisure_11']<0,'leisure_11'] = data['leisure_11'].mode()\n",
    "data.loc[data['leisure_12']<0,'leisure_12'] = data['leisure_12'].mode()\n",
    "data.loc[data['socialize']<0,'socialize'] = 2 #很少\n",
    "data.loc[data['relax']<0,'relax'] = 4 #经常\n",
    "data.loc[data['learn']<0,'learn'] = 1 #从不，哈哈哈哈\n",
    "#对‘社交’处理\n",
    "data.loc[data['social_neighbor']<0,'social_neighbor'] = 0\n",
    "data.loc[data['social_friend']<0,'social_friend'] = 0\n",
    "data.loc[data['socia_outing']<0,'socia_outing'] = 1\n",
    "data.loc[data['neighbor_familiarity']<0,'social_neighbor']= 4\n",
    "#对‘社会公平性’处理\n",
    "data.loc[data['equity']<0,'equity'] = 3\n",
    "#对‘社会等级’处理\n",
    "data.loc[data['class_10_before']<0,'class_10_before'] = 3\n",
    "data.loc[data['class']<0,'class'] = 5\n",
    "data.loc[data['class_10_after']<0,'class_10_after'] = 5\n",
    "data.loc[data['class_14']<0,'class_14'] = 2\n",
    "#对‘工作情况’处理\n",
    "data.loc[data['work_status']<0,'work_status'] = 0\n",
    "data.loc[data['work_yr']<0,'work_yr'] = 0\n",
    "data.loc[data['work_manage']<0,'work_manage'] = 0\n",
    "data.loc[data['work_type']<0,'work_type'] = 0\n",
    "#对‘社会保障’处理\n",
    "data.loc[data['insur_1']<0,'insur_1'] = 1\n",
    "data.loc[data['insur_2']<0,'insur_2'] = 1\n",
    "data.loc[data['insur_3']<0,'insur_3'] = 1\n",
    "data.loc[data['insur_4']<0,'insur_4'] = 1\n",
    "data.loc[data['insur_1']==0,'insur_1'] = 0\n",
    "data.loc[data['insur_2']==0,'insur_2'] = 0\n",
    "data.loc[data['insur_3']==0,'insur_3'] = 0\n",
    "data.loc[data['insur_4']==0,'insur_4'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取均值进行缺失值的补全（代码实现为mean()），在这里因为家庭的收入是连续值，所以不能再使用取众数的方法进行处理，这里就直接使用了均值进行缺失值的补全。具体的代码参考如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对家庭情况处理\n",
    "family_income_mean = data['family_income'].mean()\n",
    "data.loc[data['family_income']<0,'family_income'] = family_income_mean\n",
    "data.loc[data['family_m']<0,'family_m'] = 2\n",
    "data.loc[data['family_status']<0,'family_status'] = 3\n",
    "data.loc[data['house']<0,'house'] = 1\n",
    "data.loc[data['car']<0,'car'] = 0\n",
    "data.loc[data['car']==2,'car'] = 0\n",
    "data.loc[data['son']<0,'son'] = 1\n",
    "data.loc[data['daughter']<0,'daughter'] = 0\n",
    "data.loc[data['minor_child']<0,'minor_child'] = 0\n",
    "#对‘婚姻’处理\n",
    "data.loc[data['marital_1st']<0,'marital_1st'] = 0\n",
    "data.loc[data['marital_now']<0,'marital_now'] = 0\n",
    "#对‘配偶’处理\n",
    "data.loc[data['s_birth']<0,'s_birth'] = 0\n",
    "data.loc[data['s_edu']<0,'s_edu'] = 0\n",
    "data.loc[data['s_political']<0,'s_political'] = 0\n",
    "data.loc[data['s_hukou']<0,'s_hukou'] = 0\n",
    "data.loc[data['s_income']<0,'s_income'] = 0\n",
    "data.loc[data['s_work_type']<0,'s_work_type'] = 0\n",
    "data.loc[data['s_work_status']<0,'s_work_status'] = 0\n",
    "data.loc[data['s_work_exper']<0,'s_work_exper'] = 0\n",
    "#对‘父母情况’处理\n",
    "data.loc[data['f_birth']<0,'f_birth'] = 1945\n",
    "data.loc[data['f_edu']<0,'f_edu'] = 1\n",
    "data.loc[data['f_political']<0,'f_political'] = 1\n",
    "data.loc[data['f_work_14']<0,'f_work_14'] = 2\n",
    "data.loc[data['m_birth']<0,'m_birth'] = 1940\n",
    "data.loc[data['m_edu']<0,'m_edu'] = 1\n",
    "data.loc[data['m_political']<0,'m_political'] = 1\n",
    "data.loc[data['m_work_14']<0,'m_work_14'] = 2\n",
    "#和同龄人相比社会经济地位\n",
    "data.loc[data['status_peer']<0,'status_peer'] = 2\n",
    "#和3年前比社会经济地位\n",
    "data.loc[data['status_3_before']<0,'status_3_before'] = 2\n",
    "#对‘观点’处理\n",
    "data.loc[data['view']<0,'view'] = 4\n",
    "#对期望年收入处理\n",
    "data.loc[data['inc_ability']<=0,'inc_ability']= 2\n",
    "inc_exp_mean = data['inc_exp'].mean()\n",
    "data.loc[data['inc_exp']<=0,'inc_exp']= inc_exp_mean #取均值\n",
    "\n",
    "#部分特征处理，取众数\n",
    "for i in range(1,9+1):\n",
    "    data.loc[data['public_service_'+str(i)]<0,'public_service_'+str(i)] = data['public_service_'+str(i)].dropna().mode()\n",
    "for i in range(1,13+1):\n",
    "    data.loc[data['trust_'+str(i)]<0,'trust_'+str(i)] = data['trust_'+str(i)].dropna().mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10956, 272)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征扩增\n",
    "\n",
    "这一步，我们需要进一步分析每一个特征之间的关系，从而进行数据增广。经过思考，这里我添加了如下的特征：第一次结婚年龄、最近结婚年龄、是否再婚、配偶年龄、配偶年龄差、各种收入比（与配偶之间的收入比、十年后预期收入与现在收入之比等等）、收入与住房面积比（其中也包括10年后期望收入等等各种情况）、社会阶级（10年后的社会阶级、14年后的社会阶级等等）、悠闲指数、满意指数、信任指数等等。除此之外，我还考虑了对于同一省、市、县进行了归一化。例如同一省市内的收入的平均值等以及一个个体相对于同省、市、县其他人的各个指标的情况。同时也考虑了对于同龄人之间的相互比较，即在同龄人中的收入情况、健康情况等等。具体的实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一次结婚年龄 特征数量+1 = 147\n",
    "data['marital_1stbir'] = data['marital_1st'] - data['birth'] \n",
    "#最近结婚年龄 特征数量+1 = 148\n",
    "data['marital_nowtbir'] = data['marital_now'] - data['birth'] \n",
    "#是否再婚 特征数量+1 = 149\n",
    "data['mar'] = data['marital_nowtbir'] - data['marital_1stbir']\n",
    "#配偶年龄 特征数量+1 = 150\n",
    "data['marital_sbir'] = data['marital_now']-data['s_birth']\n",
    "#配偶年龄差 特征数量+1 = 151\n",
    "data['age_'] = data['marital_nowtbir'] - data['marital_sbir'] \n",
    "\n",
    "#收入比 特征数量 = 151+7 =158\n",
    "data['income/s_income'] = data['income']/(data['s_income']+1)\n",
    "data['income+s_income'] = data['income']+(data['s_income']+1)\n",
    "data['income/family_income'] = data['income']/(data['family_income']+1)\n",
    "data['all_income/family_income'] = (data['income']+data['s_income'])/(data['family_income']+1)\n",
    "data['income/inc_exp'] = data['income']/(data['inc_exp']+1)\n",
    "data['family_income/m'] = data['family_income']/(data['family_m']+0.01)\n",
    "data['income/m'] = data['income']/(data['family_m']+0.01)\n",
    "\n",
    "#收入/面积比 特征数量 = 158+4=162\n",
    "data['income/floor_area'] = data['income']/(data['floor_area']+0.01)\n",
    "data['all_income/floor_area'] = (data['income']+data['s_income'])/(data['floor_area']+0.01)\n",
    "data['family_income/floor_area'] = data['family_income']/(data['floor_area']+0.01)\n",
    "data['floor_area/m'] = data['floor_area']/(data['family_m']+0.01)\n",
    "\n",
    "#社会等级 特征数量 = 162+3=165\n",
    "data['class_10_diff'] = (data['class_10_after'] - data['class'])\n",
    "data['class_diff'] = data['class'] - data['class_10_before']\n",
    "data['class_14_diff'] = data['class'] - data['class_14']\n",
    "#悠闲指数 特征数量+1 = 166\n",
    "leisure_fea_lis = ['leisure_'+str(i) for i in range(1,13)]\n",
    "data['leisure_sum'] = data[leisure_fea_lis].sum(axis=1) #skew\n",
    "#满意指数 特征数量+1 = 1167\n",
    "public_service_fea_lis = ['public_service_'+str(i) for i in range(1,10)]\n",
    "data['public_service_sum'] = data[public_service_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#信任指数 特征数量+1 = 1168\n",
    "trust_fea_lis = ['trust_'+str(i) for i in range(1,14)]\n",
    "data['trust_sum'] = data[trust_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#province mean 特征数量 = 168+13=181\n",
    "data['province_income_mean'] = data.groupby(['province'])['income'].transform('mean').values\n",
    "data['province_family_income_mean'] = data.groupby(['province'])['family_income'].transform('mean').values\n",
    "data['province_equity_mean'] = data.groupby(['province'])['equity'].transform('mean').values\n",
    "data['province_depression_mean'] = data.groupby(['province'])['depression'].transform('mean').values\n",
    "data['province_floor_area_mean'] = data.groupby(['province'])['floor_area'].transform('mean').values\n",
    "data['province_health_mean'] = data.groupby(['province'])['health'].transform('mean').values\n",
    "data['province_class_10_diff_mean'] = data.groupby(['province'])['class_10_diff'].transform('mean').values\n",
    "data['province_class_mean'] = data.groupby(['province'])['class'].transform('mean').values\n",
    "data['province_health_problem_mean'] = data.groupby(['province'])['health_problem'].transform('mean').values\n",
    "data['province_family_status_mean'] = data.groupby(['province'])['family_status'].transform('mean').values\n",
    "data['province_leisure_sum_mean'] = data.groupby(['province'])['leisure_sum'].transform('mean').values\n",
    "data['province_public_service_sum_mean'] = data.groupby(['province'])['public_service_sum'].transform('mean').values\n",
    "data['province_trust_sum_mean'] = data.groupby(['province'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#city   mean 特征数量 = 特征数量 = 181+13=194\n",
    "data['city_income_mean'] = data.groupby(['city'])['income'].transform('mean').values\n",
    "data['city_family_income_mean'] = data.groupby(['city'])['family_income'].transform('mean').values\n",
    "data['city_equity_mean'] = data.groupby(['city'])['equity'].transform('mean').values\n",
    "data['city_depression_mean'] = data.groupby(['city'])['depression'].transform('mean').values\n",
    "data['city_floor_area_mean'] = data.groupby(['city'])['floor_area'].transform('mean').values\n",
    "data['city_health_mean'] = data.groupby(['city'])['health'].transform('mean').values\n",
    "data['city_class_10_diff_mean'] = data.groupby(['city'])['class_10_diff'].transform('mean').values\n",
    "data['city_class_mean'] = data.groupby(['city'])['class'].transform('mean').values\n",
    "data['city_health_problem_mean'] = data.groupby(['city'])['health_problem'].transform('mean').values\n",
    "data['city_family_status_mean'] = data.groupby(['city'])['family_status'].transform('mean').values\n",
    "data['city_leisure_sum_mean'] = data.groupby(['city'])['leisure_sum'].transform('mean').values\n",
    "data['city_public_service_sum_mean'] = data.groupby(['city'])['public_service_sum'].transform('mean').values\n",
    "data['city_trust_sum_mean'] = data.groupby(['city'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#county  mean 特征数量 = 194 + 13 = 207\n",
    "data['county_income_mean'] = data.groupby(['county'])['income'].transform('mean').values\n",
    "data['county_family_income_mean'] = data.groupby(['county'])['family_income'].transform('mean').values\n",
    "data['county_equity_mean'] = data.groupby(['county'])['equity'].transform('mean').values\n",
    "data['county_depression_mean'] = data.groupby(['county'])['depression'].transform('mean').values\n",
    "data['county_floor_area_mean'] = data.groupby(['county'])['floor_area'].transform('mean').values\n",
    "data['county_health_mean'] = data.groupby(['county'])['health'].transform('mean').values\n",
    "data['county_class_10_diff_mean'] = data.groupby(['county'])['class_10_diff'].transform('mean').values\n",
    "data['county_class_mean'] = data.groupby(['county'])['class'].transform('mean').values\n",
    "data['county_health_problem_mean'] = data.groupby(['county'])['health_problem'].transform('mean').values\n",
    "data['county_family_status_mean'] = data.groupby(['county'])['family_status'].transform('mean').values\n",
    "data['county_leisure_sum_mean'] = data.groupby(['county'])['leisure_sum'].transform('mean').values\n",
    "data['county_public_service_sum_mean'] = data.groupby(['county'])['public_service_sum'].transform('mean').values\n",
    "data['county_trust_sum_mean'] = data.groupby(['county'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#ratio 相比同省 特征数量 = 207 + 13 =220\n",
    "data['income/province'] = data['income']/(data['province_income_mean'])                                      \n",
    "data['family_income/province'] = data['family_income']/(data['province_family_income_mean'])   \n",
    "data['equity/province'] = data['equity']/(data['province_equity_mean'])       \n",
    "data['depression/province'] = data['depression']/(data['province_depression_mean'])                                                \n",
    "data['floor_area/province'] = data['floor_area']/(data['province_floor_area_mean'])\n",
    "data['health/province'] = data['health']/(data['province_health_mean'])\n",
    "data['class_10_diff/province'] = data['class_10_diff']/(data['province_class_10_diff_mean'])\n",
    "data['class/province'] = data['class']/(data['province_class_mean'])\n",
    "data['health_problem/province'] = data['health_problem']/(data['province_health_problem_mean'])\n",
    "data['family_status/province'] = data['family_status']/(data['province_family_status_mean'])\n",
    "data['leisure_sum/province'] = data['leisure_sum']/(data['province_leisure_sum_mean'])\n",
    "data['public_service_sum/province'] = data['public_service_sum']/(data['province_public_service_sum_mean'])\n",
    "data['trust_sum/province'] = data['trust_sum']/(data['province_trust_sum_mean']+1)\n",
    "\n",
    "#ratio 相比同市 特征数量 = 220 + 13 =233\n",
    "data['income/city'] = data['income']/(data['city_income_mean'])                                      \n",
    "data['family_income/city'] = data['family_income']/(data['city_family_income_mean'])   \n",
    "data['equity/city'] = data['equity']/(data['city_equity_mean'])       \n",
    "data['depression/city'] = data['depression']/(data['city_depression_mean'])                                                \n",
    "data['floor_area/city'] = data['floor_area']/(data['city_floor_area_mean'])\n",
    "data['health/city'] = data['health']/(data['city_health_mean'])\n",
    "data['class_10_diff/city'] = data['class_10_diff']/(data['city_class_10_diff_mean'])\n",
    "data['class/city'] = data['class']/(data['city_class_mean'])\n",
    "data['health_problem/city'] = data['health_problem']/(data['city_health_problem_mean'])\n",
    "data['family_status/city'] = data['family_status']/(data['city_family_status_mean'])\n",
    "data['leisure_sum/city'] = data['leisure_sum']/(data['city_leisure_sum_mean'])\n",
    "data['public_service_sum/city'] = data['public_service_sum']/(data['city_public_service_sum_mean'])\n",
    "data['trust_sum/city'] = data['trust_sum']/(data['city_trust_sum_mean'])\n",
    "\n",
    "#ratio 相比同个地区 特征数量 = 233 + 13 =246\n",
    "data['income/county'] = data['income']/(data['county_income_mean'])                                      \n",
    "data['family_income/county'] = data['family_income']/(data['county_family_income_mean'])   \n",
    "data['equity/county'] = data['equity']/(data['county_equity_mean'])       \n",
    "data['depression/county'] = data['depression']/(data['county_depression_mean'])                                                \n",
    "data['floor_area/county'] = data['floor_area']/(data['county_floor_area_mean'])\n",
    "data['health/county'] = data['health']/(data['county_health_mean'])\n",
    "data['class_10_diff/county'] = data['class_10_diff']/(data['county_class_10_diff_mean'])\n",
    "data['class/county'] = data['class']/(data['county_class_mean'])\n",
    "data['health_problem/county'] = data['health_problem']/(data['county_health_problem_mean'])\n",
    "data['family_status/county'] = data['family_status']/(data['county_family_status_mean'])\n",
    "data['leisure_sum/county'] = data['leisure_sum']/(data['county_leisure_sum_mean'])\n",
    "data['public_service_sum/county'] = data['public_service_sum']/(data['county_public_service_sum_mean'])\n",
    "data['trust_sum/county'] = data['trust_sum']/(data['county_trust_sum_mean'])\n",
    "\n",
    "#age   mean 特征数量 = 246+ 13 =259\n",
    "data['age_income_mean'] = data.groupby(['age'])['income'].transform('mean').values\n",
    "data['age_family_income_mean'] = data.groupby(['age'])['family_income'].transform('mean').values\n",
    "data['age_equity_mean'] = data.groupby(['age'])['equity'].transform('mean').values\n",
    "data['age_depression_mean'] = data.groupby(['age'])['depression'].transform('mean').values\n",
    "data['age_floor_area_mean'] = data.groupby(['age'])['floor_area'].transform('mean').values\n",
    "data['age_health_mean'] = data.groupby(['age'])['health'].transform('mean').values\n",
    "data['age_class_10_diff_mean'] = data.groupby(['age'])['class_10_diff'].transform('mean').values\n",
    "data['age_class_mean'] = data.groupby(['age'])['class'].transform('mean').values\n",
    "data['age_health_problem_mean'] = data.groupby(['age'])['health_problem'].transform('mean').values\n",
    "data['age_family_status_mean'] = data.groupby(['age'])['family_status'].transform('mean').values\n",
    "data['age_leisure_sum_mean'] = data.groupby(['age'])['leisure_sum'].transform('mean').values\n",
    "data['age_public_service_sum_mean'] = data.groupby(['age'])['public_service_sum'].transform('mean').values\n",
    "data['age_trust_sum_mean'] = data.groupby(['age'])['trust_sum'].transform('mean').values\n",
    "\n",
    "# 和同龄人相比  特征数量 = 259 + 13 =272\n",
    "data['income/age'] = data['income']/(data['age_income_mean'])                                      \n",
    "data['family_income/age'] = data['family_income']/(data['age_family_income_mean'])   \n",
    "data['equity/age'] = data['equity']/(data['age_equity_mean'])       \n",
    "data['depression/age'] = data['depression']/(data['age_depression_mean'])                                                \n",
    "data['floor_area/age'] = data['floor_area']/(data['age_floor_area_mean'])\n",
    "data['health/age'] = data['health']/(data['age_health_mean'])\n",
    "data['class_10_diff/age'] = data['class_10_diff']/(data['age_class_10_diff_mean'])\n",
    "data['class/age'] = data['class']/(data['age_class_mean'])\n",
    "data['health_problem/age'] = data['health_problem']/(data['age_health_problem_mean'])\n",
    "data['family_status/age'] = data['family_status']/(data['age_family_status_mean'])\n",
    "data['leisure_sum/age'] = data['leisure_sum']/(data['age_leisure_sum_mean'])\n",
    "data['public_service_sum/age'] = data['public_service_sum']/(data['age_public_service_sum_mean'])\n",
    "data['trust_sum/age'] = data['trust_sum']/(data['age_trust_sum_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过如上的操作后，最终我们的特征从一开始的131维，扩充为了272维的特征。接下来考虑特征工程、训练模型以及模型融合的工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据的shape: (10956, 272)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey_type</th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>survey_time</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>religion</th>\n",
       "      <th>religion_freq</th>\n",
       "      <th>edu</th>\n",
       "      <th>edu_other</th>\n",
       "      <th>edu_status</th>\n",
       "      <th>edu_yr</th>\n",
       "      <th>income</th>\n",
       "      <th>political</th>\n",
       "      <th>join_party</th>\n",
       "      <th>floor_area</th>\n",
       "      <th>property_0</th>\n",
       "      <th>property_1</th>\n",
       "      <th>property_2</th>\n",
       "      <th>property_3</th>\n",
       "      <th>property_4</th>\n",
       "      <th>property_5</th>\n",
       "      <th>property_6</th>\n",
       "      <th>property_7</th>\n",
       "      <th>property_8</th>\n",
       "      <th>property_other</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_jin</th>\n",
       "      <th>health</th>\n",
       "      <th>health_problem</th>\n",
       "      <th>depression</th>\n",
       "      <th>hukou</th>\n",
       "      <th>hukou_loc</th>\n",
       "      <th>media_1</th>\n",
       "      <th>media_2</th>\n",
       "      <th>media_3</th>\n",
       "      <th>media_4</th>\n",
       "      <th>media_5</th>\n",
       "      <th>media_6</th>\n",
       "      <th>leisure_1</th>\n",
       "      <th>leisure_2</th>\n",
       "      <th>leisure_3</th>\n",
       "      <th>leisure_4</th>\n",
       "      <th>leisure_5</th>\n",
       "      <th>leisure_6</th>\n",
       "      <th>leisure_7</th>\n",
       "      <th>leisure_8</th>\n",
       "      <th>leisure_9</th>\n",
       "      <th>leisure_10</th>\n",
       "      <th>leisure_11</th>\n",
       "      <th>leisure_12</th>\n",
       "      <th>socialize</th>\n",
       "      <th>relax</th>\n",
       "      <th>learn</th>\n",
       "      <th>social_neighbor</th>\n",
       "      <th>social_friend</th>\n",
       "      <th>socia_outing</th>\n",
       "      <th>equity</th>\n",
       "      <th>class</th>\n",
       "      <th>class_10_before</th>\n",
       "      <th>class_10_after</th>\n",
       "      <th>class_14</th>\n",
       "      <th>work_exper</th>\n",
       "      <th>work_status</th>\n",
       "      <th>work_yr</th>\n",
       "      <th>work_type</th>\n",
       "      <th>work_manage</th>\n",
       "      <th>insur_1</th>\n",
       "      <th>insur_2</th>\n",
       "      <th>insur_3</th>\n",
       "      <th>insur_4</th>\n",
       "      <th>family_income</th>\n",
       "      <th>family_m</th>\n",
       "      <th>family_status</th>\n",
       "      <th>house</th>\n",
       "      <th>car</th>\n",
       "      <th>invest_0</th>\n",
       "      <th>invest_1</th>\n",
       "      <th>invest_2</th>\n",
       "      <th>invest_3</th>\n",
       "      <th>invest_4</th>\n",
       "      <th>invest_5</th>\n",
       "      <th>invest_6</th>\n",
       "      <th>invest_7</th>\n",
       "      <th>invest_8</th>\n",
       "      <th>invest_other</th>\n",
       "      <th>son</th>\n",
       "      <th>daughter</th>\n",
       "      <th>minor_child</th>\n",
       "      <th>marital</th>\n",
       "      <th>marital_1st</th>\n",
       "      <th>s_birth</th>\n",
       "      <th>marital_now</th>\n",
       "      <th>s_edu</th>\n",
       "      <th>s_political</th>\n",
       "      <th>s_hukou</th>\n",
       "      <th>s_income</th>\n",
       "      <th>s_work_exper</th>\n",
       "      <th>s_work_status</th>\n",
       "      <th>s_work_type</th>\n",
       "      <th>f_birth</th>\n",
       "      <th>f_edu</th>\n",
       "      <th>f_political</th>\n",
       "      <th>f_work_14</th>\n",
       "      <th>m_birth</th>\n",
       "      <th>m_edu</th>\n",
       "      <th>m_political</th>\n",
       "      <th>m_work_14</th>\n",
       "      <th>status_peer</th>\n",
       "      <th>status_3_before</th>\n",
       "      <th>view</th>\n",
       "      <th>inc_ability</th>\n",
       "      <th>inc_exp</th>\n",
       "      <th>trust_1</th>\n",
       "      <th>trust_2</th>\n",
       "      <th>trust_3</th>\n",
       "      <th>trust_4</th>\n",
       "      <th>trust_5</th>\n",
       "      <th>trust_6</th>\n",
       "      <th>trust_7</th>\n",
       "      <th>trust_8</th>\n",
       "      <th>trust_9</th>\n",
       "      <th>trust_10</th>\n",
       "      <th>trust_11</th>\n",
       "      <th>trust_12</th>\n",
       "      <th>trust_13</th>\n",
       "      <th>neighbor_familiarity</th>\n",
       "      <th>public_service_1</th>\n",
       "      <th>public_service_2</th>\n",
       "      <th>public_service_3</th>\n",
       "      <th>public_service_4</th>\n",
       "      <th>public_service_5</th>\n",
       "      <th>public_service_6</th>\n",
       "      <th>public_service_7</th>\n",
       "      <th>public_service_8</th>\n",
       "      <th>public_service_9</th>\n",
       "      <th>neg1</th>\n",
       "      <th>neg2</th>\n",
       "      <th>neg3</th>\n",
       "      <th>neg4</th>\n",
       "      <th>neg5</th>\n",
       "      <th>age</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>marital_1stbir</th>\n",
       "      <th>marital_nowtbir</th>\n",
       "      <th>mar</th>\n",
       "      <th>marital_sbir</th>\n",
       "      <th>age_</th>\n",
       "      <th>income/s_income</th>\n",
       "      <th>income+s_income</th>\n",
       "      <th>income/family_income</th>\n",
       "      <th>all_income/family_income</th>\n",
       "      <th>income/inc_exp</th>\n",
       "      <th>family_income/m</th>\n",
       "      <th>income/m</th>\n",
       "      <th>income/floor_area</th>\n",
       "      <th>all_income/floor_area</th>\n",
       "      <th>family_income/floor_area</th>\n",
       "      <th>floor_area/m</th>\n",
       "      <th>class_10_diff</th>\n",
       "      <th>class_diff</th>\n",
       "      <th>class_14_diff</th>\n",
       "      <th>leisure_sum</th>\n",
       "      <th>public_service_sum</th>\n",
       "      <th>trust_sum</th>\n",
       "      <th>province_income_mean</th>\n",
       "      <th>province_family_income_mean</th>\n",
       "      <th>province_equity_mean</th>\n",
       "      <th>province_depression_mean</th>\n",
       "      <th>province_floor_area_mean</th>\n",
       "      <th>province_health_mean</th>\n",
       "      <th>province_class_10_diff_mean</th>\n",
       "      <th>province_class_mean</th>\n",
       "      <th>province_health_problem_mean</th>\n",
       "      <th>province_family_status_mean</th>\n",
       "      <th>province_leisure_sum_mean</th>\n",
       "      <th>province_public_service_sum_mean</th>\n",
       "      <th>province_trust_sum_mean</th>\n",
       "      <th>city_income_mean</th>\n",
       "      <th>city_family_income_mean</th>\n",
       "      <th>city_equity_mean</th>\n",
       "      <th>city_depression_mean</th>\n",
       "      <th>city_floor_area_mean</th>\n",
       "      <th>city_health_mean</th>\n",
       "      <th>city_class_10_diff_mean</th>\n",
       "      <th>city_class_mean</th>\n",
       "      <th>city_health_problem_mean</th>\n",
       "      <th>city_family_status_mean</th>\n",
       "      <th>city_leisure_sum_mean</th>\n",
       "      <th>city_public_service_sum_mean</th>\n",
       "      <th>city_trust_sum_mean</th>\n",
       "      <th>county_income_mean</th>\n",
       "      <th>county_family_income_mean</th>\n",
       "      <th>county_equity_mean</th>\n",
       "      <th>county_depression_mean</th>\n",
       "      <th>county_floor_area_mean</th>\n",
       "      <th>county_health_mean</th>\n",
       "      <th>county_class_10_diff_mean</th>\n",
       "      <th>county_class_mean</th>\n",
       "      <th>county_health_problem_mean</th>\n",
       "      <th>county_family_status_mean</th>\n",
       "      <th>county_leisure_sum_mean</th>\n",
       "      <th>county_public_service_sum_mean</th>\n",
       "      <th>county_trust_sum_mean</th>\n",
       "      <th>income/province</th>\n",
       "      <th>family_income/province</th>\n",
       "      <th>equity/province</th>\n",
       "      <th>depression/province</th>\n",
       "      <th>floor_area/province</th>\n",
       "      <th>health/province</th>\n",
       "      <th>class_10_diff/province</th>\n",
       "      <th>class/province</th>\n",
       "      <th>health_problem/province</th>\n",
       "      <th>family_status/province</th>\n",
       "      <th>leisure_sum/province</th>\n",
       "      <th>public_service_sum/province</th>\n",
       "      <th>trust_sum/province</th>\n",
       "      <th>income/city</th>\n",
       "      <th>family_income/city</th>\n",
       "      <th>equity/city</th>\n",
       "      <th>depression/city</th>\n",
       "      <th>floor_area/city</th>\n",
       "      <th>health/city</th>\n",
       "      <th>class_10_diff/city</th>\n",
       "      <th>class/city</th>\n",
       "      <th>health_problem/city</th>\n",
       "      <th>family_status/city</th>\n",
       "      <th>leisure_sum/city</th>\n",
       "      <th>public_service_sum/city</th>\n",
       "      <th>trust_sum/city</th>\n",
       "      <th>income/county</th>\n",
       "      <th>family_income/county</th>\n",
       "      <th>equity/county</th>\n",
       "      <th>depression/county</th>\n",
       "      <th>floor_area/county</th>\n",
       "      <th>health/county</th>\n",
       "      <th>class_10_diff/county</th>\n",
       "      <th>class/county</th>\n",
       "      <th>health_problem/county</th>\n",
       "      <th>family_status/county</th>\n",
       "      <th>leisure_sum/county</th>\n",
       "      <th>public_service_sum/county</th>\n",
       "      <th>trust_sum/county</th>\n",
       "      <th>age_income_mean</th>\n",
       "      <th>age_family_income_mean</th>\n",
       "      <th>age_equity_mean</th>\n",
       "      <th>age_depression_mean</th>\n",
       "      <th>age_floor_area_mean</th>\n",
       "      <th>age_health_mean</th>\n",
       "      <th>age_class_10_diff_mean</th>\n",
       "      <th>age_class_mean</th>\n",
       "      <th>age_health_problem_mean</th>\n",
       "      <th>age_family_status_mean</th>\n",
       "      <th>age_leisure_sum_mean</th>\n",
       "      <th>age_public_service_sum_mean</th>\n",
       "      <th>age_trust_sum_mean</th>\n",
       "      <th>income/age</th>\n",
       "      <th>family_income/age</th>\n",
       "      <th>equity/age</th>\n",
       "      <th>depression/age</th>\n",
       "      <th>floor_area/age</th>\n",
       "      <th>health/age</th>\n",
       "      <th>class_10_diff/age</th>\n",
       "      <th>class/age</th>\n",
       "      <th>health_problem/age</th>\n",
       "      <th>family_status/age</th>\n",
       "      <th>leisure_sum/age</th>\n",
       "      <th>public_service_sum/age</th>\n",
       "      <th>trust_sum/age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>29850.746269</td>\n",
       "      <td>9950.248756</td>\n",
       "      <td>444.345701</td>\n",
       "      <td>1333.037103</td>\n",
       "      <td>1333.037103</td>\n",
       "      <td>22.388060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>61859.505703</td>\n",
       "      <td>131638.458098</td>\n",
       "      <td>2.773764</td>\n",
       "      <td>3.954373</td>\n",
       "      <td>88.692205</td>\n",
       "      <td>3.701521</td>\n",
       "      <td>1.127376</td>\n",
       "      <td>4.572243</td>\n",
       "      <td>4.051331</td>\n",
       "      <td>2.716730</td>\n",
       "      <td>41.701521</td>\n",
       "      <td>506.456274</td>\n",
       "      <td>35.526616</td>\n",
       "      <td>43096.656716</td>\n",
       "      <td>93764.077651</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>4.011940</td>\n",
       "      <td>82.744776</td>\n",
       "      <td>3.650746</td>\n",
       "      <td>1.164179</td>\n",
       "      <td>4.465672</td>\n",
       "      <td>4.113433</td>\n",
       "      <td>2.665672</td>\n",
       "      <td>40.059701</td>\n",
       "      <td>542.671642</td>\n",
       "      <td>34.979104</td>\n",
       "      <td>28979.591837</td>\n",
       "      <td>60630.401904</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>4.081633</td>\n",
       "      <td>38.530612</td>\n",
       "      <td>3.489796</td>\n",
       "      <td>1.183673</td>\n",
       "      <td>4.938776</td>\n",
       "      <td>3.877551</td>\n",
       "      <td>2.489796</td>\n",
       "      <td>36.693878</td>\n",
       "      <td>559.163265</td>\n",
       "      <td>31.510204</td>\n",
       "      <td>0.323313</td>\n",
       "      <td>0.455794</td>\n",
       "      <td>1.081563</td>\n",
       "      <td>1.264423</td>\n",
       "      <td>0.507373</td>\n",
       "      <td>0.810478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656133</td>\n",
       "      <td>0.493665</td>\n",
       "      <td>0.736179</td>\n",
       "      <td>0.791338</td>\n",
       "      <td>0.829292</td>\n",
       "      <td>1.067715</td>\n",
       "      <td>0.464073</td>\n",
       "      <td>0.639904</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.246280</td>\n",
       "      <td>0.543841</td>\n",
       "      <td>0.821750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671791</td>\n",
       "      <td>0.486212</td>\n",
       "      <td>0.750280</td>\n",
       "      <td>0.823770</td>\n",
       "      <td>0.773949</td>\n",
       "      <td>1.114951</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.989603</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>1.167903</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.899333</td>\n",
       "      <td>0.751122</td>\n",
       "      <td>1.237694</td>\n",
       "      <td>24371.808219</td>\n",
       "      <td>68176.681796</td>\n",
       "      <td>3.061644</td>\n",
       "      <td>3.890411</td>\n",
       "      <td>109.662329</td>\n",
       "      <td>3.534247</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>4.390411</td>\n",
       "      <td>3.835616</td>\n",
       "      <td>2.726027</td>\n",
       "      <td>45.541096</td>\n",
       "      <td>608.657534</td>\n",
       "      <td>36.109589</td>\n",
       "      <td>0.820620</td>\n",
       "      <td>0.880066</td>\n",
       "      <td>0.979866</td>\n",
       "      <td>1.285211</td>\n",
       "      <td>0.410351</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683307</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>0.733668</td>\n",
       "      <td>0.724620</td>\n",
       "      <td>0.690043</td>\n",
       "      <td>1.080046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1972</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1973</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1992.0</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20001.0</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>13289.036545</td>\n",
       "      <td>6644.518272</td>\n",
       "      <td>181.801654</td>\n",
       "      <td>181.801654</td>\n",
       "      <td>363.603309</td>\n",
       "      <td>36.544850</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>29806.230241</td>\n",
       "      <td>37758.898778</td>\n",
       "      <td>3.249141</td>\n",
       "      <td>3.991409</td>\n",
       "      <td>148.463918</td>\n",
       "      <td>3.955326</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>4.414089</td>\n",
       "      <td>3.960481</td>\n",
       "      <td>2.737113</td>\n",
       "      <td>47.194158</td>\n",
       "      <td>625.233677</td>\n",
       "      <td>41.434708</td>\n",
       "      <td>64016.125654</td>\n",
       "      <td>34201.145410</td>\n",
       "      <td>3.371728</td>\n",
       "      <td>3.989529</td>\n",
       "      <td>157.528796</td>\n",
       "      <td>3.968586</td>\n",
       "      <td>0.942408</td>\n",
       "      <td>4.335079</td>\n",
       "      <td>3.963351</td>\n",
       "      <td>2.811518</td>\n",
       "      <td>48.303665</td>\n",
       "      <td>634.460733</td>\n",
       "      <td>41.732984</td>\n",
       "      <td>11628.659794</td>\n",
       "      <td>34758.996059</td>\n",
       "      <td>3.350515</td>\n",
       "      <td>3.958763</td>\n",
       "      <td>154.608247</td>\n",
       "      <td>3.927835</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>4.123711</td>\n",
       "      <td>3.948454</td>\n",
       "      <td>2.752577</td>\n",
       "      <td>48.628866</td>\n",
       "      <td>629.123711</td>\n",
       "      <td>42.731959</td>\n",
       "      <td>0.671001</td>\n",
       "      <td>1.059353</td>\n",
       "      <td>0.923321</td>\n",
       "      <td>0.751614</td>\n",
       "      <td>0.740921</td>\n",
       "      <td>1.264118</td>\n",
       "      <td>2.063830</td>\n",
       "      <td>1.359284</td>\n",
       "      <td>1.009978</td>\n",
       "      <td>1.461394</td>\n",
       "      <td>0.826373</td>\n",
       "      <td>1.079596</td>\n",
       "      <td>1.013321</td>\n",
       "      <td>0.312421</td>\n",
       "      <td>1.169551</td>\n",
       "      <td>0.889752</td>\n",
       "      <td>0.751969</td>\n",
       "      <td>0.698285</td>\n",
       "      <td>1.259894</td>\n",
       "      <td>2.122222</td>\n",
       "      <td>1.384058</td>\n",
       "      <td>1.009247</td>\n",
       "      <td>1.422719</td>\n",
       "      <td>0.807392</td>\n",
       "      <td>1.063896</td>\n",
       "      <td>1.030360</td>\n",
       "      <td>1.719889</td>\n",
       "      <td>1.150781</td>\n",
       "      <td>0.895385</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.711476</td>\n",
       "      <td>1.272966</td>\n",
       "      <td>2.179775</td>\n",
       "      <td>1.455000</td>\n",
       "      <td>1.013055</td>\n",
       "      <td>1.453184</td>\n",
       "      <td>0.801993</td>\n",
       "      <td>1.072921</td>\n",
       "      <td>1.006273</td>\n",
       "      <td>20771.900826</td>\n",
       "      <td>71277.916142</td>\n",
       "      <td>3.157025</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>115.446281</td>\n",
       "      <td>4.239669</td>\n",
       "      <td>1.975207</td>\n",
       "      <td>4.462810</td>\n",
       "      <td>4.487603</td>\n",
       "      <td>2.942149</td>\n",
       "      <td>38.545455</td>\n",
       "      <td>579.082645</td>\n",
       "      <td>39.107438</td>\n",
       "      <td>0.962839</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.950262</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.952824</td>\n",
       "      <td>1.179337</td>\n",
       "      <td>1.012552</td>\n",
       "      <td>1.344444</td>\n",
       "      <td>0.891344</td>\n",
       "      <td>1.359551</td>\n",
       "      <td>1.011792</td>\n",
       "      <td>1.165637</td>\n",
       "      <td>1.099535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>126</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333278</td>\n",
       "      <td>8001.0</td>\n",
       "      <td>0.249969</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>2657.807309</td>\n",
       "      <td>664.451827</td>\n",
       "      <td>16.665278</td>\n",
       "      <td>66.661112</td>\n",
       "      <td>66.661112</td>\n",
       "      <td>39.867110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16246.747967</td>\n",
       "      <td>95335.856164</td>\n",
       "      <td>3.298103</td>\n",
       "      <td>3.647696</td>\n",
       "      <td>107.902439</td>\n",
       "      <td>3.582656</td>\n",
       "      <td>1.029810</td>\n",
       "      <td>4.243902</td>\n",
       "      <td>3.596206</td>\n",
       "      <td>2.607046</td>\n",
       "      <td>46.653117</td>\n",
       "      <td>623.013550</td>\n",
       "      <td>36.200542</td>\n",
       "      <td>6522.105263</td>\n",
       "      <td>32477.297170</td>\n",
       "      <td>3.294737</td>\n",
       "      <td>3.957895</td>\n",
       "      <td>120.684211</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.505263</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>2.768421</td>\n",
       "      <td>48.263158</td>\n",
       "      <td>654.547368</td>\n",
       "      <td>38.778947</td>\n",
       "      <td>6522.105263</td>\n",
       "      <td>32477.297170</td>\n",
       "      <td>3.294737</td>\n",
       "      <td>3.957895</td>\n",
       "      <td>120.684211</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.505263</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>2.768421</td>\n",
       "      <td>48.263158</td>\n",
       "      <td>654.547368</td>\n",
       "      <td>38.778947</td>\n",
       "      <td>0.123102</td>\n",
       "      <td>0.083914</td>\n",
       "      <td>1.212818</td>\n",
       "      <td>1.370728</td>\n",
       "      <td>1.112116</td>\n",
       "      <td>1.116490</td>\n",
       "      <td>0.971053</td>\n",
       "      <td>1.178161</td>\n",
       "      <td>1.112283</td>\n",
       "      <td>1.150728</td>\n",
       "      <td>0.964566</td>\n",
       "      <td>1.202221</td>\n",
       "      <td>0.940847</td>\n",
       "      <td>0.306649</td>\n",
       "      <td>0.246326</td>\n",
       "      <td>1.214058</td>\n",
       "      <td>1.263298</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.109813</td>\n",
       "      <td>1.041096</td>\n",
       "      <td>1.083650</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>1.144302</td>\n",
       "      <td>0.902552</td>\n",
       "      <td>0.306649</td>\n",
       "      <td>0.246326</td>\n",
       "      <td>1.214058</td>\n",
       "      <td>1.263298</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.109813</td>\n",
       "      <td>1.041096</td>\n",
       "      <td>1.083650</td>\n",
       "      <td>0.932388</td>\n",
       "      <td>1.144302</td>\n",
       "      <td>0.902552</td>\n",
       "      <td>23468.987342</td>\n",
       "      <td>51169.247670</td>\n",
       "      <td>3.025316</td>\n",
       "      <td>3.721519</td>\n",
       "      <td>123.415190</td>\n",
       "      <td>3.476793</td>\n",
       "      <td>0.839662</td>\n",
       "      <td>4.181435</td>\n",
       "      <td>3.789030</td>\n",
       "      <td>2.518987</td>\n",
       "      <td>46.561181</td>\n",
       "      <td>607.130802</td>\n",
       "      <td>37.772152</td>\n",
       "      <td>0.085219</td>\n",
       "      <td>0.156344</td>\n",
       "      <td>1.322176</td>\n",
       "      <td>1.343537</td>\n",
       "      <td>0.972328</td>\n",
       "      <td>1.150485</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>1.195762</td>\n",
       "      <td>1.055679</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>0.966470</td>\n",
       "      <td>1.233672</td>\n",
       "      <td>0.926609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>6420</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12000.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1943.0</td>\n",
       "      <td>-1960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1943.0</td>\n",
       "      <td>6420.000000</td>\n",
       "      <td>6421.0</td>\n",
       "      <td>0.534955</td>\n",
       "      <td>0.534955</td>\n",
       "      <td>0.641936</td>\n",
       "      <td>3986.710963</td>\n",
       "      <td>2132.890365</td>\n",
       "      <td>82.297141</td>\n",
       "      <td>82.297141</td>\n",
       "      <td>153.826433</td>\n",
       "      <td>25.913621</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>26771.017391</td>\n",
       "      <td>61036.929244</td>\n",
       "      <td>3.401739</td>\n",
       "      <td>3.954783</td>\n",
       "      <td>113.826087</td>\n",
       "      <td>3.827826</td>\n",
       "      <td>0.918261</td>\n",
       "      <td>4.577391</td>\n",
       "      <td>3.923478</td>\n",
       "      <td>2.772174</td>\n",
       "      <td>45.375652</td>\n",
       "      <td>655.568696</td>\n",
       "      <td>42.198261</td>\n",
       "      <td>45077.070707</td>\n",
       "      <td>102113.496521</td>\n",
       "      <td>3.252525</td>\n",
       "      <td>4.030303</td>\n",
       "      <td>98.858586</td>\n",
       "      <td>4.070707</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>4.464646</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>2.696970</td>\n",
       "      <td>42.727273</td>\n",
       "      <td>631.636364</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>45077.070707</td>\n",
       "      <td>102113.496521</td>\n",
       "      <td>3.252525</td>\n",
       "      <td>4.030303</td>\n",
       "      <td>98.858586</td>\n",
       "      <td>4.070707</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>4.464646</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>2.696970</td>\n",
       "      <td>42.727273</td>\n",
       "      <td>631.636364</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>0.239812</td>\n",
       "      <td>0.196602</td>\n",
       "      <td>1.175869</td>\n",
       "      <td>1.011434</td>\n",
       "      <td>0.685256</td>\n",
       "      <td>1.044980</td>\n",
       "      <td>2.178030</td>\n",
       "      <td>1.092325</td>\n",
       "      <td>1.019504</td>\n",
       "      <td>1.082183</td>\n",
       "      <td>0.947645</td>\n",
       "      <td>1.159299</td>\n",
       "      <td>1.018560</td>\n",
       "      <td>0.142423</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>1.229814</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.789006</td>\n",
       "      <td>0.982630</td>\n",
       "      <td>2.869565</td>\n",
       "      <td>1.119910</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.112360</td>\n",
       "      <td>1.006383</td>\n",
       "      <td>1.203224</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>0.142423</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>1.229814</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.789006</td>\n",
       "      <td>0.982630</td>\n",
       "      <td>2.869565</td>\n",
       "      <td>1.119910</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.112360</td>\n",
       "      <td>1.006383</td>\n",
       "      <td>1.203224</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>15072.375000</td>\n",
       "      <td>123040.259642</td>\n",
       "      <td>3.348214</td>\n",
       "      <td>3.598214</td>\n",
       "      <td>121.433036</td>\n",
       "      <td>3.133929</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>4.169643</td>\n",
       "      <td>3.366071</td>\n",
       "      <td>2.580357</td>\n",
       "      <td>47.812500</td>\n",
       "      <td>607.258929</td>\n",
       "      <td>36.857143</td>\n",
       "      <td>0.425945</td>\n",
       "      <td>0.097529</td>\n",
       "      <td>1.194667</td>\n",
       "      <td>1.111663</td>\n",
       "      <td>0.642329</td>\n",
       "      <td>1.276353</td>\n",
       "      <td>4.977778</td>\n",
       "      <td>1.199143</td>\n",
       "      <td>1.188329</td>\n",
       "      <td>1.162630</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>1.251525</td>\n",
       "      <td>1.193798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>66363.231106</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1970</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1972</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1994.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16549.434191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>947.910743</td>\n",
       "      <td>17.456359</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>33930.569444</td>\n",
       "      <td>73738.595633</td>\n",
       "      <td>3.309028</td>\n",
       "      <td>4.093750</td>\n",
       "      <td>57.526389</td>\n",
       "      <td>3.774306</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.729167</td>\n",
       "      <td>44.447917</td>\n",
       "      <td>626.340278</td>\n",
       "      <td>32.631944</td>\n",
       "      <td>33930.569444</td>\n",
       "      <td>73738.595633</td>\n",
       "      <td>3.309028</td>\n",
       "      <td>4.093750</td>\n",
       "      <td>57.526389</td>\n",
       "      <td>3.774306</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.729167</td>\n",
       "      <td>44.447917</td>\n",
       "      <td>626.340278</td>\n",
       "      <td>32.631944</td>\n",
       "      <td>28430.769231</td>\n",
       "      <td>85328.619373</td>\n",
       "      <td>3.589744</td>\n",
       "      <td>3.974359</td>\n",
       "      <td>57.553846</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.641026</td>\n",
       "      <td>4.179487</td>\n",
       "      <td>3.025641</td>\n",
       "      <td>44.846154</td>\n",
       "      <td>513.076923</td>\n",
       "      <td>39.256410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899980</td>\n",
       "      <td>0.604407</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>1.216833</td>\n",
       "      <td>1.324747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.212121</td>\n",
       "      <td>1.099237</td>\n",
       "      <td>0.899930</td>\n",
       "      <td>0.718459</td>\n",
       "      <td>1.308280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899980</td>\n",
       "      <td>0.604407</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>1.216833</td>\n",
       "      <td>1.324747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.212121</td>\n",
       "      <td>1.099237</td>\n",
       "      <td>0.899930</td>\n",
       "      <td>0.718459</td>\n",
       "      <td>1.348372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777737</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.754839</td>\n",
       "      <td>1.216252</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274648</td>\n",
       "      <td>1.196319</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.891938</td>\n",
       "      <td>0.877061</td>\n",
       "      <td>1.120836</td>\n",
       "      <td>14158.165138</td>\n",
       "      <td>69149.961916</td>\n",
       "      <td>3.082569</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>119.192661</td>\n",
       "      <td>4.247706</td>\n",
       "      <td>1.779817</td>\n",
       "      <td>4.220183</td>\n",
       "      <td>4.477064</td>\n",
       "      <td>2.743119</td>\n",
       "      <td>38.266055</td>\n",
       "      <td>595.243119</td>\n",
       "      <td>41.743119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959700</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.587284</td>\n",
       "      <td>1.177106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236957</td>\n",
       "      <td>1.116803</td>\n",
       "      <td>1.093645</td>\n",
       "      <td>1.045313</td>\n",
       "      <td>0.755994</td>\n",
       "      <td>1.054066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  survey_type  province  city  county  survey_time  gender  birth  \\\n",
       "0   1            1        12    32      59         2015       1   1959   \n",
       "1   2            2        18    52      85         2015       1   1992   \n",
       "2   3            2        29    83     126         2015       2   1967   \n",
       "3   4            2        10    28      51         2015       2   1943   \n",
       "4   5            1         7    18      36         2015       2   1994   \n",
       "\n",
       "   nationality  religion  religion_freq  edu edu_other  edu_status  edu_yr  \\\n",
       "0            1         1              1   11         0         4.0     0.0   \n",
       "1            1         1              1   12         0         4.0  2013.0   \n",
       "2            1         0              3    4         0         4.0     0.0   \n",
       "3            1         1              1    3         0         4.0  1959.0   \n",
       "4            1         1              1   12         0         1.0  2014.0   \n",
       "\n",
       "   income  political  join_party  floor_area  property_0  property_1  \\\n",
       "0   20000          1         0.0        45.0           0           1   \n",
       "1   20000          1         0.0       110.0           0           0   \n",
       "2    2000          1         0.0       120.0           0           1   \n",
       "3    6420          1         0.0        78.0           0           0   \n",
       "4       0          2         0.0        70.0           0           0   \n",
       "\n",
       "   property_2  property_3  property_4  property_5  property_6  property_7  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           1           0           0           0   \n",
       "2           1           0           0           0           0           0   \n",
       "3           0           1           0           0           0           0   \n",
       "4           0           0           1           0           0           0   \n",
       "\n",
       "   property_8 property_other  height_cm  weight_jin  health  health_problem  \\\n",
       "0           0              0        176         155       3               2   \n",
       "1           0              0        170         110       5               4   \n",
       "2           0              0        160         122       4               4   \n",
       "3           0              0        163         170       4               4   \n",
       "4           0              0        165         110       5               5   \n",
       "\n",
       "   depression  hukou  hukou_loc  media_1  media_2  media_3  media_4  media_5  \\\n",
       "0           5      5        2.0        4        2        5        5        4   \n",
       "1           3      1        1.0        2        2        1        3        5   \n",
       "2           5      1        1.0        2        2        2        5        1   \n",
       "3           4      1        2.0        2        1        1        5        1   \n",
       "4           3      2        3.0        1        3        4        2        5   \n",
       "\n",
       "   media_6  leisure_1  leisure_2  leisure_3  leisure_4  leisure_5  leisure_6  \\\n",
       "0        3          1          4          3        1.0        2.0        3.0   \n",
       "1        1          2          3          4        3.0        5.0        4.0   \n",
       "2        3          1          4          4        3.0        5.0        4.0   \n",
       "3        1          1          5          2        4.0        5.0        4.0   \n",
       "4        5          3          3          3        2.0        4.0        4.0   \n",
       "\n",
       "   leisure_7  leisure_8  leisure_9  leisure_10  leisure_11  leisure_12  \\\n",
       "0        4.0        1.0        4.0         5.0         4.0         1.0   \n",
       "1        3.0        2.0        3.0         4.0         5.0         1.0   \n",
       "2        4.0        2.0        3.0         5.0         5.0         5.0   \n",
       "3        5.0        1.0        1.0         5.0         5.0         5.0   \n",
       "4        3.0        5.0        2.0         5.0         5.0         1.0   \n",
       "\n",
       "   socialize  relax  learn  social_neighbor  social_friend  socia_outing  \\\n",
       "0          2      4      3              3.0            3.0             2   \n",
       "1          2      4      3              6.0            2.0             1   \n",
       "2          3      4      2              2.0            5.0             2   \n",
       "3          2      4      4              1.0            6.0             1   \n",
       "4          4      3      4              7.0            5.0             3   \n",
       "\n",
       "   equity  class  class_10_before  class_10_after  class_14  work_exper  \\\n",
       "0       3      3                3               3         1           1   \n",
       "1       3      6                4               8         5           1   \n",
       "2       4      5                4               6         3           2   \n",
       "3       4      5                5               7         2           4   \n",
       "4       2      1                1               1         4           6   \n",
       "\n",
       "   work_status  work_yr  work_type  work_manage  insur_1  insur_2  insur_3  \\\n",
       "0          3.0     30.0        1.0          2.0        1        1        1   \n",
       "1          3.0      2.0        1.0          3.0        1        1        1   \n",
       "2          0.0      0.0        0.0          0.0        1        1        2   \n",
       "3          0.0      0.0        0.0          0.0        2        2        2   \n",
       "4          0.0      0.0        0.0          0.0        1        2        2   \n",
       "\n",
       "   insur_4  family_income  family_m  family_status  house  car  invest_0  \\\n",
       "0        2   60000.000000         2              2      1    0         0   \n",
       "1        1   40000.000000         3              4      1    0         0   \n",
       "2        2    8000.000000         3              3      1    0         0   \n",
       "3        2   12000.000000         3              3      1    1         0   \n",
       "4        2   66363.231106         4              3      1    1         0   \n",
       "\n",
       "   invest_1  invest_2  invest_3  invest_4  invest_5  invest_6  invest_7  \\\n",
       "0         1         0         0         0         0         0         0   \n",
       "1         1         0         0         0         0         0         0   \n",
       "2         1         0         0         0         0         0         0   \n",
       "3         1         0         0         0         0         0         0   \n",
       "4         1         0         0         0         0         0         0   \n",
       "\n",
       "   invest_8 invest_other  son  daughter  minor_child  marital  marital_1st  \\\n",
       "0         0            0    1         0          0.0        3       1984.0   \n",
       "1         0            0    0         0          0.0        1          0.0   \n",
       "2         0            0    0         2          1.0        3       1990.0   \n",
       "3         0            0    1         4          0.0        7       1960.0   \n",
       "4         0            0    0         0          0.0        1          0.0   \n",
       "\n",
       "   s_birth  marital_now  s_edu  s_political  s_hukou  s_income  s_work_exper  \\\n",
       "0   1958.0       1984.0    6.0          1.0      5.0   40000.0           5.0   \n",
       "1      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "2   1968.0       1990.0    3.0          1.0      1.0    6000.0           3.0   \n",
       "3      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "4      0.0          0.0    0.0          0.0      0.0       0.0           0.0   \n",
       "\n",
       "   s_work_status  s_work_type  f_birth  f_edu  f_political  f_work_14  \\\n",
       "0            0.0          0.0     1945      4            4          1   \n",
       "1            0.0          0.0     1972      3            1          2   \n",
       "2            0.0          0.0     1945      1            1          2   \n",
       "3            0.0          0.0     1945     14            1          2   \n",
       "4            0.0          0.0     1970      6            1         10   \n",
       "\n",
       "   m_birth  m_edu  m_political  m_work_14  status_peer  status_3_before  view  \\\n",
       "0     1940      4            1          1            3                2     4   \n",
       "1     1973      3            1          2            1                1     4   \n",
       "2     1940      1            1          2            2                1     4   \n",
       "3     1940      1            1          2            2                1     3   \n",
       "4     1972      4            1         15            3                2     3   \n",
       "\n",
       "   inc_ability   inc_exp  trust_1  trust_2  trust_3  trust_4  trust_5  \\\n",
       "0            3   50000.0      4.0      2.0      4.0      4.0      5.0   \n",
       "1            2   50000.0      5.0      4.0      4.0      3.0      5.0   \n",
       "2            2   80000.0      3.0      3.0      3.0      3.0      4.0   \n",
       "3            2   10000.0      3.0      3.0      4.0      3.0      5.0   \n",
       "4            2  200000.0      4.0      3.0      3.0      3.0      5.0   \n",
       "\n",
       "   trust_6  trust_7  trust_8  trust_9  trust_10  trust_11  trust_12  trust_13  \\\n",
       "0      3.0      2.0      3.0      4.0       3.0       0.0       4.0       1.0   \n",
       "1      3.0      3.0      3.0      2.0       3.0       3.0       3.0       2.0   \n",
       "2      3.0      3.0      3.0      3.0       3.0       0.0       3.0       1.0   \n",
       "3      3.0      3.0      5.0      4.0       3.0       3.0       3.0       2.0   \n",
       "4      5.0      3.0      4.0      3.0       3.0       3.0       3.0       2.0   \n",
       "\n",
       "   neighbor_familiarity  public_service_1  public_service_2  public_service_3  \\\n",
       "0                     4              50.0              60.0              50.0   \n",
       "1                     3              90.0              70.0              70.0   \n",
       "2                     4              90.0              80.0              75.0   \n",
       "3                     3             100.0              90.0              70.0   \n",
       "4                     2              50.0              50.0              50.0   \n",
       "\n",
       "   public_service_4  public_service_5  public_service_6  public_service_7  \\\n",
       "0              50.0              30.0              30.0              50.0   \n",
       "1              80.0              85.0              70.0              90.0   \n",
       "2              79.0              80.0              90.0              90.0   \n",
       "3              80.0              80.0              90.0              90.0   \n",
       "4              50.0              50.0              50.0              50.0   \n",
       "\n",
       "   public_service_8  public_service_9  neg1  neg2  neg3  neg4  neg5  age  \\\n",
       "0              50.0              50.0     5     3     0     2     0   56   \n",
       "1              60.0              60.0     0     0     0     0     0   23   \n",
       "2              90.0              75.0     3     1     0     2     0   48   \n",
       "3              80.0              80.0     2     0     0     2     0   72   \n",
       "4              50.0              50.0     2     1     1     0     0   21   \n",
       "\n",
       "  age_bin  marital_1stbir  marital_nowtbir     mar  marital_sbir    age_  \\\n",
       "0       4            25.0             25.0     0.0          26.0    -1.0   \n",
       "1       1         -1992.0          -1992.0     0.0           0.0 -1992.0   \n",
       "2       3            23.0             23.0     0.0          22.0     1.0   \n",
       "3       5            17.0          -1943.0 -1960.0           0.0 -1943.0   \n",
       "4       1         -1994.0          -1994.0     0.0           0.0 -1994.0   \n",
       "\n",
       "   income/s_income  income+s_income  income/family_income  \\\n",
       "0         0.499988          60001.0              0.333328   \n",
       "1     20000.000000          20001.0              0.499988   \n",
       "2         0.333278           8001.0              0.249969   \n",
       "3      6420.000000           6421.0              0.534955   \n",
       "4         0.000000              1.0              0.000000   \n",
       "\n",
       "   all_income/family_income  income/inc_exp  family_income/m     income/m  \\\n",
       "0                  0.999983        0.399992     29850.746269  9950.248756   \n",
       "1                  0.499988        0.399992     13289.036545  6644.518272   \n",
       "2                  0.999875        0.025000      2657.807309   664.451827   \n",
       "3                  0.534955        0.641936      3986.710963  2132.890365   \n",
       "4                  0.000000        0.000000     16549.434191     0.000000   \n",
       "\n",
       "   income/floor_area  all_income/floor_area  family_income/floor_area  \\\n",
       "0         444.345701            1333.037103               1333.037103   \n",
       "1         181.801654             181.801654                363.603309   \n",
       "2          16.665278              66.661112                 66.661112   \n",
       "3          82.297141              82.297141                153.826433   \n",
       "4           0.000000               0.000000                947.910743   \n",
       "\n",
       "   floor_area/m  class_10_diff  class_diff  class_14_diff  leisure_sum  \\\n",
       "0     22.388060              0           0              2         33.0   \n",
       "1     36.544850              2           2              1         39.0   \n",
       "2     39.867110              1           1              2         45.0   \n",
       "3     25.913621              2           0              3         43.0   \n",
       "4     17.456359              0           0             -3         40.0   \n",
       "\n",
       "   public_service_sum  trust_sum  province_income_mean  \\\n",
       "0               420.0       39.0          61859.505703   \n",
       "1               675.0       43.0          29806.230241   \n",
       "2               749.0       35.0          16246.747967   \n",
       "3               760.0       44.0          26771.017391   \n",
       "4               450.0       44.0          33930.569444   \n",
       "\n",
       "   province_family_income_mean  province_equity_mean  \\\n",
       "0                131638.458098              2.773764   \n",
       "1                 37758.898778              3.249141   \n",
       "2                 95335.856164              3.298103   \n",
       "3                 61036.929244              3.401739   \n",
       "4                 73738.595633              3.309028   \n",
       "\n",
       "   province_depression_mean  province_floor_area_mean  province_health_mean  \\\n",
       "0                  3.954373                 88.692205              3.701521   \n",
       "1                  3.991409                148.463918              3.955326   \n",
       "2                  3.647696                107.902439              3.582656   \n",
       "3                  3.954783                113.826087              3.827826   \n",
       "4                  4.093750                 57.526389              3.774306   \n",
       "\n",
       "   province_class_10_diff_mean  province_class_mean  \\\n",
       "0                     1.127376             4.572243   \n",
       "1                     0.969072             4.414089   \n",
       "2                     1.029810             4.243902   \n",
       "3                     0.918261             4.577391   \n",
       "4                     0.527778             4.444444   \n",
       "\n",
       "   province_health_problem_mean  province_family_status_mean  \\\n",
       "0                      4.051331                     2.716730   \n",
       "1                      3.960481                     2.737113   \n",
       "2                      3.596206                     2.607046   \n",
       "3                      3.923478                     2.772174   \n",
       "4                      4.125000                     2.729167   \n",
       "\n",
       "   province_leisure_sum_mean  province_public_service_sum_mean  \\\n",
       "0                  41.701521                        506.456274   \n",
       "1                  47.194158                        625.233677   \n",
       "2                  46.653117                        623.013550   \n",
       "3                  45.375652                        655.568696   \n",
       "4                  44.447917                        626.340278   \n",
       "\n",
       "   province_trust_sum_mean  city_income_mean  city_family_income_mean  \\\n",
       "0                35.526616      43096.656716             93764.077651   \n",
       "1                41.434708      64016.125654             34201.145410   \n",
       "2                36.200542       6522.105263             32477.297170   \n",
       "3                42.198261      45077.070707            102113.496521   \n",
       "4                32.631944      33930.569444             73738.595633   \n",
       "\n",
       "   city_equity_mean  city_depression_mean  city_floor_area_mean  \\\n",
       "0          2.800000              4.011940             82.744776   \n",
       "1          3.371728              3.989529            157.528796   \n",
       "2          3.294737              3.957895            120.684211   \n",
       "3          3.252525              4.030303             98.858586   \n",
       "4          3.309028              4.093750             57.526389   \n",
       "\n",
       "   city_health_mean  city_class_10_diff_mean  city_class_mean  \\\n",
       "0          3.650746                 1.164179         4.465672   \n",
       "1          3.968586                 0.942408         4.335079   \n",
       "2          3.684211                 0.789474         4.505263   \n",
       "3          4.070707                 0.696970         4.464646   \n",
       "4          3.774306                 0.527778         4.444444   \n",
       "\n",
       "   city_health_problem_mean  city_family_status_mean  city_leisure_sum_mean  \\\n",
       "0                  4.113433                 2.665672              40.059701   \n",
       "1                  3.963351                 2.811518              48.303665   \n",
       "2                  3.842105                 2.768421              48.263158   \n",
       "3                  4.090909                 2.696970              42.727273   \n",
       "4                  4.125000                 2.729167              44.447917   \n",
       "\n",
       "   city_public_service_sum_mean  city_trust_sum_mean  county_income_mean  \\\n",
       "0                    542.671642            34.979104        28979.591837   \n",
       "1                    634.460733            41.732984        11628.659794   \n",
       "2                    654.547368            38.778947         6522.105263   \n",
       "3                    631.636364            38.333333        45077.070707   \n",
       "4                    626.340278            32.631944        28430.769231   \n",
       "\n",
       "   county_family_income_mean  county_equity_mean  county_depression_mean  \\\n",
       "0               60630.401904            2.571429                4.081633   \n",
       "1               34758.996059            3.350515                3.958763   \n",
       "2               32477.297170            3.294737                3.957895   \n",
       "3              102113.496521            3.252525                4.030303   \n",
       "4               85328.619373            3.589744                3.974359   \n",
       "\n",
       "   county_floor_area_mean  county_health_mean  county_class_10_diff_mean  \\\n",
       "0               38.530612            3.489796                   1.183673   \n",
       "1              154.608247            3.927835                   0.917526   \n",
       "2              120.684211            3.684211                   0.789474   \n",
       "3               98.858586            4.070707                   0.696970   \n",
       "4               57.553846            4.000000                   0.666667   \n",
       "\n",
       "   county_class_mean  county_health_problem_mean  county_family_status_mean  \\\n",
       "0           4.938776                    3.877551                   2.489796   \n",
       "1           4.123711                    3.948454                   2.752577   \n",
       "2           4.505263                    3.842105                   2.768421   \n",
       "3           4.464646                    4.090909                   2.696970   \n",
       "4           3.641026                    4.179487                   3.025641   \n",
       "\n",
       "   county_leisure_sum_mean  county_public_service_sum_mean  \\\n",
       "0                36.693878                      559.163265   \n",
       "1                48.628866                      629.123711   \n",
       "2                48.263158                      654.547368   \n",
       "3                42.727273                      631.636364   \n",
       "4                44.846154                      513.076923   \n",
       "\n",
       "   county_trust_sum_mean  income/province  family_income/province  \\\n",
       "0              31.510204         0.323313                0.455794   \n",
       "1              42.731959         0.671001                1.059353   \n",
       "2              38.778947         0.123102                0.083914   \n",
       "3              38.333333         0.239812                0.196602   \n",
       "4              39.256410         0.000000                0.899980   \n",
       "\n",
       "   equity/province  depression/province  floor_area/province  health/province  \\\n",
       "0         1.081563             1.264423             0.507373         0.810478   \n",
       "1         0.923321             0.751614             0.740921         1.264118   \n",
       "2         1.212818             1.370728             1.112116         1.116490   \n",
       "3         1.175869             1.011434             0.685256         1.044980   \n",
       "4         0.604407             0.732824             1.216833         1.324747   \n",
       "\n",
       "   class_10_diff/province  class/province  health_problem/province  \\\n",
       "0                0.000000        0.656133                 0.493665   \n",
       "1                2.063830        1.359284                 1.009978   \n",
       "2                0.971053        1.178161                 1.112283   \n",
       "3                2.178030        1.092325                 1.019504   \n",
       "4                0.000000        0.225000                 1.212121   \n",
       "\n",
       "   family_status/province  leisure_sum/province  public_service_sum/province  \\\n",
       "0                0.736179              0.791338                     0.829292   \n",
       "1                1.461394              0.826373                     1.079596   \n",
       "2                1.150728              0.964566                     1.202221   \n",
       "3                1.082183              0.947645                     1.159299   \n",
       "4                1.099237              0.899930                     0.718459   \n",
       "\n",
       "   trust_sum/province  income/city  family_income/city  equity/city  \\\n",
       "0            1.067715     0.464073            0.639904     1.071429   \n",
       "1            1.013321     0.312421            1.169551     0.889752   \n",
       "2            0.940847     0.306649            0.246326     1.214058   \n",
       "3            1.018560     0.142423            0.117516     1.229814   \n",
       "4            1.308280     0.000000            0.899980     0.604407   \n",
       "\n",
       "   depression/city  floor_area/city  health/city  class_10_diff/city  \\\n",
       "0         1.246280         0.543841     0.821750            0.000000   \n",
       "1         0.751969         0.698285     1.259894            2.122222   \n",
       "2         1.263298         0.994331     1.085714            1.266667   \n",
       "3         0.992481         0.789006     0.982630            2.869565   \n",
       "4         0.732824         1.216833     1.324747            0.000000   \n",
       "\n",
       "   class/city  health_problem/city  family_status/city  leisure_sum/city  \\\n",
       "0    0.671791             0.486212            0.750280          0.823770   \n",
       "1    1.384058             1.009247            1.422719          0.807392   \n",
       "2    1.109813             1.041096            1.083650          0.932388   \n",
       "3    1.119910             0.977778            1.112360          1.006383   \n",
       "4    0.225000             1.212121            1.099237          0.899930   \n",
       "\n",
       "   public_service_sum/city  trust_sum/city  income/county  \\\n",
       "0                 0.773949        1.114951       0.690141   \n",
       "1                 1.063896        1.030360       1.719889   \n",
       "2                 1.144302        0.902552       0.306649   \n",
       "3                 1.203224        1.147826       0.142423   \n",
       "4                 0.718459        1.348372       0.000000   \n",
       "\n",
       "   family_income/county  equity/county  depression/county  floor_area/county  \\\n",
       "0              0.989603       1.166667           1.225000           1.167903   \n",
       "1              1.150781       0.895385           0.757812           0.711476   \n",
       "2              0.246326       1.214058           1.263298           0.994331   \n",
       "3              0.117516       1.229814           0.992481           0.789006   \n",
       "4              0.777737       0.557143           0.754839           1.216252   \n",
       "\n",
       "   health/county  class_10_diff/county  class/county  health_problem/county  \\\n",
       "0       0.859649              0.000000      0.607438               0.515789   \n",
       "1       1.272966              2.179775      1.455000               1.013055   \n",
       "2       1.085714              1.266667      1.109813               1.041096   \n",
       "3       0.982630              2.869565      1.119910               0.977778   \n",
       "4       1.250000              0.000000      0.274648               1.196319   \n",
       "\n",
       "   family_status/county  leisure_sum/county  public_service_sum/county  \\\n",
       "0              0.803279            0.899333                   0.751122   \n",
       "1              1.453184            0.801993                   1.072921   \n",
       "2              1.083650            0.932388                   1.144302   \n",
       "3              1.112360            1.006383                   1.203224   \n",
       "4              0.991525            0.891938                   0.877061   \n",
       "\n",
       "   trust_sum/county  age_income_mean  age_family_income_mean  age_equity_mean  \\\n",
       "0          1.237694     24371.808219            68176.681796         3.061644   \n",
       "1          1.006273     20771.900826            71277.916142         3.157025   \n",
       "2          0.902552     23468.987342            51169.247670         3.025316   \n",
       "3          1.147826     15072.375000           123040.259642         3.348214   \n",
       "4          1.120836     14158.165138            69149.961916         3.082569   \n",
       "\n",
       "   age_depression_mean  age_floor_area_mean  age_health_mean  \\\n",
       "0             3.890411           109.662329         3.534247   \n",
       "1             4.090909           115.446281         4.239669   \n",
       "2             3.721519           123.415190         3.476793   \n",
       "3             3.598214           121.433036         3.133929   \n",
       "4             4.000000           119.192661         4.247706   \n",
       "\n",
       "   age_class_10_diff_mean  age_class_mean  age_health_problem_mean  \\\n",
       "0                0.479452        4.390411                 3.835616   \n",
       "1                1.975207        4.462810                 4.487603   \n",
       "2                0.839662        4.181435                 3.789030   \n",
       "3                0.401786        4.169643                 3.366071   \n",
       "4                1.779817        4.220183                 4.477064   \n",
       "\n",
       "   age_family_status_mean  age_leisure_sum_mean  age_public_service_sum_mean  \\\n",
       "0                2.726027             45.541096                   608.657534   \n",
       "1                2.942149             38.545455                   579.082645   \n",
       "2                2.518987             46.561181                   607.130802   \n",
       "3                2.580357             47.812500                   607.258929   \n",
       "4                2.743119             38.266055                   595.243119   \n",
       "\n",
       "   age_trust_sum_mean  income/age  family_income/age  equity/age  \\\n",
       "0           36.109589    0.820620           0.880066    0.979866   \n",
       "1           39.107438    0.962839           0.561184    0.950262   \n",
       "2           37.772152    0.085219           0.156344    1.322176   \n",
       "3           36.857143    0.425945           0.097529    1.194667   \n",
       "4           41.743119    0.000000           0.959700    0.648810   \n",
       "\n",
       "   depression/age  floor_area/age  health/age  class_10_diff/age  class/age  \\\n",
       "0        1.285211        0.410351    0.848837           0.000000   0.683307   \n",
       "1        0.733333        0.952824    1.179337           1.012552   1.344444   \n",
       "2        1.343537        0.972328    1.150485           1.190955   1.195762   \n",
       "3        1.111663        0.642329    1.276353           4.977778   1.199143   \n",
       "4        0.750000        0.587284    1.177106           0.000000   0.236957   \n",
       "\n",
       "   health_problem/age  family_status/age  leisure_sum/age  \\\n",
       "0            0.521429           0.733668         0.724620   \n",
       "1            0.891344           1.359551         1.011792   \n",
       "2            1.055679           1.190955         0.966470   \n",
       "3            1.188329           1.162630         0.899346   \n",
       "4            1.116803           1.093645         1.045313   \n",
       "\n",
       "   public_service_sum/age  trust_sum/age  \n",
       "0                0.690043       1.080046  \n",
       "1                1.165637       1.099535  \n",
       "2                1.233672       0.926609  \n",
       "3                1.251525       1.193798  \n",
       "4                0.755994       1.054066  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('当前数据的shape:',data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还应该删去有效样本数很少的特征，例如负值太多的特征或者是缺失值太多的特征，这里我一共删除了包括“目前的最高教育程度”在内的9类特征，得到了最终的263维的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 263)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#272-9=263\n",
    "#删除数值特别少的和之前用过的特征\n",
    "del_list=['id','survey_time','edu_other','invest_other','property_other','join_party','province','city','county']\n",
    "use_feature = [clo for clo in data.columns if clo not in del_list]\n",
    "data.fillna(0,inplace=True) #还是补0\n",
    "train_shape = train.shape[0] #训练集的样本数\n",
    "features = data[use_feature].columns #删除后所有的特征\n",
    "X_train_263 = data[:train_shape][use_feature].values\n",
    "y_train = target\n",
    "X_test_263 = data[train_shape:][use_feature].values\n",
    "X_train_263.shape #最终一种263个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里选择了最重要的49个特征，作为除了以上263维特征外的另外一组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 49)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_fea_49 = ['equity','depression','health','class','family_status','health_problem','class_10_after',\n",
    "           'equity/province','equity/city','equity/county',\n",
    "           'depression/province','depression/city','depression/county',\n",
    "           'health/province','health/city','health/county',\n",
    "           'class/province','class/city','class/county',\n",
    "           'family_status/province','family_status/city','family_status/county',\n",
    "           'family_income/province','family_income/city','family_income/county',\n",
    "           'floor_area/province','floor_area/city','floor_area/county',\n",
    "           'leisure_sum/province','leisure_sum/city','leisure_sum/county',\n",
    "           'public_service_sum/province','public_service_sum/city','public_service_sum/county',\n",
    "           'trust_sum/province','trust_sum/city','trust_sum/county',\n",
    "           'income/m','public_service_sum','class_diff','status_3_before','age_income_mean','age_floor_area_mean',\n",
    "           'weight_jin','height_cm',\n",
    "           'health/age','depression/age','equity/age','leisure_sum/age'\n",
    "          ]\n",
    "train_shape = train.shape[0]\n",
    "X_train_49 = data[:train_shape][imp_fea_49].values\n",
    "X_test_49 = data[train_shape:][imp_fea_49].values\n",
    "X_train_49.shape #最重要的49个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择需要进行onehot编码的离散变量进行one-hot编码，再合成为第三类特征，共383维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10956, 21)\n",
      "onehot之前： (10956, 21)\n",
      "[[ 1.  1.  1.  4.  1.  5.  2.  1.  3.  1.  2.  3.  1.  5.  5.  0.  0.  4.\n",
      "   1.  1.  1.]\n",
      " [ 2.  1.  1.  4.  1.  1.  1.  1.  3.  1.  3.  1.  0.  0.  0.  0.  0.  1.\n",
      "   2.  1.  2.]\n",
      " [ 2.  2.  1.  4.  1.  1.  1.  2.  0.  0.  0.  3.  1.  1.  3.  0.  0.  1.\n",
      "   2.  1.  2.]\n",
      " [ 2.  2.  1.  4.  1.  1.  2.  4.  0.  0.  0.  7.  0.  0.  0.  0.  0.  1.\n",
      "   2.  1.  2.]\n",
      " [ 1.  2.  1.  1.  2.  2.  3.  6.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "  10.  1. 15.]]\n",
      "onehot之后： (10956, 141)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7988, 383)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fea = ['survey_type','gender','nationality','edu_status','political','hukou','hukou_loc','work_exper','work_status','work_type',\n",
    "           'work_manage','marital','s_political','s_hukou','s_work_exper','s_work_status','s_work_type','f_political','f_work_14',\n",
    "           'm_political','m_work_14']\n",
    "noc_fea = [clo for clo in use_feature if clo not in cat_fea]\n",
    "\n",
    "onehot_data = data[cat_fea].values\n",
    "print(data[cat_fea].shape)\n",
    "print(\"onehot之前：\",onehot_data.shape)\n",
    "print(onehot_data[:5])\n",
    "enc = preprocessing.OneHotEncoder(categories = 'auto')\n",
    "oh_data=enc.fit_transform(onehot_data).toarray()\n",
    "print(\"onehot之后：\",oh_data.shape) #变为onehot编码格式\n",
    "\n",
    "X_train_oh = oh_data[:train_shape,:]\n",
    "X_test_oh = oh_data[train_shape:,:]\n",
    "X_train_oh.shape #其中的训练集\n",
    "\n",
    "# 特征数量：263-21+141=383\n",
    "X_train_383 = np.column_stack([data[:train_shape][noc_fea].values,X_train_oh])#先是noc，再是cat_fea\n",
    "X_test_383 = np.column_stack([data[train_shape:][noc_fea].values,X_test_oh])\n",
    "X_train_383.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于此，我们构建完成了三种特征工程（训练数据集），其一是上面提取的最重要的49中特征，其中包括健康程度、社会阶级、在同龄人中的收入情况等等特征。其二是扩充后的263维特征（这里可以认为是初始特征）。其三是使用One-hot编码后的特征，这里要使用One-hot进行编码的原因在于，有部分特征为分离值，例如性别中男女，男为1，女为2，我们想使用One-hot将其变为男为0，女为1，来增强机器学习算法的鲁棒性能；再如民族这个特征，原本是1-56这56个数值，如果直接分类会让分类器的鲁棒性变差，所以使用One-hot编码将其变为6个特征进行非零即一的处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征建模\n",
    "\n",
    "首先我们对于原始的263维的特征，使用lightGBM进行处理，这里我们使用5折交叉验证的方法：\n",
    "\n",
    "1.lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.500101\tvalid_1's l2: 0.533752\n",
      "[1000]\ttraining's l2: 0.451901\tvalid_1's l2: 0.501173\n",
      "[1500]\ttraining's l2: 0.425947\tvalid_1's l2: 0.488292\n",
      "[2000]\ttraining's l2: 0.40796\tvalid_1's l2: 0.482135\n",
      "[2500]\ttraining's l2: 0.393562\tvalid_1's l2: 0.478504\n",
      "[3000]\ttraining's l2: 0.381322\tvalid_1's l2: 0.476275\n",
      "[3500]\ttraining's l2: 0.370533\tvalid_1's l2: 0.474937\n",
      "[4000]\ttraining's l2: 0.360708\tvalid_1's l2: 0.47365\n",
      "[4500]\ttraining's l2: 0.351674\tvalid_1's l2: 0.472831\n",
      "[5000]\ttraining's l2: 0.343309\tvalid_1's l2: 0.472262\n",
      "[5500]\ttraining's l2: 0.335526\tvalid_1's l2: 0.471889\n",
      "[6000]\ttraining's l2: 0.327919\tvalid_1's l2: 0.471772\n",
      "[6500]\ttraining's l2: 0.3206\tvalid_1's l2: 0.471425\n",
      "[7000]\ttraining's l2: 0.313754\tvalid_1's l2: 0.471305\n",
      "[7500]\ttraining's l2: 0.307077\tvalid_1's l2: 0.471123\n",
      "[8000]\ttraining's l2: 0.300714\tvalid_1's l2: 0.471138\n",
      "[8500]\ttraining's l2: 0.294556\tvalid_1's l2: 0.471527\n",
      "Early stopping, best iteration is:\n",
      "[7884]\ttraining's l2: 0.302154\tvalid_1's l2: 0.471024\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.504793\tvalid_1's l2: 0.514266\n",
      "[1000]\ttraining's l2: 0.45572\tvalid_1's l2: 0.479856\n",
      "[1500]\ttraining's l2: 0.429662\tvalid_1's l2: 0.466736\n",
      "[2000]\ttraining's l2: 0.411893\tvalid_1's l2: 0.459803\n",
      "[2500]\ttraining's l2: 0.397769\tvalid_1's l2: 0.455909\n",
      "[3000]\ttraining's l2: 0.385627\tvalid_1's l2: 0.453133\n",
      "[3500]\ttraining's l2: 0.37499\tvalid_1's l2: 0.451601\n",
      "[4000]\ttraining's l2: 0.365231\tvalid_1's l2: 0.450413\n",
      "[4500]\ttraining's l2: 0.356109\tvalid_1's l2: 0.449476\n",
      "[5000]\ttraining's l2: 0.347679\tvalid_1's l2: 0.448683\n",
      "[5500]\ttraining's l2: 0.339694\tvalid_1's l2: 0.448202\n",
      "[6000]\ttraining's l2: 0.332006\tvalid_1's l2: 0.447925\n",
      "[6500]\ttraining's l2: 0.324762\tvalid_1's l2: 0.44805\n",
      "Early stopping, best iteration is:\n",
      "[6067]\ttraining's l2: 0.331017\tvalid_1's l2: 0.447785\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.503936\tvalid_1's l2: 0.518093\n",
      "[1000]\ttraining's l2: 0.455847\tvalid_1's l2: 0.481584\n",
      "[1500]\ttraining's l2: 0.430455\tvalid_1's l2: 0.465353\n",
      "[2000]\ttraining's l2: 0.412961\tvalid_1's l2: 0.456101\n",
      "[2500]\ttraining's l2: 0.39882\tvalid_1's l2: 0.450573\n",
      "[3000]\ttraining's l2: 0.386784\tvalid_1's l2: 0.447143\n",
      "[3500]\ttraining's l2: 0.37598\tvalid_1's l2: 0.445034\n",
      "[4000]\ttraining's l2: 0.366144\tvalid_1's l2: 0.443147\n",
      "[4500]\ttraining's l2: 0.357138\tvalid_1's l2: 0.442282\n",
      "[5000]\ttraining's l2: 0.348583\tvalid_1's l2: 0.441686\n",
      "[5500]\ttraining's l2: 0.340424\tvalid_1's l2: 0.441247\n",
      "[6000]\ttraining's l2: 0.332757\tvalid_1's l2: 0.440769\n",
      "[6500]\ttraining's l2: 0.325372\tvalid_1's l2: 0.440368\n",
      "[7000]\ttraining's l2: 0.318348\tvalid_1's l2: 0.440106\n",
      "[7500]\ttraining's l2: 0.311716\tvalid_1's l2: 0.440161\n",
      "Early stopping, best iteration is:\n",
      "[6966]\ttraining's l2: 0.318826\tvalid_1's l2: 0.440027\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.505022\tvalid_1's l2: 0.512594\n",
      "[1000]\ttraining's l2: 0.456324\tvalid_1's l2: 0.477768\n",
      "[1500]\ttraining's l2: 0.429818\tvalid_1's l2: 0.465901\n",
      "[2000]\ttraining's l2: 0.41171\tvalid_1's l2: 0.46005\n",
      "[2500]\ttraining's l2: 0.397491\tvalid_1's l2: 0.456899\n",
      "[3000]\ttraining's l2: 0.385421\tvalid_1's l2: 0.455068\n",
      "[3500]\ttraining's l2: 0.374702\tvalid_1's l2: 0.454019\n",
      "[4000]\ttraining's l2: 0.364885\tvalid_1's l2: 0.453208\n",
      "[4500]\ttraining's l2: 0.355825\tvalid_1's l2: 0.45231\n",
      "[5000]\ttraining's l2: 0.347265\tvalid_1's l2: 0.451684\n",
      "[5500]\ttraining's l2: 0.339268\tvalid_1's l2: 0.451392\n",
      "[6000]\ttraining's l2: 0.331709\tvalid_1's l2: 0.451388\n",
      "[6500]\ttraining's l2: 0.324377\tvalid_1's l2: 0.451193\n",
      "[7000]\ttraining's l2: 0.317514\tvalid_1's l2: 0.450968\n",
      "[7500]\ttraining's l2: 0.310845\tvalid_1's l2: 0.450856\n",
      "Early stopping, best iteration is:\n",
      "[7169]\ttraining's l2: 0.315279\tvalid_1's l2: 0.450794\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.503607\tvalid_1's l2: 0.520348\n",
      "[1000]\ttraining's l2: 0.455508\tvalid_1's l2: 0.485518\n",
      "[1500]\ttraining's l2: 0.429751\tvalid_1's l2: 0.472021\n",
      "[2000]\ttraining's l2: 0.411727\tvalid_1's l2: 0.465589\n",
      "[2500]\ttraining's l2: 0.397191\tvalid_1's l2: 0.461789\n",
      "[3000]\ttraining's l2: 0.384894\tvalid_1's l2: 0.459522\n",
      "[3500]\ttraining's l2: 0.373886\tvalid_1's l2: 0.458195\n",
      "[4000]\ttraining's l2: 0.363915\tvalid_1's l2: 0.457343\n",
      "[4500]\ttraining's l2: 0.35466\tvalid_1's l2: 0.4572\n",
      "[5000]\ttraining's l2: 0.345997\tvalid_1's l2: 0.457142\n",
      "[5500]\ttraining's l2: 0.337831\tvalid_1's l2: 0.456735\n",
      "[6000]\ttraining's l2: 0.33011\tvalid_1's l2: 0.456705\n",
      "[6500]\ttraining's l2: 0.322735\tvalid_1's l2: 0.456881\n",
      "Early stopping, best iteration is:\n",
      "[5771]\ttraining's l2: 0.333574\tvalid_1's l2: 0.456612\n",
      "CV score: 0.45324832\n"
     ]
    }
   ],
   "source": [
    "##### lgb_263 #\n",
    "#lightGBM决策树\n",
    "lgb_263_param = {\n",
    "'num_leaves': 7, \n",
    "'min_data_in_leaf': 20, #叶子可能具有的最小记录数\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.003,\n",
    "\"boosting\": \"gbdt\", #用gbdt算法\n",
    "\"feature_fraction\": 0.18, #例如 0.18时，意味着在每次迭代中随机选择18％的参数来建树\n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.55, #每次迭代时用的数据比例\n",
    "\"bagging_seed\": 14,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l1\": 0.1005,\n",
    "\"lambda_l2\": 0.1996, \n",
    "\"verbosity\": -1}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)   #交叉切分：5\n",
    "oof_lgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_lgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_263[val_idx], y_train[val_idx])#train:val=4:1\n",
    "\n",
    "    num_round = 10000\n",
    "    lgb_263 = lgb.train(lgb_263_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 800)\n",
    "    oof_lgb_263[val_idx] = lgb_263.predict(X_train_263[val_idx], num_iteration=lgb_263.best_iteration)\n",
    "    predictions_lgb_263 += lgb_263.predict(X_test_263, num_iteration=lgb_263.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_263, target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我使用已经训练完的lightGBM的模型进行特征重要性的判断以及可视化，从结果我们可以看出，排在重要性第一位的是health/age，就是同龄人中的健康程度，与我们主观的看法基本一致。<font color=red>我得到的是equity/city，且前四个都是equity相关的（equity是社会公平性，第五个才是health/age） </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature\n",
      "0  survey_type\n",
      "1       gender\n",
      "2        birth\n",
      "3  nationality\n",
      "4     religion\n",
      "       feature  importance\n",
      "0  survey_type           5\n",
      "1       gender          60\n",
      "2        birth         124\n",
      "3  nationality           7\n",
      "4     religion          81\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x2016 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------特征重要性\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "df = pd.DataFrame(data[use_feature].columns.tolist(), columns=['feature'])\n",
    "print(df.head())\n",
    "df['importance']=list(lgb_263.feature_importance())\n",
    "print(df.head())\n",
    "df = df.sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df.head(50))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面，我们使用常见的机器学习方法，对于263维特征进行建模：\n",
    "\n",
    "2.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[13:39:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:39:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40426\tvalid_data-rmse:3.38320\n",
      "[500]\ttrain-rmse:0.40602\tvalid_data-rmse:0.70888\n",
      "[976]\ttrain-rmse:0.27333\tvalid_data-rmse:0.71214\n",
      "fold n°2\n",
      "[13:40:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:40:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39812\tvalid_data-rmse:3.40793\n",
      "[500]\ttrain-rmse:0.40425\tvalid_data-rmse:0.69590\n",
      "[1000]\ttrain-rmse:0.27142\tvalid_data-rmse:0.69568\n",
      "[1202]\ttrain-rmse:0.23042\tvalid_data-rmse:0.69698\n",
      "fold n°3\n",
      "[13:40:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:40:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40183\tvalid_data-rmse:3.39295\n",
      "[500]\ttrain-rmse:0.41103\tvalid_data-rmse:0.66062\n",
      "[1000]\ttrain-rmse:0.27205\tvalid_data-rmse:0.66275\n",
      "[1053]\ttrain-rmse:0.26179\tvalid_data-rmse:0.66328\n",
      "fold n°4\n",
      "[14:03:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:03:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40240\tvalid_data-rmse:3.39014\n",
      "[500]\ttrain-rmse:0.40794\tvalid_data-rmse:0.66856\n",
      "[1000]\ttrain-rmse:0.26989\tvalid_data-rmse:0.67006\n",
      "[1035]\ttrain-rmse:0.26201\tvalid_data-rmse:0.67079\n",
      "fold n°5\n",
      "[14:03:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:03:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39342\tvalid_data-rmse:3.42633\n",
      "[500]\ttrain-rmse:0.41331\tvalid_data-rmse:0.65291\n",
      "[1000]\ttrain-rmse:0.27402\tvalid_data-rmse:0.64990\n",
      "[1500]\ttrain-rmse:0.18408\tvalid_data-rmse:0.65140\n",
      "[1621]\ttrain-rmse:0.16771\tvalid_data-rmse:0.65178\n",
      "CV score: 0.45721409\n"
     ]
    }
   ],
   "source": [
    "##### xgb_263\n",
    "#xgboost\n",
    "xgb_263_params = {'eta': 0.02,  #lr\n",
    "              'max_depth': 6,  \n",
    "              'min_child_weight':3,#最小叶子节点样本权重和\n",
    "              'gamma':0, #指定节点分裂所需的最小损失函数下降值。\n",
    "              'subsample': 0.7,  #控制对于每棵树，随机采样的比例\n",
    "              'colsample_bytree': 0.3,  #用来控制每棵随机采样的列数的占比 (每一列是一个特征)。\n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_xgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_263[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_263 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_263_params)\n",
    "    oof_xgb_263[val_idx] = xgb_263.predict(xgb.DMatrix(X_train_263[val_idx]), ntree_limit=xgb_263.best_ntree_limit)\n",
    "    predictions_xgb_263 += xgb_263.predict(xgb.DMatrix(X_test_263), ntree_limit=xgb_263.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RandomForestRegressor随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:  2.9min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   18.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   17.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.47912471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#RandomForestRegressor随机森林\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_rfr_263 = np.zeros(len(X_train_263))\n",
    "predictions_rfr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    rfr_263 = rfr(n_estimators=1600,max_depth=9, min_samples_leaf=9, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.25,verbose=1,n_jobs=-1)\n",
    "    #verbose = 0 为不在标准输出流输出日志信息\n",
    "#verbose = 1 为输出进度条记录\n",
    "#verbose = 2 为每个epoch输出一行记录\n",
    "    rfr_263.fit(tr_x,tr_y)\n",
    "    oof_rfr_263[val_idx] = rfr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_rfr_263 += rfr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_rfr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6592           0.0032           17.27s\n",
      "         2           0.6596           0.0032           16.78s\n",
      "         3           0.6360           0.0031           16.70s\n",
      "         4           0.6484           0.0031           16.65s\n",
      "         5           0.6557           0.0030           16.64s\n",
      "         6           0.6555           0.0028           16.59s\n",
      "         7           0.6427           0.0032           16.67s\n",
      "         8           0.6429           0.0027           16.73s\n",
      "         9           0.6221           0.0028           16.71s\n",
      "        10           0.6272           0.0027           16.65s\n",
      "        20           0.5827           0.0022           16.10s\n",
      "        30           0.5614           0.0019           15.62s\n",
      "        40           0.5556           0.0015           15.13s\n",
      "        50           0.5164           0.0013           14.69s\n",
      "        60           0.5086           0.0010           14.24s\n",
      "        70           0.4720           0.0009           13.80s\n",
      "        80           0.4482           0.0010           13.44s\n",
      "        90           0.4484           0.0007           13.14s\n",
      "       100           0.4295           0.0005           12.71s\n",
      "       200           0.3540           0.0001            8.40s\n",
      "       300           0.3001          -0.0000            4.19s\n",
      "       400           0.2763          -0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6531           0.0034           17.86s\n",
      "         2           0.6632           0.0035           17.71s\n",
      "         3           0.6498           0.0032           17.53s\n",
      "         4           0.6515           0.0033           17.43s\n",
      "         5           0.6462           0.0031           17.37s\n",
      "         6           0.6355           0.0030           17.39s\n",
      "         7           0.6548           0.0030           17.25s\n",
      "         8           0.6267           0.0028           17.10s\n",
      "         9           0.6405           0.0029           16.94s\n",
      "        10           0.6420           0.0027           16.83s\n",
      "        20           0.5953           0.0023           16.19s\n",
      "        30           0.5655           0.0017           15.67s\n",
      "        40           0.5294           0.0016           15.19s\n",
      "        50           0.5217           0.0014           14.75s\n",
      "        60           0.4956           0.0012           14.32s\n",
      "        70           0.4765           0.0009           13.88s\n",
      "        80           0.4589           0.0007           13.44s\n",
      "        90           0.4583           0.0006           13.00s\n",
      "       100           0.4274           0.0006           12.58s\n",
      "       200           0.3428           0.0001            8.33s\n",
      "       300           0.2999           0.0001            4.15s\n",
      "       400           0.2690           0.0000            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6802           0.0032           17.06s\n",
      "         2           0.6425           0.0033           17.03s\n",
      "         3           0.6800           0.0030           16.92s\n",
      "         4           0.6603           0.0033           16.91s\n",
      "         5           0.6615           0.0030           16.77s\n",
      "         6           0.6445           0.0031           16.68s\n",
      "         7           0.6253           0.0030           16.52s\n",
      "         8           0.6257           0.0032           16.41s\n",
      "         9           0.6410           0.0028           16.39s\n",
      "        10           0.6082           0.0030           16.32s\n",
      "        20           0.5903           0.0023           15.88s\n",
      "        30           0.5519           0.0021           15.43s\n",
      "        40           0.5385           0.0016           15.04s\n",
      "        50           0.5121           0.0012           14.66s\n",
      "        60           0.4844           0.0013           14.28s\n",
      "        70           0.4797           0.0009           13.85s\n",
      "        80           0.4663           0.0008           13.42s\n",
      "        90           0.4530           0.0008           13.01s\n",
      "       100           0.4293           0.0005           12.57s\n",
      "       200           0.3387           0.0002            8.34s\n",
      "       300           0.2966           0.0001            4.15s\n",
      "       400           0.2615          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6832           0.0033           17.12s\n",
      "         2           0.6754           0.0032           16.90s\n",
      "         3           0.6547           0.0035           16.70s\n",
      "         4           0.6401           0.0033           16.70s\n",
      "         5           0.6462           0.0030           16.64s\n",
      "         6           0.6567           0.0027           16.65s\n",
      "         7           0.6422           0.0029           16.63s\n",
      "         8           0.6329           0.0030           16.58s\n",
      "         9           0.6372           0.0026           16.53s\n",
      "        10           0.6317           0.0029           16.45s\n",
      "        20           0.5858           0.0023           15.97s\n",
      "        30           0.5782           0.0019           15.48s\n",
      "        40           0.5279           0.0017           15.09s\n",
      "        50           0.5245           0.0014           14.64s\n",
      "        60           0.4889           0.0012           14.21s\n",
      "        70           0.4715           0.0008           13.89s\n",
      "        80           0.4494           0.0007           13.46s\n",
      "        90           0.4392           0.0007           13.02s\n",
      "       100           0.4297           0.0005           12.66s\n",
      "       200           0.3391           0.0002            8.47s\n",
      "       300           0.2915           0.0000            4.21s\n",
      "       400           0.2707          -0.0000            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6497           0.0036           16.98s\n",
      "         2           0.6717           0.0035           17.12s\n",
      "         3           0.6578           0.0029           17.01s\n",
      "         4           0.6665           0.0030           16.91s\n",
      "         5           0.6470           0.0030           16.83s\n",
      "         6           0.6345           0.0033           16.70s\n",
      "         7           0.6282           0.0030           16.61s\n",
      "         8           0.6355           0.0031           16.56s\n",
      "         9           0.6407           0.0029           16.52s\n",
      "        10           0.6165           0.0029           16.47s\n",
      "        20           0.5990           0.0024           16.00s\n",
      "        30           0.5693           0.0021           15.65s\n",
      "        40           0.5352           0.0017           15.30s\n",
      "        50           0.5013           0.0013           14.84s\n",
      "        60           0.4926           0.0010           14.42s\n",
      "        70           0.4790           0.0009           13.97s\n",
      "        80           0.4505           0.0009           13.58s\n",
      "        90           0.4548           0.0006           13.19s\n",
      "       100           0.4284           0.0007           12.74s\n",
      "       200           0.3416           0.0001            8.39s\n",
      "       300           0.2860           0.0000            4.19s\n",
      "       400           0.2724          -0.0000            0.00s\n",
      "CV score: 0.45747991\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingRegressor梯度提升决策树\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_263 = np.zeros(train_shape)\n",
    "predictions_gbr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_263 = gbr(n_estimators=400, learning_rate=0.01,subsample=0.65,max_depth=7, min_samples_leaf=20,\n",
    "            max_features=0.22,verbose=1)\n",
    "    gbr_263.fit(tr_x,tr_y)\n",
    "    oof_gbr_263[val_idx] = gbr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_gbr_263 += gbr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ExtraTreesRegressor 极端随机森林回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    3.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.48600564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesRegressor 极端随机森林回归\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_etr_263 = np.zeros(train_shape)\n",
    "predictions_etr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    etr_263 = etr(n_estimators=1000,max_depth=8, min_samples_leaf=12, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.4,verbose=1,n_jobs=-1)\n",
    "    etr_263.fit(tr_x,tr_y)\n",
    "    oof_etr_263[val_idx] = etr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_etr_263 += etr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_etr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上5种模型的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44980759982458474"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack2 = np.vstack([oof_lgb_263,oof_xgb_263,oof_gbr_263,oof_rfr_263,oof_etr_263]).transpose()\n",
    "# transpose()函数的作用就是调换x,y,z的位置,也就是数组的索引值\n",
    "test_stack2 = np.vstack([predictions_lgb_263, predictions_xgb_263,predictions_gbr_263,predictions_rfr_263,predictions_etr_263]).transpose()\n",
    "\n",
    "#交叉验证:5折，重复2次\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack2 = np.zeros(train_stack2.shape[0])\n",
    "predictions_lr2 = np.zeros(test_stack2.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack2,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack2[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack2[val_idx], target.iloc[val_idx].values\n",
    "    #Kernel Ridge Regression\n",
    "    lr2 = kr()\n",
    "    lr2.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack2[val_idx] = lr2.predict(val_data)\n",
    "    predictions_lr2 += lr2.predict(test_stack2) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们对于49维的数据进行与上述263维数据相同的操作\n",
    "\n",
    "1.lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.472117\tvalid_1's l2: 0.494335\n",
      "[2000]\ttraining's l2: 0.432469\tvalid_1's l2: 0.474129\n",
      "[3000]\ttraining's l2: 0.410256\tvalid_1's l2: 0.468791\n",
      "[4000]\ttraining's l2: 0.39256\tvalid_1's l2: 0.466552\n",
      "[5000]\ttraining's l2: 0.377013\tvalid_1's l2: 0.466016\n",
      "[6000]\ttraining's l2: 0.363014\tvalid_1's l2: 0.466067\n",
      "Early stopping, best iteration is:\n",
      "[5373]\ttraining's l2: 0.371605\tvalid_1's l2: 0.465741\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.471663\tvalid_1's l2: 0.494652\n",
      "[2000]\ttraining's l2: 0.431201\tvalid_1's l2: 0.475832\n",
      "[3000]\ttraining's l2: 0.40857\tvalid_1's l2: 0.471509\n",
      "[4000]\ttraining's l2: 0.390546\tvalid_1's l2: 0.470323\n",
      "[5000]\ttraining's l2: 0.375102\tvalid_1's l2: 0.470296\n",
      "Early stopping, best iteration is:\n",
      "[4431]\ttraining's l2: 0.38363\tvalid_1's l2: 0.470181\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.474605\tvalid_1's l2: 0.488842\n",
      "[2000]\ttraining's l2: 0.434093\tvalid_1's l2: 0.465848\n",
      "[3000]\ttraining's l2: 0.411314\tvalid_1's l2: 0.461478\n",
      "[4000]\ttraining's l2: 0.393582\tvalid_1's l2: 0.460987\n",
      "Early stopping, best iteration is:\n",
      "[3568]\ttraining's l2: 0.400807\tvalid_1's l2: 0.460616\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.467855\tvalid_1's l2: 0.509235\n",
      "[2000]\ttraining's l2: 0.42834\tvalid_1's l2: 0.492237\n",
      "[3000]\ttraining's l2: 0.406451\tvalid_1's l2: 0.487713\n",
      "[4000]\ttraining's l2: 0.389168\tvalid_1's l2: 0.484667\n",
      "[5000]\ttraining's l2: 0.374352\tvalid_1's l2: 0.482771\n",
      "[6000]\ttraining's l2: 0.361117\tvalid_1's l2: 0.481064\n",
      "[7000]\ttraining's l2: 0.348909\tvalid_1's l2: 0.48018\n",
      "[8000]\ttraining's l2: 0.337402\tvalid_1's l2: 0.479332\n",
      "[9000]\ttraining's l2: 0.326674\tvalid_1's l2: 0.47855\n",
      "[10000]\ttraining's l2: 0.316538\tvalid_1's l2: 0.478033\n",
      "[11000]\ttraining's l2: 0.30707\tvalid_1's l2: 0.477687\n",
      "[12000]\ttraining's l2: 0.297954\tvalid_1's l2: 0.47717\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\ttraining's l2: 0.297954\tvalid_1's l2: 0.47717\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.469248\tvalid_1's l2: 0.504587\n",
      "[2000]\ttraining's l2: 0.429303\tvalid_1's l2: 0.487942\n",
      "[3000]\ttraining's l2: 0.40704\tvalid_1's l2: 0.484008\n",
      "[4000]\ttraining's l2: 0.389376\tvalid_1's l2: 0.482436\n",
      "[5000]\ttraining's l2: 0.374067\tvalid_1's l2: 0.481523\n",
      "[6000]\ttraining's l2: 0.360334\tvalid_1's l2: 0.48131\n",
      "Early stopping, best iteration is:\n",
      "[5907]\ttraining's l2: 0.361534\tvalid_1's l2: 0.481207\n",
      "CV score: 0.47098109\n"
     ]
    }
   ],
   "source": [
    "##### lgb_49\n",
    "lgb_49_param = {\n",
    "'num_leaves': 9,\n",
    "'min_data_in_leaf': 23,\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.002,\n",
    "\"boosting\": \"gbdt\",\n",
    "\"feature_fraction\": 0.45,\n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.65,\n",
    "\"bagging_seed\": 15,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l2\": 0.2, \n",
    "\"verbosity\": -1}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)   \n",
    "oof_lgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_lgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 12000\n",
    "    lgb_49 = lgb.train(lgb_49_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
    "    oof_lgb_49[val_idx] = lgb_49.predict(X_train_49[val_idx], num_iteration=lgb_49.best_iteration)\n",
    "    predictions_lgb_49 += lgb_49.predict(X_test_49, num_iteration=lgb_49.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[14:10:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:10:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40429\tvalid_data-rmse:3.38309\n",
      "[500]\ttrain-rmse:0.52836\tvalid_data-rmse:0.72140\n",
      "[961]\ttrain-rmse:0.44290\tvalid_data-rmse:0.72415\n",
      "fold n°2\n",
      "[14:10:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:10:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39821\tvalid_data-rmse:3.40792\n",
      "[500]\ttrain-rmse:0.52810\tvalid_data-rmse:0.70530\n",
      "[1000]\ttrain-rmse:0.43820\tvalid_data-rmse:0.70914\n",
      "[1088]\ttrain-rmse:0.42459\tvalid_data-rmse:0.70970\n",
      "fold n°3\n",
      "[14:10:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:10:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40184\tvalid_data-rmse:3.39298\n",
      "[500]\ttrain-rmse:0.53301\tvalid_data-rmse:0.66837\n",
      "[1000]\ttrain-rmse:0.44182\tvalid_data-rmse:0.67092\n",
      "[1024]\ttrain-rmse:0.43818\tvalid_data-rmse:0.67125\n",
      "fold n°4\n",
      "[14:10:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:10:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40244\tvalid_data-rmse:3.39015\n",
      "[500]\ttrain-rmse:0.53045\tvalid_data-rmse:0.67763\n",
      "[1000]\ttrain-rmse:0.43863\tvalid_data-rmse:0.68145\n",
      "[1103]\ttrain-rmse:0.42255\tvalid_data-rmse:0.68201\n",
      "fold n°5\n",
      "[14:10:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:10:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39348\tvalid_data-rmse:3.42626\n",
      "[500]\ttrain-rmse:0.53623\tvalid_data-rmse:0.66441\n",
      "[1000]\ttrain-rmse:0.44450\tvalid_data-rmse:0.66730\n",
      "[1096]\ttrain-rmse:0.42970\tvalid_data-rmse:0.66834\n",
      "CV score: 0.47234781\n"
     ]
    }
   ],
   "source": [
    "##### xgb_49\n",
    "xgb_49_params = {'eta': 0.02, \n",
    "              'max_depth': 5, \n",
    "              'min_child_weight':3,\n",
    "              'gamma':0,\n",
    "              'subsample': 0.7, \n",
    "              'colsample_bytree': 0.35, \n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_xgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_49 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_49_params)\n",
    "    oof_xgb_49[val_idx] = xgb_49.predict(xgb.DMatrix(X_train_49[val_idx]), ntree_limit=xgb_49.best_ntree_limit)\n",
    "    predictions_xgb_49 += xgb_49.predict(xgb.DMatrix(X_test_49), ntree_limit=xgb_49.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6916           0.0031           11.14s\n",
      "         2           0.6610           0.0033           11.23s\n",
      "         3           0.6647           0.0032           11.05s\n",
      "         4           0.6811           0.0029           11.00s\n",
      "         5           0.6579           0.0031           10.98s\n",
      "         6           0.6555           0.0028           10.91s\n",
      "         7           0.6383           0.0029           10.92s\n",
      "         8           0.6214           0.0034           10.88s\n",
      "         9           0.6535           0.0025           10.83s\n",
      "        10           0.6238           0.0030           10.78s\n",
      "        20           0.6207           0.0021           10.55s\n",
      "        30           0.5685           0.0019           10.30s\n",
      "        40           0.5508           0.0015           10.10s\n",
      "        50           0.5221           0.0012            9.93s\n",
      "        60           0.5155           0.0011            9.74s\n",
      "        70           0.4961           0.0010            9.55s\n",
      "        80           0.4755           0.0008            9.37s\n",
      "        90           0.4853           0.0006            9.19s\n",
      "       100           0.4648           0.0006            9.00s\n",
      "       200           0.4111           0.0000            7.20s\n",
      "       300           0.3666          -0.0000            5.40s\n",
      "       400           0.3569          -0.0000            3.60s\n",
      "       500           0.3393          -0.0001            1.80s\n",
      "       600           0.3182          -0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6629           0.0032           11.36s\n",
      "         2           0.6605           0.0034           11.08s\n",
      "         3           0.6315           0.0030           10.88s\n",
      "         4           0.6500           0.0032           10.85s\n",
      "         5           0.6385           0.0033           10.80s\n",
      "         6           0.6382           0.0031           10.80s\n",
      "         7           0.6421           0.0033           10.80s\n",
      "         8           0.6280           0.0029           10.78s\n",
      "         9           0.6386           0.0028           10.77s\n",
      "        10           0.6324           0.0028           10.71s\n",
      "        20           0.5900           0.0026           10.43s\n",
      "        30           0.5705           0.0019           10.22s\n",
      "        40           0.5564           0.0014           10.06s\n",
      "        50           0.5395           0.0013            9.88s\n",
      "        60           0.5147           0.0011            9.69s\n",
      "        70           0.5014           0.0009            9.51s\n",
      "        80           0.4987           0.0007            9.32s\n",
      "        90           0.4845           0.0006            9.15s\n",
      "       100           0.4644           0.0004            8.97s\n",
      "       200           0.4066           0.0001            7.18s\n",
      "       300           0.3700           0.0000            5.39s\n",
      "       400           0.3455           0.0000            3.59s\n",
      "       500           0.3280           0.0000            1.80s\n",
      "       600           0.3079          -0.0000            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6530           0.0033           10.77s\n",
      "         2           0.6522           0.0031           10.74s\n",
      "         3           0.6440           0.0035           10.85s\n",
      "         4           0.6438           0.0031           10.70s\n",
      "         5           0.6653           0.0029           10.70s\n",
      "         6           0.6678           0.0030           10.76s\n",
      "         7           0.6346           0.0032           10.73s\n",
      "         8           0.6451           0.0032           10.78s\n",
      "         9           0.6517           0.0026           10.72s\n",
      "        10           0.6360           0.0027           10.70s\n",
      "        20           0.5988           0.0024           10.48s\n",
      "        30           0.5494           0.0021           10.34s\n",
      "        40           0.5442           0.0015           10.17s\n",
      "        50           0.5240           0.0014            9.99s\n",
      "        60           0.5230           0.0012            9.79s\n",
      "        70           0.4943           0.0010            9.60s\n",
      "        80           0.4831           0.0008            9.41s\n",
      "        90           0.4635           0.0006            9.21s\n",
      "       100           0.4652           0.0006            9.04s\n",
      "       200           0.3975           0.0001            7.21s\n",
      "       300           0.3614           0.0000            5.40s\n",
      "       400           0.3401          -0.0000            3.60s\n",
      "       500           0.3218          -0.0001            1.81s\n",
      "       600           0.3054          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6607           0.0033           10.89s\n",
      "         2           0.6733           0.0032           10.83s\n",
      "         3           0.6574           0.0031           10.82s\n",
      "         4           0.6478           0.0031           10.85s\n",
      "         5           0.6469           0.0033           10.75s\n",
      "         6           0.6505           0.0032           10.80s\n",
      "         7           0.6270           0.0030           10.79s\n",
      "         8           0.6394           0.0031           10.75s\n",
      "         9           0.6392           0.0029           10.75s\n",
      "        10           0.6409           0.0026           10.69s\n",
      "        20           0.6122           0.0026           10.48s\n",
      "        30           0.5775           0.0022           10.48s\n",
      "        40           0.5517           0.0017           10.30s\n",
      "        50           0.5277           0.0013           10.06s\n",
      "        60           0.5150           0.0011            9.83s\n",
      "        70           0.5094           0.0009            9.64s\n",
      "        80           0.4801           0.0008            9.44s\n",
      "        90           0.4688           0.0006            9.24s\n",
      "       100           0.4547           0.0006            9.06s\n",
      "       200           0.3920           0.0000            7.21s\n",
      "       300           0.3624          -0.0000            5.41s\n",
      "       400           0.3491          -0.0000            3.61s\n",
      "       500           0.3237          -0.0001            1.80s\n",
      "       600           0.3033          -0.0000            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6662           0.0032           11.01s\n",
      "         2           0.6561           0.0034           11.08s\n",
      "         3           0.6525           0.0030           11.10s\n",
      "         4           0.6470           0.0031           11.44s\n",
      "         5           0.6655           0.0030           11.27s\n",
      "         6           0.6412           0.0030           11.22s\n",
      "         7           0.6516           0.0031           11.16s\n",
      "         8           0.6378           0.0027           11.08s\n",
      "         9           0.6406           0.0027           11.02s\n",
      "        10           0.6139           0.0028           10.96s\n",
      "        20           0.6073           0.0024           10.68s\n",
      "        30           0.5560           0.0020           10.47s\n",
      "        40           0.5429           0.0015           10.28s\n",
      "        50           0.5332           0.0015           10.07s\n",
      "        60           0.5043           0.0010            9.88s\n",
      "        70           0.4997           0.0008            9.68s\n",
      "        80           0.4718           0.0007            9.47s\n",
      "        90           0.4671           0.0006            9.28s\n",
      "       100           0.4731           0.0005            9.08s\n",
      "       200           0.3886           0.0001            7.24s\n",
      "       300           0.3667          -0.0001            5.43s\n",
      "       400           0.3429          -0.0000            3.62s\n",
      "       500           0.3130          -0.0000            1.81s\n",
      "       600           0.3110          -0.0000            0.00s\n",
      "CV score: 0.47284644\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_49 = np.zeros(train_shape)\n",
    "predictions_gbr_49 = np.zeros(len(X_test_49))\n",
    "#GradientBoostingRegressor梯度提升决策树\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_49 = gbr(n_estimators=600, learning_rate=0.01,subsample=0.65,max_depth=6, min_samples_leaf=20,\n",
    "            max_features=0.35,verbose=1)\n",
    "    gbr_49.fit(tr_x,tr_y)\n",
    "    oof_gbr_49[val_idx] = gbr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_gbr_49 += gbr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上3种模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47004512522175457"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack3 = np.vstack([oof_lgb_49,oof_xgb_49,oof_gbr_49]).transpose()\n",
    "test_stack3 = np.vstack([predictions_lgb_49, predictions_xgb_49,predictions_gbr_49]).transpose()\n",
    "#\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack3 = np.zeros(train_stack3.shape[0])\n",
    "predictions_lr3 = np.zeros(test_stack3.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack3,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack3[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack3[val_idx], target.iloc[val_idx].values\n",
    "        #Kernel Ridge Regression\n",
    "    lr3 = kr()\n",
    "    lr3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack3[val_idx] = lr3.predict(val_data)\n",
    "    predictions_lr3 += lr3.predict(test_stack3) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们对于383维的数据进行与上述263以及49维数据相同的操作\n",
    "\n",
    "1. Kernel Ridge Regression 基于核的岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.51732109\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_383 = np.zeros(train_shape)\n",
    "predictions_kr_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #Kernel Ridge Regression 岭回归\n",
    "    kr_383 = kr()\n",
    "    kr_383.fit(tr_x,tr_y)\n",
    "    oof_kr_383[val_idx] = kr_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_kr_383 += kr_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用普通岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48782897\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_383 = np.zeros(train_shape)\n",
    "predictions_ridge_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #使用岭回归\n",
    "    ridge_383 = Ridge(alpha=1200)\n",
    "    ridge_383.fit(tr_x,tr_y)\n",
    "    oof_ridge_383[val_idx] = ridge_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_ridge_383 += ridge_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53650414\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_383 = np.zeros(train_shape)\n",
    "predictions_en_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #ElasticNet 弹性网络\n",
    "    en_383 = en(alpha=1.0,l1_ratio=0.06)\n",
    "    en_383.fit(tr_x,tr_y)\n",
    "    oof_en_383[val_idx] = en_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_en_383 += en_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 使用BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48828501\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_383 = np.zeros(train_shape)\n",
    "predictions_br_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #BayesianRidge 贝叶斯回归\n",
    "    br_383 = br()\n",
    "    br_383.fit(tr_x,tr_y)\n",
    "    oof_br_383[val_idx] = br_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_br_383 += br_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上4种模型的基于383个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4890155890702697"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack1 = np.vstack([oof_br_383,oof_kr_383,oof_en_383,oof_ridge_383]).transpose()\n",
    "test_stack1 = np.vstack([predictions_br_383, predictions_kr_383,predictions_en_383,predictions_ridge_383]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack1 = np.zeros(train_stack1.shape[0])\n",
    "predictions_lr1 = np.zeros(test_stack1.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack1,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack1[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack1[val_idx], target.iloc[val_idx].values\n",
    "    # LinearRegression简单的线性回归\n",
    "    lr1 = lr()\n",
    "    lr1.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack1[val_idx] = lr1.predict(val_data)\n",
    "    predictions_lr1 += lr1.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于49维的特征是最重要的特征，所以这里考虑增加更多的模型进行49维特征的数据的构建工作。\n",
    "1. KernelRidge 核岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.50446138\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_49 = np.zeros(train_shape)\n",
    "predictions_kr_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    kr_49 = kr()\n",
    "    kr_49.fit(tr_x,tr_y)\n",
    "    oof_kr_49[val_idx] = kr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_kr_49 += kr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ridge 岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49587513\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_49 = np.zeros(train_shape)\n",
    "predictions_ridge_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    ridge_49 = Ridge(alpha=6)\n",
    "    ridge_49.fit(tr_x,tr_y)\n",
    "    oof_ridge_49[val_idx] = ridge_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_ridge_49 += ridge_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49684901\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_49 = np.zeros(train_shape)\n",
    "predictions_br_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    br_49 = br()\n",
    "    br_49.fit(tr_x,tr_y)\n",
    "    oof_br_49[val_idx] = br_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_br_49 += br_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53979354\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_49 = np.zeros(train_shape)\n",
    "predictions_en_49 = np.zeros(len(X_test_49))\n",
    "#\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    en_49 = en(alpha=1.0,l1_ratio=0.05)\n",
    "    en_49.fit(tr_x,tr_y)\n",
    "    oof_en_49[val_idx] = en_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_en_49 += en_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了以上4种新模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49622456741436527"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack4 = np.vstack([oof_br_49,oof_kr_49,oof_en_49,oof_ridge_49]).transpose()\n",
    "test_stack4 = np.vstack([predictions_br_49, predictions_kr_49,predictions_en_49,predictions_ridge_49]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack4 = np.zeros(train_stack4.shape[0])\n",
    "predictions_lr4 = np.zeros(test_stack4.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack4,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack4[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack4[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr4 = lr()\n",
    "    lr4.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack4[val_idx] = lr4.predict(val_data)\n",
    "    predictions_lr4 += lr4.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合\n",
    "\n",
    "这里对于上述四种集成学习的模型的预测结果进行加权的求和，得到最终的结果，当然这种方式是很不准确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4548466089286124"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#和下面作对比\n",
    "mean_squared_error(target.values, 0.7*(0.6*oof_stack2 + 0.4*oof_stack3)+0.3*(0.55*oof_stack1+0.45*oof_stack4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更好的方式是将以上的4中集成学习模型再次进行集成学习的训练，这里直接使用LinearRegression简单线性回归的进行集成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4499468854413023"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack5 = np.vstack([oof_stack1,oof_stack2,oof_stack3,oof_stack4]).transpose()\n",
    "test_stack5 = np.vstack([predictions_lr1, predictions_lr2,predictions_lr3,predictions_lr4]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack5 = np.zeros(train_stack5.shape[0])\n",
    "predictions_lr5= np.zeros(test_stack5.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack5,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack5[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack5[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr5 = lr()\n",
    "    lr5.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack5[val_idx] = lr5.predict(val_data)\n",
    "    predictions_lr5 += lr5.predict(test_stack5) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果保存\n",
    "\n",
    "进行index的读取工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.879846\n",
       "std         0.461572\n",
       "min         1.619144\n",
       "25%         3.668566\n",
       "50%         3.953703\n",
       "75%         4.188419\n",
       "max         5.026208\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example = pd.read_csv('submit_example.csv',sep=',',encoding='latin-1')\n",
    "\n",
    "submit_example['happiness'] = predictions_lr5\n",
    "\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行结果保存，这里我们预测出的值是1-5的连续值，但是我们的ground truth是整数值，所以为了进一步优化我们的结果，我们对于结果进行了整数解的近似，并保存到了csv文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.879827\n",
       "std         0.461525\n",
       "min         1.619144\n",
       "25%         3.668566\n",
       "50%         3.953703\n",
       "75%         4.188419\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example.loc[submit_example['happiness']>4.96,'happiness']= 5\n",
    "submit_example.loc[submit_example['happiness']<=1.04,'happiness']= 1\n",
    "submit_example.loc[(submit_example['happiness']>1.96)&(submit_example['happiness']<2.04),'happiness']= 2\n",
    "\n",
    "submit_example.to_csv(\"submision.csv\",index=False)\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家可以对于model的参数进行更进一步的调整，例如使用网格搜索的方法。这留给大家做进一步的思考喽～"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
