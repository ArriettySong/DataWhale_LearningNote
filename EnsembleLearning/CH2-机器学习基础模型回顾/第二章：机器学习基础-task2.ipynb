{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 使用sklearn构建完整的机器学习项目流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，一个完整的机器学习项目分为以下步骤：\n",
    "   - 明确项目任务：回归/分类\n",
    "   - 收集数据集并选择合适的特征。\n",
    "   - 选择度量模型性能的指标。\n",
    "   - 选择具体的模型并进行训练以优化模型。\n",
    "   - 评估模型的性能并调参。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 使用sklearn构建完整的回归项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 收集数据集并选择合适的特征：        \n",
    "在数据集上我们使用我们比较熟悉的Boston房价数据集，原因是：        \n",
    "   - 第一个，我们通过这些简单的数据集快速让我们上手sklearn，以及掌握sklearn的相关操作。\n",
    "   - 第二个，我们用简单的数据集能更加清晰地介绍机器学习的相关模型，避免在处理数据上花费较大的精力。\n",
    "\n",
    "如果您对具体的项目感兴趣，我们会在第六章给出三个大型的案例让大家体验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  Price  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()     # 返回一个类似于字典的类\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "boston_data = pd.DataFrame(X,columns=features)\n",
    "boston_data[\"Price\"] = y\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个特征的相关解释：\n",
    "   - CRIM：各城镇的人均犯罪率\n",
    "   - ZN：规划地段超过25,000平方英尺的住宅用地比例\n",
    "   - INDUS：城镇非零售商业用地比例\n",
    "   - CHAS：是否在查尔斯河边(=1是)\n",
    "   - NOX：一氧化氮浓度(/千万分之一)\n",
    "   - RM：每个住宅的平均房间数\n",
    "   - AGE：1940年以前建造的自住房屋的比例\n",
    "   - DIS：到波士顿五个就业中心的加权距离\n",
    "   - RAD：放射状公路的可达性指数\n",
    "   - TAX：全部价值的房产税率(每1万美元)\n",
    "   - PTRATIO：按城镇分配的学生与教师比例\n",
    "   - B：1000(Bk - 0.63)^2其中Bk是每个城镇的黑人比例\n",
    "   - LSTAT：较低地位人口\n",
    "   - Price：房价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 选择度量模型性能的指标：                      \n",
    "   - MSE均方误差：$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2.$\n",
    "   - MAE平均绝对误差:$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|$\n",
    "   - $R^2$决定系数：$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "   - 解释方差得分:$explained\\_{}variance(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}$\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "![jupyter](./1.3.png)              \n",
    "在这个案例中，我们使用MSE均方误差为模型的性能度量指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 选择具体的模型并进行训练\n",
    "   - **线性回归模型**         \n",
    "   回归这个概念是19世纪80年代由英国统计学家郎西斯.高尔顿在研究父子身高关系提出来的，他发现：在同一族群中，子代的平均身高介于父代的身高以及族群的平均身高之间。具体而言，高个子父亲的儿子的身高有低于其父亲身高的趋势，而矮个子父亲的儿子身高则有高于父亲的身高的趋势。也就是说，子代的身高有向族群平均身高\"平均\"的趋势，这就是统计学上\"回归\"的最初含义。回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（特征）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。通常使用曲线/线来拟合数据点，目标是使曲线到数据点的距离差异最小。而线性回归就是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数w ：                                                \n",
    "   假设：数据集$D = \\{(x_1,y_1),...,(x_N,y_N) \\}$，$x_i \\in R^p,y_i \\in R,i = 1,2,...,N$，$X = (x_1,x_2,...,x_N)^T,Y=(y_1,y_2,...,y_N)^T$                        \n",
    "   假设X和Y之间存在线性关系，模型的具体形式为$\\hat{y}=f(w) =w^Tx$           \n",
    "   ![jupyter](./1.4.png)      \n",
    "   (a) 最小二乘估计：                 \n",
    "   我们需要衡量真实值$y_i$与线性回归模型的预测值$w^Tx_i$之间的差距，在这里我们和使用二范数的平方和L(w)来描述这种差距，即：                      \n",
    "   $$\n",
    "   L(w) = \\sum\\limits_{i=1}^{N}||w^Tx_i-y_i||_2^2=\\sum\\limits_{i=1}^{N}(w^Tx_i-y_i)^2 = (w^TX^T-Y^T)(w^TX^T-Y^T)^T = w^TX^TXw - 2w^TX^TY+YY^T\\\\\n",
    "   因此，我们需要找到使得L(w)最小时对应的参数w，即：\\\\\n",
    "   \\hat{w} = argmin\\;L(w)\\\\\n",
    "   为了达到求解最小化L(w)问题，我们应用高等数学的知识，使用求导来解决这个问题： \\\\\n",
    "   \\frac{\\partial L(w)}{\\partial w} = 2X^TXw-2X^TY = 0,因此： \\\\\n",
    "   \\hat{w} = (X^TX)^{-1}X^TY\n",
    "   $$                   \n",
    "   (b) 几何解释：                \n",
    "   在线性代数中，我们知道两个向量a和b相互垂直可以得出：$<a,b> = a.b = a^Tb = 0$,而平面X的法向量为Y-Xw，与平面X互相垂直，因此：$X^T(Y-Xw) = 0$，即：$w = (X^TX)^{-1}X^TY$                             \n",
    "   ![jupyter](./1.5.png)               \n",
    "   (c) 概率视角：       \n",
    "   假设噪声$\\epsilon \\backsim N(0,\\sigma^2),y=f(w)+\\epsilon=w^Tx+\\epsilon$，因此：$y|x_i,w ~ N(w^Tx,\\sigma^2)$          \n",
    "   我们使用极大似然估计MLE对参数w进行估计：       \n",
    "   $$\n",
    "   L(w) = log\\;P(Y|X;w) = log\\;\\prod_{i=1}^N P(y_i|x_i;w) = \\sum\\limits_{i=1}^{N} log\\; P(y_i|x_i;w)\\\\\n",
    "    = \\sum\\limits_{i=1}^{N}log(\\frac{1}{\\sqrt{2\\pi \\sigma}}exp(-\\frac{(y_i-w^Tx_i)^2}{2\\sigma^2})) = \\sum\\limits_{i=1}^{N}[log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\frac{1}{2\\sigma^2}(y_i-w^Tx_i)^2] \\\\\n",
    "    argmax_w L(w) = argmin_w[l(w) = \\sum\\limits_{i = 1}^{N}(y_i-w^Tx_i)^2]\\\\\n",
    "    因此：线性回归的最小二乘估计<==>噪声\\epsilon\\backsim N(0,\\sigma^2)的极大似然估计\n",
    "   $$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们使用sklearn的线性回归实例来演示：                   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型系数： [-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n",
      "模型得分： 0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model      # 引入线性回归方法\n",
    "lin_reg = linear_model.LinearRegression()       # 创建线性回归的类\n",
    "lin_reg.fit(X,y)        # 输入特征X和因变量y进行训练\n",
    "print(\"模型系数：\",lin_reg.coef_)             # 输出模型的系数\n",
    "print(\"模型得分：\",lin_reg.score(X,y))    # 输出模型的决定系数R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **线性回归的推广**                     \n",
    "   在线性回归中，我们假设因变量与特征之间的关系是线性关系，这样的假设使得模型很简单，但是缺点也是显然的，那就是当数据存在非线性关系时，我们使用线性回归模型进行预测会导致预测性能极其低下，因为模型的形式本身是线性的，无法表达数据中的非线性关系。我们一个很自然的想法就是去推广线性回归模型，使得推广后的模型更能表达非线性的关系。                     \n",
    "   (a) 多项式回归：        \n",
    "   为了体现因变量和特征的非线性关系，一个很自然而然的想法就是将标准的线性回归模型：             \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_i + \\epsilon_i\n",
    "   $$               \n",
    "   换成一个多项式函数：          \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_i + w_2x_i^2 + ...+w_dx_i^d + \\epsilon\n",
    "   $$                        \n",
    "   对于多项式的阶数d不能取过大，一般不大于3或者4，因为d越大，多项式曲线就会越光滑，在X的边界处有异常的波动。（图中的边界处的4阶多项式拟合曲线的置信区间(虚线表示置信区间)明显增大，预测效果的稳定性下降。）   \n",
    "\n",
    "   ![jupyter](./1.6.1.png) ![jupyter](./1.6.2.png)                             \n",
    "   \n",
    "   (b) 广义可加模型(GAM)：         \n",
    "   广义可加模型GAM实际上是线性模型推广至非线性模型的一个框架，在这个框架中，每一个变量都用一个非线性函数来代替，但是模型本身保持整体可加性。GAM模型不仅仅可以用在线性回归的推广，还可以将线性分类模型进行推广。具体的推广形式是：          \n",
    "   标准的线性回归模型：           \n",
    "   $$\n",
    "   y_i = w_0 + w_1x_{i1} +...+w_px_{ip} + \\epsilon_i \n",
    "   $$                 \n",
    "   GAM模型框架：                  \n",
    "   $$\n",
    "   y_i = w_0 + \\sum\\limits_{j=1}^{p}f_{j}(x_{ij}) + \\epsilon_i\n",
    "   $$                  \n",
    "   GAM模型的优点与不足：             \n",
    "      - 优点：简单容易操作，能够很自然地推广线性回归模型至非线性模型，使得模型的预测精度有所上升；由于模型本身是可加的，因此GAM还是能像线性回归模型一样把其他因素控制不变的情况下单独对某个变量进行推断，极大地保留了线性回归的易于推断的性质。\n",
    "      - 缺点：GAM模型会经常忽略一些有意义的交互作用，比如某两个特征共同影响因变量，不过GAM还是能像线性回归一样加入交互项$x^{(i)} \\times x^{(j)}$的形式进行建模；但是GAM模型本质上还是一个可加模型，如果我们能摆脱可加性模型形式，可能还会提升模型预测精度，详情请看后面的算法。\n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 多项式回归实例介绍：                    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=poly#sklearn.preprocessing.PolynomialFeatures                              \n",
    "sklearn.preprocessing.PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C'):               \n",
    "   - 参数：         \n",
    "   degree：特征转换的阶数。                       \n",
    "   interaction_onlyboolean：是否只包含交互项，默认False 。              \n",
    "   include_bias：是否包含截距项，默认True。           \n",
    "   order：str in {‘C’, ‘F’}, default ‘C’，输出数组的顺序。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始X为：\n",
      " [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "2次转化X：\n",
      " [[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n",
      "2次转化X：\n",
      " [[ 1.  0.  1.  0.]\n",
      " [ 1.  2.  3.  6.]\n",
      " [ 1.  4.  5. 20.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X_arr = np.arange(6).reshape(3, 2)\n",
    "print(\"原始X为：\\n\",X_arr)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "print(\"2次转化X：\\n\",poly.fit_transform(X_arr))\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "print(\"2次转化X：\\n\",poly.fit_transform(X_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) GAM模型实例介绍：          \n",
    "安装pygam：pip install pygam               \n",
    "https://github.com/dswah/pyGAM/blob/master/doc/source/notebooks/quick_start.ipynb                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearGAM                                                                                                 \n",
      "=============================================== ==========================================================\n",
      "Distribution:                        NormalDist Effective DoF:                                    103.2423\n",
      "Link Function:                     IdentityLink Log Likelihood:                                 -1589.7653\n",
      "Number of Samples:                          506 AIC:                                             3388.0152\n",
      "                                                AICc:                                            3442.7649\n",
      "                                                GCV:                                               13.7683\n",
      "                                                Scale:                                              8.8269\n",
      "                                                Pseudo R-Squared:                                   0.9168\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "s(0)                              [0.6]                20           11.1         2.20e-11     ***         \n",
      "s(1)                              [0.6]                20           12.8         8.15e-02     .           \n",
      "s(2)                              [0.6]                20           13.5         2.59e-03     **          \n",
      "s(3)                              [0.6]                20           3.8          2.76e-01                 \n",
      "s(4)                              [0.6]                20           11.4         1.11e-16     ***         \n",
      "s(5)                              [0.6]                20           10.1         1.11e-16     ***         \n",
      "s(6)                              [0.6]                20           10.4         8.22e-01                 \n",
      "s(7)                              [0.6]                20           8.5          4.44e-16     ***         \n",
      "s(8)                              [0.6]                20           3.5          5.96e-03     **          \n",
      "s(9)                              [0.6]                20           3.4          1.33e-09     ***         \n",
      "s(10)                             [0.6]                20           1.8          3.26e-03     **          \n",
      "s(11)                             [0.6]                20           6.4          6.25e-02     .           \n",
      "s(12)                             [0.6]                20           6.5          1.11e-16     ***         \n",
      "intercept                                              1            0.0          2.23e-13     ***         \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bb049c74476f>:3: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n",
      " \n",
      "Please do not make inferences based on these values! \n",
      "\n",
      "Collaborate on a solution, and stay up to date at: \n",
      "github.com/dswah/pyGAM/issues/163 \n",
      "\n",
      "  gam.summary()\n"
     ]
    }
   ],
   "source": [
    "from pygam import LinearGAM\n",
    "gam = LinearGAM().fit(boston_data[boston.feature_names], y)\n",
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **回归树：**                                   \n",
    "   基于树的回归方法主要是依据分层和分割的方式将特征空间划分为一系列简单的区域。对某个给定的待预测的自变量，用他所属区域中训练集的平均数或者众数对其进行预测。由于划分特征空间的分裂规则可以用树的形式进行概括，因此这类方法称为决策树方法。决策树由结点(node)和有向边(diredcted edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类别或者某个值。区域$R_1,R_2$等称为叶节点，将特征空间分开的点为内部节点。                           \n",
    "   ![jupyter](./1.7.png)                     \n",
    "   建立回归树的过程大致可以分为以下两步：          \n",
    "      - 将自变量的特征空间(即$x^{(1)},x^{(2)},x^{(3)},...,x^{(p)}$)的可能取值构成的集合分割成J个互不重叠的区域$R_1,R_2,...,R_j$。        \n",
    "      - 对落入区域$R_j$的每个观测值作相同的预测，预测值等于$R_j$上训练集的因变量的简单算术平均。              \n",
    "   具体来说，就是：             \n",
    "      a.  选择最优切分特征j以及该特征上的最优点s：                \n",
    "      遍历特征j以及固定j后遍历切分点s，选择使得下式最小的(j,s)  $min_{j,s}[min_{c_1}\\sum\\limits_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\\sum\\limits_{x_i\\in R_2(j,s)}(y_i-c_2)^2 ]$                           \n",
    "       b. 按照(j,s)分裂特征空间：$R_1(j,s) = \\{x|x^{j} \\le s \\}和R_2(j,s) = \\{x|x^{j} > s \\},\\hat{c}_m = \\frac{1}{N_m}\\sum\\limits_{x \\in R_m(j,s)}y_i,\\;m=1,2$                           \n",
    "       c. 继续调用步骤1，2直到满足停止条件，就是每个区域的样本数小于等于5。        \n",
    "       d. 将特征空间划分为J个不同的区域，生成回归树：$f(x) = \\sum\\limits_{m=1}^{J}\\hat{c}_mI(x \\in R_m)$                \n",
    "       如以下生成的关于运动员在棒球大联盟数据的回归树：             \n",
    "       ![jupyter](./1.8.png)                   \n",
    "    回归树与线性模型的比较：              \n",
    "    线性模型的模型形式与树模型的模型形式有着本质的区别，具体而言，线性回归对模型形式做了如下假定：$f(x) = w_0 + \\sum\\limits_{j=1}^{p}w_jx^{(j)}$，而回归树则是$f(x) = \\sum\\limits_{m=1}^{J}\\hat{c}_mI(x \\in R_m)$。那问题来了，哪种模型更优呢？这个要视具体情况而言，如果特征变量与因变量的关系能很好的用线性关系来表达，那么线性回归通常有着不错的预测效果，拟合效果则优于不能揭示线性结构的回归树。反之，如果特征变量与因变量的关系呈现高度复杂的非线性，那么树方法比传统方法更优。                     \n",
    "    ![jupyter](./1.9.1.png)                        \n",
    "    树模型的优缺点：                 \n",
    "    - 树模型的解释性强，在解释性方面可能比线性回归还要方便。\n",
    "    - 树模型更接近人的决策方式。\n",
    "    - 树模型可以用图来表示，非专业人士也可以轻松解读。\n",
    "    - 树模型可以直接做定性的特征而不需要像线性回归一样哑元化。\n",
    "    - 树模型能很好处理缺失值和异常值，对异常值不敏感，但是这个对线性模型来说却是致命的。\n",
    "    - 树模型的预测准确性一般无法达到其他回归模型的水平，但是改进的方法很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn使用回归树的实例：                \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=tree#sklearn.tree.DecisionTreeRegressor                         \n",
    "sklearn.tree.DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0）                                                 \n",
    "   - 参数：(列举几个重要的，常用的，详情请看上面的官网)                 \n",
    "   criterion：{“ mse”，“ friedman_mse”，“ mae”}，默认=“ mse”。衡量分割标准的函数 。                      \n",
    "   splitter：{“best”, “random”}, default=”best”。分割方式。                    \n",
    "   max_depth：树的最大深度。               \n",
    "   min_samples_split：拆分内部节点所需的最少样本数，默认是2。                      \n",
    "   min_samples_leaf：在叶节点处需要的最小样本数。默认是1。                    \n",
    "   min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。默认是0。                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9376307599929274"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor    \n",
    "reg_tree = DecisionTreeRegressor(criterion = \"mse\",min_samples_leaf = 5)\n",
    "reg_tree.fit(X,y)\n",
    "reg_tree.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 支持向量机回归(SVR)                                        \n",
    "   在介绍支持向量回归SVR之前，我们先来了解下约束优化的相关知识：          \n",
    "      - 约束优化问题(P)：                   \n",
    "      $$\n",
    "      min f(x)  \\\\\n",
    "      s.t.\\;\\;\\;g_i(x) \\le 0,\\; i=1,2,...,m\\\\\n",
    "      \\;\\;\\;\\;\\; h_j(x) = 0,\\; j=1,2,...,l\n",
    "      $$                         \n",
    "   我们假设$x^*$为满足以上条件的局部最优解，$p^* = f(x^*)$，我们的目的就是要找到$x^*$与$p^*$，满足不等式和等式约束的x集合成为可行域，记作S。\n",
    "   \n",
    "    - KKT条件(最优解的一阶必要条件)          \n",
    "    因为KKT条件是最优化的相关内容，在本次开源学习中并不是重点，因此在这里我用一个更加简单的例子说明KKT条件，严格的证明请参见凸优化相关书籍。                       \n",
    "    在这个例子中，我们考虑：($x^*$为我们的最优解)               \n",
    "    $$\n",
    "    minf(x)\\\\\n",
    "    s.t.\\;g_1(x) \\le 0,\\;x \\in R^n\\\\\n",
    "    \\;\\;\\;g_2(x) \\le 0\\\\\n",
    "    \\;\\;\\;g_3(x) \\le 0\n",
    "    $$\n",
    "    ![jupyter](./1.11.png)\n",
    "      \n",
    "      我们可以看到：$-\\nabla f(x^*)$可以由$\\nabla g_1(x^*)$与$\\nabla g_2(x^*)$线性表出，因此有：$-\\nabla f(x^*) = \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*)$，其中$\\lambda_1,\\lambda_2 \\ge 0$，即：                          \n",
    "      $$\n",
    "      \\nabla f(x^*) + \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*) = 0,\\;\\;\\;其中\\lambda_1,\\lambda_2 \\ge 0\n",
    "      $$                       \n",
    "      我们把没有起作用的约束$g_3(x)$也放到式子里面去，目的也就是为了书写方便，即要求：                  \n",
    "      $$\n",
    "      \\nabla f(x^*) + \\lambda_1 \\nabla g_1(x^*) + \\lambda_2 \\nabla g_2(x^*) + \\lambda_3 \\nabla g_3(x^*)= 0,\\;\\;\\;其中\\lambda_1,\\lambda_2 \\ge 0,\\lambda_3 = 0\n",
    "      $$                      \n",
    "      由于点$x^*$位于方程$g_1(x)=0$与$g_2(x)=0$上，因此：$\\lambda_1 g_1(x^*)  = 0,\\lambda_2  g_2(x^*) = 0 , \\lambda_3  g_3(x^*)= 0$                        \n",
    "      \n",
    "      因此，KKT条件就是：假设$x^*$为最优化问题(P)的局部最优解，且$x^*$ 在某个适当的条件下 ,有：                             \n",
    "      $$\n",
    "     \\nabla f(x^*) + \\sum\\limits_{i=1}^{m}\\lambda_i \\nabla g(x^*) + \\sum\\limits_{j=1}^{l}\\mu_j \\nabla h_j(x^*) = 0(对偶条件)\\\\     \n",
    "      \\lambda_i \\ge 0,\\;i = 1,2,...,m(对偶条件)\\\\\n",
    "      g_i(x^*) \\le 0(原问题条件)\\\\\n",
    "      h_j(x^*) = 0(原问题条件)\\\\\n",
    "      \\lambda_i g(x^*) = 0(互补松弛定理)\n",
    "      $$                              \n",
    " - 对偶理论：               \n",
    "   为什么要引入对偶问题呢？是因为原问题与对偶问题就像是一个问题两个角度去看，如利润最大与成本最低等。有时侯原问题上难以解决，但是在对偶问题上就会变得很简单。再者，任何一个原问题在变成对偶问题后都会变成一个凸优化的问题，这点我们后面会有介绍。下面我们来引入对偶问题：                           \n",
    "   首先，我们的原问题(P)是：\n",
    "      $$\n",
    "      min f(x)  \\\\\n",
    "      s.t.\\;\\;\\;g_i(x) \\le 0,\\; i=1,2,...,m\\\\\n",
    "      \\;\\;\\;\\;\\; h_j(x) = 0,\\; j=1,2,...,l\n",
    "      $$                             \n",
    "      引入拉格朗日函数：$L(x,\\lambda,\\mu) = f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x)$                        \n",
    "      拉格朗日对偶函数：                                  \n",
    "      $$\n",
    "      d(\\lambda,\\mu)  =  min_{x\\in X}\\{ f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x)\\} ,其中X为满足条件的x变量\\\\\n",
    "      \\le min_{x\\in S}\\{ f(x) + \\sum\\limits_{i=1}^{m}\\lambda_i g_i(x) + \\sum\\limits_{j=1}^{l}\\mu_j h_j(x) \\},由于g_i(x) \\le 0,h_j(x) = 0,\\lambda_i \\ge 0 ,其中S为可行域\\\\\n",
    "      \\le min_{x\\in S}\\{f(x) \\}\n",
    "      $$                                   \n",
    "      因此：拉格朗日对偶函数$d(\\lambda,\\mu)$是原问题最优解的函数值$p^*$的下界，即每个不同的$\\lambda$与$\\mu$确定的$d(\\lambda,\\mu)$都是$p^*$的下界，但是我们希望下界越大越好，因为越大就更能接近真实的$p^*$。因此：                               \n",
    "      拉格朗日对偶问题(D)转化为：                 \n",
    "      $$\n",
    "      max_{\\lambda,\\mu}d(\\lambda,\\mu)\\\\\n",
    "      s.t. \\lambda_i \\ge 0,i = 1,2,...,m\\\\\n",
    "      也就是：\\\\\n",
    "      max_{\\lambda \\ge 0,\\mu}\\;min_{x \\in S} L(x,\\lambda,\\mu)\n",
    "      $$                   \n",
    "      我们可以观察到，对偶问题是关于$\\lambda$和$\\mu$的线性函数，因此对偶问题是一个凸优化问题，凸优化问题在最优化理论较为简单。\n",
    "      弱对偶定理：对偶问题(D)的最优解$D^*$一定小于原问题最优解$P^*$，这点在刚刚的讨论得到了充分的证明，一定成立。                   \n",
    "      强对偶定理：对偶问题(D)的最优解$D^*$在一定的条件下等于原问题最优解$P^*$，条件非常多样化且不是唯一的，也就是说这是个开放性的问题，在这里我给出一个最简单的条件，即：$f(x)$与$g_i(x)$为凸函数，$h_j(x)$为线性函数，X是凸集，$x^*$满足KKT条件，那么$D^* = P^*$。 \n",
    "      \n",
    "   \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 支持向量回归SVR                    \n",
    "   在介绍完了相关的优化知识以后，我们开始正式学习支持向量回归SVR。                    \n",
    "   ![jupyter](./1.10.png)                         \n",
    "   在线性回归的理论中，每个样本点都要计算平方损失，但是SVR却是不一样的。SVR认为：落在$f(x)$的$\\epsilon$邻域空间中的样本点不需要计算损失，这些都是预测正确的，其余的落在$\\epsilon$邻域空间以外的样本才需要计算损失，因此：              \n",
    "   ![jupyter](./1.12.png)                                      \n",
    "   $$\n",
    "   min_{w,b,\\xi_i,\\hat{\\xi}_i} \\frac{1}{2}||w||^2 +C \\sum\\limits_{i=1}^{N}(\\xi_i,\\hat{\\xi}_i)\\\\\n",
    "   s.t.\\;\\;\\; f(x_i) - y_i \\le \\epsilon + \\xi_i\\\\\n",
    "   \\;\\;\\;\\;\\;y_i - f(x_i) \\le  \\epsilon +\\hat{\\xi}_i\\\\\n",
    "   \\;\\;\\;\\;\\; \\xi_i,\\hat{\\xi}_i \\le 0,i = 1,2,...,N\n",
    "   $$                                 \n",
    "   引入拉格朗日函数：                  \n",
    "   $$\n",
    "   \\begin{array}{l}\n",
    "L(w, b, \\alpha, \\hat{\\alpha}, \\xi, \\xi, \\mu, \\hat{\\mu}) \\\\\n",
    "\\quad=\\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N}\\left(\\xi_{i}+\\widehat{\\xi}_{i}\\right)-\\sum_{i=1}^{N} \\xi_{i} \\mu_{i}-\\sum_{i=1}^{N} \\widehat{\\xi}_{i} \\widehat{\\mu}_{i} \\\\\n",
    "\\quad+\\sum_{i=1}^{N} \\alpha_{i}\\left(f\\left(x_{i}\\right)-y_{i}-\\epsilon-\\xi_{i}\\right)+\\sum_{i=1}^{N} \\widehat{\\alpha}_{i}\\left(y_{i}-f\\left(x_{i}\\right)-\\epsilon-\\widehat{\\xi}_{i}\\right)\n",
    "\\end{array}\n",
    "   $$                       \n",
    "   再令$L(w, b, \\alpha, \\hat{\\alpha}, \\xi, \\xi, \\mu, \\hat{\\mu})$对$w,b,\\xi,\\hat{\\xi}$求偏导等于0，得： $w=\\sum_{i=1}^{N}\\left(\\widehat{\\alpha}_{i}-\\alpha_{i}\\right) x_{i}$。                             \n",
    "   上述过程中需满足KKT条件，即要求：                 \n",
    "   $$\n",
    "   \\left\\{\\begin{array}{c}\n",
    "\\alpha_{i}\\left(f\\left(x_{i}\\right)-y_{i}-\\epsilon-\\xi_{i}\\right)=0 \\\\\n",
    "\\hat{\\alpha_{i}}\\left(y_{i}-f\\left(x_{i}\\right)-\\epsilon-\\hat{\\xi}_{i}\\right)=0 \\\\\n",
    "\\alpha_{i} \\widehat{\\alpha}_{i}=0, \\xi_{i} \\hat{\\xi}_{i}=0 \\\\\n",
    "\\left(C-\\alpha_{i}\\right) \\xi_{i}=0,\\left(C-\\widehat{\\alpha}_{i}\\right) \\hat{\\xi}_{i}=0\n",
    "\\end{array}\\right.\n",
    "   $$                         \n",
    "   SVR的解形如:$f(x)=\\sum_{i=1}^{N}\\left(\\widehat{\\alpha}_{i}-\\alpha_{i}\\right) x_{i}^{T} x+b$                                                  \n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn中使用SVR实例：                        \n",
    "sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)            \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svr#sklearn.svm.SVR                        \n",
    "   - 参数：                   \n",
    "   kernel：核函数，{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, 默认=’rbf’。(后面会详细介绍)              \n",
    "   degree：多项式核函数的阶数。默认 = 3。                          \n",
    "   C：正则化参数，默认=1.0。(后面会详细介绍)                            \n",
    "   epsilon：SVR模型允许的不计算误差的邻域大小。默认0.1。              \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7024525421955277"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler     # 标准化数据\n",
    "from sklearn.pipeline import make_pipeline   # 使用管道，把预处理和模型形成一个流程\n",
    "\n",
    "reg_svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "reg_svr.fit(X, y)\n",
    "reg_svr.score(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
